
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>14. Statistical inference &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Cat": "\\mathcal{C}at", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bpi": "\\boldsymbol\\pi", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bf": "\\mathbf{f}", "bg": "\\mathbf{g}", "bp": "\\mathbf{p}", "br": "\\mathbf{r}", "bs": "\\mathbf{s}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bD": "\\mathbf{D}", "bH": "\\mathbf{H}", "bI": "\\mathbf{I}", "bK": "\\mathbf{K}", "bS": "\\mathbf{S}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bW": "\\mathbf{W}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calJ": "\\mathcal{J}", "calH": "\\mathcal{H}", "calI": "\\mathcal{I}", "calL": "\\mathcal{L}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "calS": "\\mathcal{S}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/14-stat-infer';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="15. Bibliography" href="bib.html" />
    <link rel="prev" title="13. Learning" href="13-learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-halfway.html">9. The halfway point: pivoting toward models and data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-info-theory.html">10. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-optim.html">11. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-models.html">12. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-learning.html">13. Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">14. Statistical inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">15. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/14-stat-infer.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Statistical inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-estimators-and-their-distributions">14.1. Statistics, estimators, and their distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">14.2. Confidence intervals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests">14.3. Hypothesis tests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-inference-in-linear-regression">14.4. Statistical inference in linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-variance">14.5. Analysis of variance</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><strong>THIS CHAPTER IS UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="statistical-inference">
<span id="stat-infer"></span><h1><span class="section-number">14. </span>Statistical inference<a class="headerlink" href="#statistical-inference" title="Link to this heading">#</a></h1>
<section id="statistics-estimators-and-their-distributions">
<h2><span class="section-number">14.1. </span>Statistics, estimators, and their distributions<a class="headerlink" href="#statistics-estimators-and-their-distributions" title="Link to this heading">#</a></h2>
<p>Our story of statistical inference begins by defining exactly what we mean by a <em>statistic</em>. The definition we give is very abstract in order to cover all the different cases in which statistics appear. However, as we go through the rest of the chapter, the statistics that we consider will mostly be familiar ones like means, variances, and standard deviations, as well as parameter estimators.</p>
<div class="proof definition admonition" id="statistic-def">
<p class="admonition-title"><span class="caption-number">Definition 14.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX\)</span> be a <span class="math notranslate nohighlight">\(m\)</span>-dimensional random vector. A <em>statistic</em> is a random variable of the form</p>
<div class="math notranslate nohighlight">
\[
T = r(\bX),
\]</div>
<p>where <span class="math notranslate nohighlight">\(r:\bbr^m \to \bbr\)</span> is a real-valued function. An observed value <span class="math notranslate nohighlight">\(t\)</span> of <span class="math notranslate nohighlight">\(T\)</span> is called an <em>observed statistic</em> or <em>empirical statistic</em>.</p>
</section>
</div><p>If we conceptualize the components of a random vector <span class="math notranslate nohighlight">\(\bX\)</span> as a random sample <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span>, then a statistic</p>
<div class="math notranslate nohighlight">
\[
T = r(X_1,X_2,\ldots,X_m)
\]</div>
<p>is simply a function of the sample. Crucially important examples of statistics (and empirical statistics) include those defined as follows:</p>
<div class="proof definition admonition" id="sample-mean-var-def">
<p class="admonition-title"><span class="caption-number">Definition 14.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_m)\)</span> be an <span class="math notranslate nohighlight">\(m\)</span>-dimensional random vector. The <em>sample mean</em> is defined to be the statistic</p>
<div class="math notranslate nohighlight">
\[
\overline{X} \def \frac{1}{m}(X_1+\cdots+X_m),
\]</div>
<p>while the <em>sample variance</em> is defined to be the statistic</p>
<div class="math notranslate nohighlight">
\[
S^2 \def \frac{1}{m-1} \sum_{i=1}^m(X_i - \overline{X})^2.
\]</div>
<p>The corresponding empirical statistics are the <em>empirical mean</em> and <em>empirical variance</em> defined as</p>
<div class="math notranslate nohighlight">
\[
\overline{x} \def \frac{1}{m}(x_1+\cdots+x_m) \quad \text{and} \quad s^2 \def \frac{1}{m-1} \sum_{i=1}^m(x_i - \overline{x})^2.
\]</div>
</section>
</div><p>As we mentioned above, very often the component random variables <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> of the random vector <span class="math notranslate nohighlight">\(\bX\)</span> in the definition are assumed to form a random sample, i.e., an IID sequence of random variables. The dimension <span class="math notranslate nohighlight">\(m\)</span> is then referred to as the <em>sample size</em>. In principle, then, the sample size <span class="math notranslate nohighlight">\(m\)</span> can be <em>any</em> positive integer, and so it is often convenient to write <span class="math notranslate nohighlight">\(\overline{X}_m\)</span> for the sample mean, explicitly displaying the sample size. This gives us an entire <em>infinite sequence</em> of sample means, one for each sample size <span class="math notranslate nohighlight">\(m\)</span>.</p>
<p>Since statistics are random vectors, they have their own probability distributions. These are given special names:</p>
<div class="proof definition admonition" id="samp-dist-def">
<p class="admonition-title"><span class="caption-number">Definition 14.3 </span></p>
<section class="definition-content" id="proof-content">
<p>The probability distribution of a statistic <span class="math notranslate nohighlight">\(T\)</span> is called the <em>sampling distribution</em> of <span class="math notranslate nohighlight">\(T\)</span>.</p>
</section>
</div><p>In general, computing the sampling distributions of statistics is difficult. But if we actually have <em>observed</em> data <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span>, then (as you will explore in the programming assignment) there is a resampling method known as <em>bootstrapping</em> that yields approximations to sampling distributions. An example is given by the histogram (with KDE) on the right-hand side of the following figure, where a histogram (with KDE) of the empirical distribution of an observed dataset is given on the left-hand side:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">ss</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="k">as</span> <span class="nn">clr</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">sample_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">resample_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">random_sample</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
<span class="n">replicate_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">num_resamples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_resamples</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">random_sample</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">resample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">replicate_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">random_sample</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">replicate_means</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;observed data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;sampling distribution of the mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/418dc91c541f6366971dc312f7518f4990d2829ff32f0bb3c123dafdf04b9912.svg" src="../_images/418dc91c541f6366971dc312f7518f4990d2829ff32f0bb3c123dafdf04b9912.svg" /></figure>
</div>
</div>
<p>Observe that the sampling distribution on the right-hand side appears to be well approximated by a normal distribution. This is actually a manifestation of the Central Limit Theorem (see <a class="reference internal" href="#clt-thm">Theorem 14.5</a>), which says that the sample means <span class="math notranslate nohighlight">\(\overline{X}_m\)</span> converge (in distribution) to a normal distribution as <span class="math notranslate nohighlight">\(m\to \infty\)</span>, provided that the random variables are IID. This is true even when the observed data are not normally distributed.</p>
<p>Though our definition of a statistic (in <a class="reference internal" href="#statistic-def">Definition 14.1</a>) is a <em>completely arbitrary function</em> of the data, very often statistics are constructed with a particular “target” in mind. For example, if we suppose that we have an IID random sample</p>
<div class="math notranslate nohighlight">
\[
X_1,X_2,\ldots,X_m
\]</div>
<p>drawn from a distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, then the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> and variance <span class="math notranslate nohighlight">\(S^2\)</span> are cooked up as sample-based proxies for <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. The sample mean and variance are not haphazardly constructed statistics with no clear purpose—rather, they are “aimed” at <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. When statistics are constructed to serve as estimators for preexisting quantities of interest, they are (naturally) called <em>estimators</em>:</p>
<div class="proof definition admonition" id="exp-var-estimators-def">
<p class="admonition-title"><span class="caption-number">Definition 14.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\sim P\)</span> be an IID random sample drawn from some probability distribution <span class="math notranslate nohighlight">\(P\)</span>, and let <span class="math notranslate nohighlight">\(\theta\)</span> be a quantity of interest, possibly (and very often) a parameter of the distribution <span class="math notranslate nohighlight">\(P\)</span>. A statistic <span class="math notranslate nohighlight">\(T = r(X_1,X_2,\ldots,X_m)\)</span> that is intended to serve as an estimator of <span class="math notranslate nohighlight">\(\theta\)</span> is called an <em>estimator</em> and is often denoted by <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> instead of <span class="math notranslate nohighlight">\(T\)</span>.</p>
<ol class="arabic">
<li><p>The <em>expected value</em> of the estimator is the quantity</p>
<div class="math notranslate nohighlight">
\[
    \mu_{\hat{\theta}} \def E(\hat{\theta}).
    \]</div>
<p>The <em>bias</em> of the estimator is the difference <span class="math notranslate nohighlight">\(E(\hat{\theta}) - \theta\)</span>. The estimator is said to be <em>unbiased</em> if its bias is <span class="math notranslate nohighlight">\(0\)</span>.</p>
</li>
<li><p>The <em>variance</em> of the estimator is the quantity</p>
<div class="math notranslate nohighlight">
\[
    \sigma^2_{\hat{\theta}} \def V(\hat{\theta}).
    \]</div>
</li>
<li><p>The <em>standard error</em> of the estimator is the quantity</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{\hat{\theta}} \def \sqrt{\sigma^2_{\hat{\theta}}}.
    \]</div>
</li>
</ol>
</section>
</div><p>Observed values of estimators are called <em>estimates</em> and, confusingly, they are also often denoted <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. So, the single notation <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> very often stands for two things, the estimator itself, and its observed values.</p>
<p>As our first example, let’s compute the bias and standard error of the sample mean. Note that the sample mean may also be written as <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>, since it is intended as an estimator of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<div class="proof theorem admonition" id="prop-sample-mean-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.1 </span> (Properties of the sample mean)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<ol class="arabic simple">
<li><p>The expectation of the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>, and thus the sample mean is an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>The variance of the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> is <span class="math notranslate nohighlight">\(\sigma^2/m\)</span>, and hence its standard error is <span class="math notranslate nohighlight">\(\sigma_{\overline{X}} = \sigma/\sqrt{m}\)</span>.</p></li>
<li><p>If the <span class="math notranslate nohighlight">\(X_i\)</span>’s are normally distributed, then so too is the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Each random variable <span class="math notranslate nohighlight">\(X_i\)</span> has mean <span class="math notranslate nohighlight">\(\mu\)</span>. By linearity of expectation, we have</p>
<div class="math notranslate nohighlight">
\[
E(\overline{X}) = \frac{1}{m}E(X_1 + \cdots + X_m) = \frac{1}{m}\left[ E(X_1) + \cdots + E(X_m) \right] = \frac{1}{m} \cdot m \mu = \mu.
\]</div>
<p>This proves statement (1). For (2), we note that each <span class="math notranslate nohighlight">\(X_i\)</span> has variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> and that they are independent. But independent random variables are uncorrelated, and so by <a class="reference internal" href="08-more-prob.html#variance-lin-combo-thm">Theorem 8.15</a> we get</p>
<div class="math notranslate nohighlight">
\[
V(\overline{X}) = \frac{1}{m^2} \sum_{i=1}^m V(X_i) = \frac{1}{m^2} \cdot m \sigma^2 = \frac{\sigma^2}{m}.
\]</div>
<p>Thus, the standard error is <span class="math notranslate nohighlight">\(\sqrt{\sigma^2/m} = \sigma / \sqrt{m}\)</span>. This proves statement (2). Statement (3) follows from <a class="reference internal" href="08-more-prob.html#char-normal-thm">Theorem 8.24</a>. Q.E.D.</p>
</div>
</section>
<section id="confidence-intervals">
<span id="cis"></span><h2><span class="section-number">14.2. </span>Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Link to this heading">#</a></h2>
<p>Suppose we have constructed an estimator</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = r(X_1,X_2,\ldots,X_m)
\]</div>
<p>for some quantity of interest <span class="math notranslate nohighlight">\(\theta\)</span>, as a function of an IID random sample <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m \sim P\)</span>. If we feed in an observed dataset <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> to the estimator, then we produce a single-number estimate, or <em>point estimate</em>, for theta:</p>
<a class="reference internal image-reference" href="../_images/point-est.svg"><img alt="../_images/point-est.svg" class="align-center" src="../_images/point-est.svg" width="65%" /></a>
<p> </p>
<p>On the number line, we have marked a point estimate, as well as the “true” value of the quantity <span class="math notranslate nohighlight">\(\theta\)</span> to be estimated. Now, the point estimate may prove to be sufficient for our needs, but sometimes we would like an entire <em>interval estimate</em> for <span class="math notranslate nohighlight">\(\theta\)</span>. Or, said differently, we would like an interval that yields estimates for <span class="math notranslate nohighlight">\(\theta\)</span> while also giving some sense of the variability in the estimate. These intervals are called <em>confidence intervals</em>:</p>
<a class="reference internal image-reference" href="../_images/ci.svg"><img alt="../_images/ci.svg" class="align-center" src="../_images/ci.svg" width="65%" /></a>
<p> </p>
<p>The end points of the interval will depend on three things:</p>
<ol class="arabic simple">
<li><p>A predetermined <em>confidence level</em> that, in some sense, tells us how likely it is that the interval contains the “true” value <span class="math notranslate nohighlight">\(\theta\)</span>. As your intuition would suggest, higher confidence levels generally go with wider confidence intervals.</p></li>
<li><p>The distribution <span class="math notranslate nohighlight">\(P\)</span> from which the random sample <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> is drawn.</p></li>
<li><p>The variability in the estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> itself, as measured by its standard error <span class="math notranslate nohighlight">\(\sigma_{\hat{\theta}}\)</span>.</p></li>
</ol>
<p>So, a confidence interval (centered at the point estimate <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>) often looks like</p>
<div class="math notranslate nohighlight">
\[
\left( \hat{\theta} - l \cdot \sigma_{\hat{\theta}} , \hat{\theta} + u \cdot \sigma_{\hat{\theta}}  \right),
\]</div>
<p>where the (positive) numbers <span class="math notranslate nohighlight">\(l\)</span> and <span class="math notranslate nohighlight">\(u\)</span> depend on both the confidence level of the interval and the underlying data distribution. (In this form, notice that a smaller standard error yields a narrower interval.) The goal of this section is to describe several types of confidence intervals for different types of estimators and underlying distributions <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>We begin in the simple case of a normally distributed random sample</p>
<div class="math notranslate nohighlight">
\[
X_1,X_2,\ldots,X_m \sim \mathcal{N}(\mu,\sigma^2)
\]</div>
<p>with unknown mean <span class="math notranslate nohighlight">\(\mu\)</span> and known variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. A natural estimator for <span class="math notranslate nohighlight">\(\mu\)</span> is the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> studied in the previous section. Thus, we aim to construct a confidence interval of the form</p>
<div class="math notranslate nohighlight">
\[
\left( \overline{X} - l \cdot \sigma_{\overline{X}}, \overline{X} + u \cdot \sigma_{\overline{X}} \right) = \left( \overline{X} - l \cdot \frac{\sigma}{\sqrt{m}}, \overline{X} + u \cdot \frac{\sigma}{\sqrt{m}} \right).
\]</div>
<p>Of course, the empirical mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> serves as an estimate for <span class="math notranslate nohighlight">\(\mu\)</span>. But <span class="math notranslate nohighlight">\(\bar{x}\)</span> depends on the dataset, and thus <span class="math notranslate nohighlight">\(\bar{x}\)</span> carries variability—a different dataset would yield a different empirical mean <span class="math notranslate nohighlight">\(\bar{x}\)</span>, and thus a different estimate for <span class="math notranslate nohighlight">\(\mu\)</span>. To account for this variability, we may want to produce not only a single <em>point estimate</em> for <span class="math notranslate nohighlight">\(\mu\)</span>, but rather an entire <em>interval estimate</em>, i.e., an interval of values that captures the “true” value of <span class="math notranslate nohighlight">\(\mu\)</span> with a certain level of confidence. To produce such an interval estimate, we consider the following statistic:</p>
<div class="proof theorem admonition" id="standardized-mean-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.2 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. Then the statistic</p>
<div class="math notranslate nohighlight">
\[
Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{m}}
\]</div>
<p>has a standard normal distribution. This statistic is called the <em>standardized mean</em>.</p>
</section>
</div><p>The standardized mean is obtained by first subtracting the expected value <span class="math notranslate nohighlight">\(\mu = E(\overline{X})\)</span> of the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> from itself, and then dividing by the standard error <span class="math notranslate nohighlight">\(\sigma_{\overline{X}} = \sigma/\sqrt{m}\)</span>. The crucial property of the standardized mean is that its distribution does <em>not</em> depend on the unknown “true” value of <span class="math notranslate nohighlight">\(\mu\)</span>. Such a statistic is sometimes called a <em>pivotal quantity</em>.</p>
<p>Now, suppose that we choose a number <span class="math notranslate nohighlight">\(\alpha \in [0,1]\)</span>. Since <span class="math notranslate nohighlight">\(Z\)</span> is a standard normal variable, we have</p>
<div class="math notranslate nohighlight">
\[
P(-z_{\alpha/2} &lt; Z &lt; z_{\alpha/2}) = 1 - \alpha,
\]</div>
<p>where <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> is the critical value defined so that</p>
<div class="math notranslate nohighlight">
\[
\Phi(z_{\alpha/2}) = 1 - \alpha/2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi\)</span> is the cumulative distribution function of <span class="math notranslate nohighlight">\(Z\)</span>. But the inequality</p>
<div class="math notranslate nohighlight">
\[
-z_{\alpha/2} &lt; Z &lt; z_{\alpha/2}
\]</div>
<p>is easily seen to be equivalent to the inequality</p>
<div class="math notranslate nohighlight" id="equation-ci-first-eq">
<span class="eqno">(14.1)<a class="headerlink" href="#equation-ci-first-eq" title="Link to this equation">#</a></span>\[
\overline{X} -z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}} &lt; \mu &lt; \overline{X} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}},
\]</div>
<p>and so we have</p>
<div class="math notranslate nohighlight" id="equation-prob-ci-eq">
<span class="eqno">(14.2)<a class="headerlink" href="#equation-prob-ci-eq" title="Link to this equation">#</a></span>\[
P\left( \overline{X} -z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}} &lt; \mu &lt; \overline{X} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}} \right) = 1 - \alpha.
\]</div>
<p>Interpreting this equality requires some care. First, we must understand that the mean <span class="math notranslate nohighlight">\(\mu\)</span>, although it is unknown to us, is assumed <em>fixed</em> and does not vary. The variable quantity in the inequality <a class="reference internal" href="#equation-ci-first-eq">(14.1)</a> is the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span>, <em>not</em> the mean <span class="math notranslate nohighlight">\(\mu\)</span>. So, when we interpret <a class="reference internal" href="#equation-prob-ci-eq">(14.2)</a> as saying that the probability that <span class="math notranslate nohighlight">\(\mu\)</span> lies in the interval</p>
<div class="math notranslate nohighlight" id="equation-ci-second-eq">
<span class="eqno">(14.3)<a class="headerlink" href="#equation-ci-second-eq" title="Link to this equation">#</a></span>\[
\left( \overline{X} -z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}}, \overline{X} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}}\right)
\]</div>
<p>is <span class="math notranslate nohighlight">\(1-\alpha\)</span>, we must always remember that it is the interval that is random, not <span class="math notranslate nohighlight">\(\mu\)</span>!</p>
<p>Substitution of an observation of the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> into the interval <a class="reference internal" href="#equation-ci-second-eq">(14.3)</a> produces our first example of a <em>confidence interval</em>. Such an observation is nothing but an empirical mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> obtained from a dataset.</p>
<div class="proof definition admonition" id="CI-norm-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.5 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a normal distribution with <strong>unknown</strong> mean <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>known</strong> standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, and let <span class="math notranslate nohighlight">\(\bar{x}\)</span> be the empirical mean computed from an observed dataset. For <span class="math notranslate nohighlight">\(\alpha\in [0,1]\)</span>, the interval</p>
<div class="math notranslate nohighlight" id="equation-ci-third-eq">
<span class="eqno">(14.4)<a class="headerlink" href="#equation-ci-third-eq" title="Link to this equation">#</a></span>\[
\left(\bar{x} - z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}}, \bar{x} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}} \right)
\]</div>
<p>is called a <em><span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> confidence interval</em> for the mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</section>
</div><p>Again, we must remember that the interval <a class="reference internal" href="#equation-ci-third-eq">(14.4)</a> is but <em>one</em> realization of the random interval <a class="reference internal" href="#equation-ci-second-eq">(14.3)</a>. It is therefore <em>not</em> correct to say that the true value <span class="math notranslate nohighlight">\(\mu\)</span> lies in the interval <a class="reference internal" href="#equation-ci-third-eq">(14.4)</a> with probability <span class="math notranslate nohighlight">\(1-\alpha\)</span>. Rather, a frequentist interpretation of probability would tell us that if we repeatedly sampled the true distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> over and over again many times, producing a large number of empirical means <span class="math notranslate nohighlight">\(\bar{x}\)</span> and therefore also a large number of confidence intervals <a class="reference internal" href="#equation-ci-third-eq">(14.4)</a>, then approximately <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> of the confidence intervals would contain the true value <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>In the rest of this section, we shall be interested in generalizing our initial construction of a confidence interval in two ways. First: We would like to have confidence intervals in the (more realistic) scenario when <em>both</em> the mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> of the underlying normal distribution are unknown. This will lead us to confidence intervals involving critical values <span class="math notranslate nohighlight">\(z_{\alpha/2}\)</span> not drawn from a standard normal distribution, but rather from a new type of distribution called a <em><span class="math notranslate nohighlight">\(t\)</span>-distribution</em>. Second: We would like to have confidence intervals for data drawn from <em>any</em> distribution, not only normal distributions. But in order to obtain these confidence intervals, we must assume that our observed datasets are large, enabling us to invoke the Central Limit Theorem. And, besides only working for “large” samples, in this case we must also settle for <em>approximate</em> confidence intervals.</p>
<p>We begin by considering the first generalization, when we have data drawn from a normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> with unknown mean and variance.</p>
<div class="proof definition admonition" id="t-dist-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.6 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\nu\geq 1\)</span> be an integer. A continuous random variable <span class="math notranslate nohighlight">\(X\)</span> is said to have a <em><span class="math notranslate nohighlight">\(t\)</span>-distribution with <span class="math notranslate nohighlight">\(\nu\)</span> degrees of freedom</em>, denoted</p>
<div class="math notranslate nohighlight">
\[
X \sim t_{\nu},
\]</div>
<p>if its probability density function is given by</p>
<div class="math notranslate nohighlight">
\[
f(x;\nu) = \frac{\Gamma\left(\frac{\nu+1}{2} \right)}{\Gamma\left( \frac{\nu}{2} \right) \sqrt{\nu\pi}}\left(1 + \frac{x^2}{\nu} \right)^{- \frac{\nu+1}{2}}
\]</div>
<p>with support <span class="math notranslate nohighlight">\(\bbr\)</span>.</p>
</section>
</div><div class="proof theorem admonition" id="student-mean-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.3 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, and let <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(s\)</span> be the empirical mean and standard deviation computed from an observed random sample. Then the statistic</p>
<div class="math notranslate nohighlight">
\[
T = \frac{\overline{X} - \mu}{s/\sqrt{m}}
\]</div>
<p>has a <span class="math notranslate nohighlight">\(t_{m-1}\)</span> distribution. This statistic is called the <em>studentized mean</em>.</p>
</section>
</div><div class="proof theorem admonition" id="t-CI-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.4 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a normal distribution with <strong>unknown</strong> mean <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>unknown</strong> standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, and let <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(s\)</span> be the empirical mean and standard deviation computed from an observed random sample. For <span class="math notranslate nohighlight">\(\alpha \in [0,1]\)</span>, the interval</p>
<div class="math notranslate nohighlight">
\[
\left( \overline{x} - t_{\alpha/2, m-1} \cdot \frac{s}{\sqrt{m}}, \overline{x} + t_{\alpha/2, m-1} \cdot \frac{s}{\sqrt{m}} \right)
\]</div>
<p>is a <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> confidence interval for the mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</section>
</div><div class="proof theorem admonition" id="clt-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.5 </span> (Central Limit Theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. Then, in the limit as <span class="math notranslate nohighlight">\(m\to \infty\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{m\to \infty} P \left( \frac{\overline{X} - \mu}{\sigma/\sqrt{m}} \leq z \right) = P(Z \leq z) = \Phi(z),
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span>.</p>
</section>
</div><div class="proof theorem admonition" id="CI-large-sample-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.6 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be an IID random sample from a distribution with <strong>unknown</strong> mean <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>known</strong> standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, and let <span class="math notranslate nohighlight">\(\bar{x}\)</span> be the empirical mean computed from an observed random sample. For <span class="math notranslate nohighlight">\(\alpha\in [0,1]\)</span> and <span class="math notranslate nohighlight">\(m\)</span> sufficiently large, the random interval</p>
<div class="math notranslate nohighlight">
\[
\left(\bar{x} - z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}}, \bar{x} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{m}} \right)
\]</div>
<p>is approximately a <span class="math notranslate nohighlight">\(100(1-\alpha)\%\)</span> confidence interval for the mean <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</section>
</div></section>
<section id="hypothesis-tests">
<h2><span class="section-number">14.3. </span>Hypothesis tests<a class="headerlink" href="#hypothesis-tests" title="Link to this heading">#</a></h2>
</section>
<section id="statistical-inference-in-linear-regression">
<h2><span class="section-number">14.4. </span>Statistical inference in linear regression<a class="headerlink" href="#statistical-inference-in-linear-regression" title="Link to this heading">#</a></h2>
<p>We begin our discussion in this section by considering an observed dataset</p>
<div class="math notranslate nohighlight">
\[
y_1,y_2,\ldots,y_m \in \bbr.
\]</div>
<p>As always, the <span class="math notranslate nohighlight">\(y\)</span>’s are conceptualized as observed values of a random variable <span class="math notranslate nohighlight">\(Y\)</span>. The goal of a linear regression model is to “explain” the variability of the <span class="math notranslate nohighlight">\(y\)</span>’s through a linear relationship to (the observed values of) another random variable <span class="math notranslate nohighlight">\(X\)</span>, the <em>explanatory variable</em> or the <em>predictor variable</em>. In this case, <span class="math notranslate nohighlight">\(Y\)</span> is called the <em>response variable</em>.</p>
<p>So, we suppose that our <span class="math notranslate nohighlight">\(y\)</span>’s form part of an observed bivariate dataset</p>
<div class="math notranslate nohighlight">
\[
(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m) \in \bbr^2.
\]</div>
<p>A measure of variability of the <span class="math notranslate nohighlight">\(y\)</span>’s is the empirical variance</p>
<div class="math notranslate nohighlight">
\[
s^2 = \frac{1}{m-1} \sum_{i=1}^m (y_i - \bar{y})^2.
\]</div>
<p>In our analysis, however, it is convenient to measure the variability via the quantity</p>
<div class="math notranslate nohighlight">
\[
SST \def \sum_{i=1}^m (y_i - \bar{y})^2,
\]</div>
<p>called the <em>total sum of squares</em>. The fundamental insight in this section is that the total sum of squares may be decomposed as a sum of two <em>other</em> sums of squares, one which accounts for the variance “explained” via a linear regression model, and the other that accounts for the residual or “unexplained” variance.</p>
<p>To obtain these other sums of squares, we suppose that our (simple) linear regression model has parameters <span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We saw in <a class="reference internal" href="13-learning.html#mle-simple-lin-reg-cor">Corollary 13.1</a> that the maximum likelihood estimates</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_0 \def (\beta_0)_\text{MLE}^\star \quad \text{and} \quad \hat{\beta}_1 \def (\beta_1)_\text{MLE}^\star
\]</div>
<p>for the “true” bias and slope terms <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^m \left(x_i - \bar{x} \right)\left( y_i - \bar{y} \right)}{\sum_{i=1}^m \left(x_i - \bar{x} \right)^2} \quad \text{and} \quad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
\]</div>
<p>We also defined the <em>predicted values</em> and <em>residuals</em> by the equations</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i = \beta_0 + \beta_1 x_i \quad \text{and} \quad r_i = y_i - \hat{y}_i.
\]</div>
<p>These definitions of <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> and <span class="math notranslate nohighlight">\(r_i\)</span> suited our brief analysis in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>. However, in order to obtain their values, we would need to know the “true” values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, which we presumably don’t know! So, our discussion in this section begins with alternate definitions of these quantities based on the MLEs for the bias and slope terms:</p>
<div class="proof definition admonition" id="predict-resid-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.7 </span></p>
<section class="definition-content" id="proof-content">
<p>We define the <em><span class="math notranslate nohighlight">\(i\)</span>-th predicted value</em> and <em><span class="math notranslate nohighlight">\(i\)</span>-th residual</em> to be</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \quad \text{and} \quad r_i = y_i - \hat{y}_i,
\]</div>
<p>for each <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>.</p>
</section>
</div><p>With these new definitions of predicted and residual values in hand, we now define the two other sums of squares mentioned above:</p>
<div class="proof definition admonition" id="ss-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.8 </span></p>
<section class="definition-content" id="proof-content">
<p>We define</p>
<div class="math notranslate nohighlight">
\[
SSE = \sum_{i=1}^m (y_i - \hat{y}_i)^2 \quad \text{and} \quad SSR = \sum_{i=1}^m (\hat{y}_i - \bar{y})^2,
\]</div>
<p>called the <em>error sum of squares</em> and <em>regression sum of squares</em>.</p>
</section>
</div><p>Using these new sums of squares, we state and prove that the total sum of squares may be decomposed as the error and regression sums of squares. This is called the “ANOVA” identity, which is short for <em>analysis of variance</em>.</p>
<div class="proof theorem admonition" id="anova-lr-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.7 </span> (ANOVA identity for linear regression)</p>
<section class="theorem-content" id="proof-content">
<p>There is an equality <span class="math notranslate nohighlight">\(SST = SSE + SSR\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We begin by noting that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
SST &amp;= \sum_{i=1}^m ( y_i - \bar{y})^2 \\
&amp;= \sum_{i=1}^m \left[ (y_i - \hat{y}_i) + (\hat{y}_i - \bar{y}) \right]^2 \\
&amp;= SSE + SSR + 2 \sum_{i=1}^m (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}).
\end{align*}\]</div>
<p>So, all we need to do is show that <span class="math notranslate nohighlight">\(\sum_{i=1}^m (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) =0\)</span>. To do this, we note that</p>
<div class="math notranslate nohighlight">
\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \quad \text{and} \quad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x},
\]</div>
<p>and so</p>
<div class="math notranslate nohighlight">
\[
y_i - \hat{y}_i = (y_i - \bar{y}) - \hat{\beta}_1(x_i - \bar{x}) \quad \text{and} \quad \hat{y}_i - \bar{y} = \hat{\beta}_1 (x_i - \bar{x}).
\]</div>
<p>But then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{i=1}^m (y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) &amp;= \hat{\beta}_1\sum_{i=1}^m \left[(y_i - \bar{y}) - \hat{\beta}_1(x_i - \bar{x}) \right](x_i - \bar{x}) \\
&amp;= \hat{\beta}_1\sum_{i=1}^m (y_i - \bar{y})(x_i - \bar{x}) - \hat{\beta}_1^2 \sum_{i=1}^m (x_i - \bar{x})^2\\
&amp;= \hat{\beta}_1\sum_{i=1}^m (y_i - \bar{y})(x_i - \bar{x}) - \hat{\beta}_1 \sum_{i=1}^m (y_i - \hat{y})(x_i - \bar{x})\\
&amp;= 0
\end{align*}\]</div>
<p>where we used the equality</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^m (y_i - \bar{y})(x_i - \bar{x})}{\sum_{i=1}^m (x_i - \bar{x})^2}
\]</div>
<p>in moving from the second to the third line. Q.E.D.</p>
</div>
<p>The error sum of squares</p>
<div class="math notranslate nohighlight">
\[
SSE = \sum_{i=1}^m (y_i - \hat{y}_i)^2
\]</div>
<p>is easily conceptualized as the amount of error accumulated in using the predicted values <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> from the linear regression model as proxies for the true values <span class="math notranslate nohighlight">\(y_i\)</span>. In this sense, the error sum of squares quantifies the variance in the <span class="math notranslate nohighlight">\(y\)</span>’s that is left “unexplained” by the linear regression model. Since the total sum of squares is the total variance in the <span class="math notranslate nohighlight">\(y\)</span>’s, the ANOVA identity shows that the regression sum of squares</p>
<div class="math notranslate nohighlight">
\[
SSR = \sum_{i=1}^m (\hat{y}_i - \bar{y})^2
\]</div>
<p>should quantify the variance in the <span class="math notranslate nohighlight">\(y\)</span>’s that <em>is</em> “explained” by the model. These considerations motivate the definition of the following quantity, which is the proportion of total variance of the <span class="math notranslate nohighlight">\(y\)</span>’s that is “explained” by the model:</p>
<div class="proof definition admonition" id="coeff-det-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.9 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>coefficient of determination</em>, denoted <span class="math notranslate nohighlight">\(R^2\)</span>, is given by</p>
<div class="math notranslate nohighlight">
\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}.
\]</div>
</section>
</div><p>Note that <span class="math notranslate nohighlight">\(R^2\)</span> always lies between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, by the ANOVA identity. Values closer to <span class="math notranslate nohighlight">\(1\)</span> indicate that a large portion of the total variance in the <span class="math notranslate nohighlight">\(y\)</span>’s is “explained” by the linear regression model, which means that the regression line should fit the data well. In the other direction, values of <span class="math notranslate nohighlight">\(R^2\)</span> close to <span class="math notranslate nohighlight">\(0\)</span> should mean that the regression line is a poor fit for the data.</p>
<p>However, as we learned in <a class="reference internal" href="08-more-prob.html#covar-correl-sec"><span class="std std-numref">Section 8.5</span></a>, the correlation coefficient <span class="math notranslate nohighlight">\(\rho\)</span> is a measure of the strength of the linear relationship between two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Moreover, given an observed bivariate dataset</p>
<div class="math notranslate nohighlight">
\[
(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m) \in \bbr^2,
\]</div>
<p>we learned in a previous <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_08.ipynb">programming assignment</a> that the quantity</p>
<div class="math notranslate nohighlight">
\[
r \def \frac{\sum_{i=1}^m (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^m (x_i - \bar{x})^2 \sum_{i=1}^m (y_i - \bar{y})^2}}
\]</div>
<p>serves as an empirical estimate of <span class="math notranslate nohighlight">\(\rho\)</span>. The empirical correlation <span class="math notranslate nohighlight">\(r\)</span> must lie between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, with negative values indicating a linear relationship between the <span class="math notranslate nohighlight">\(x\)</span>’s and <span class="math notranslate nohighlight">\(y\)</span>’s with negative slope. Thus, the square <span class="math notranslate nohighlight">\(r^2\)</span> must lie between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, and it measures the strength of <em>any</em> type of linear relationship, whether positive or negative slope. In fact, we have:</p>
<div class="proof theorem admonition" id="r-R-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.8 </span></p>
<section class="theorem-content" id="proof-content">
<p>There is an equality <span class="math notranslate nohighlight">\(r^2 = R^2\)</span>.</p>
</section>
</div><p>Before beginning the proof, it will be convenient to introduce some notation. We set:</p>
<div class="math notranslate nohighlight">
\[
s_{xy} = \sum_{i=1}^m (x_i - \bar{x})(y_i - \bar{y}), \quad s_{xx} = \sum_{i=1}^m (x_i - \bar{x})^2, \quad s_{yy} = \sum_{i=1}^m (y_i - \bar{y})^2.
\]</div>
<p>Then, we have</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{s_{xy}}{s_{xx}}, \quad r = \frac{s_{xy}}{\sqrt{s_{xx} s_{yy}}}, \quad \text{and} \quad SST = s_{yy}.
\]</div>
<p>With this notation set, we prove the theorem:</p>
<div class="proof admonition" id="proof">
<p>Proof. First, recall from the proof of <a class="reference internal" href="#anova-lr-thm">Theorem 14.7</a> that <span class="math notranslate nohighlight">\(\hat{y}_i - \bar{y} = \hat{\beta}_1(x_i - \bar{x})\)</span>. Thus:</p>
<div class="math notranslate nohighlight">
\[
SSR = \sum_{i=1}^m (\hat{y}_i - \bar{y})^2 = \hat{\beta}_1^2 \sum_{i=1}^m(x_i - \bar{x})^2 = \frac{s_{xy}^2}{s_{xx}^2} \cdot s_{xx} = \frac{s^2_{xy}}{s_{xx}}.
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
R^2 = \frac{SSR}{SST} = \frac{s_{xy}^2 / s_{xx}}{s_{yy}} = \frac{s_{xy}^2}{s_{xx}s_{yy}} = r^2,
\]</div>
<p>as desired. Q.E.D.</p>
</div>
<p>We now turn toward inference problems in linear regression centered on the slope coefficient</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{s_{xy}}{s_{xx}} = \frac{\sum_{i=1}^m (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^m (x_i - \bar{x})^2}.
\]</div>
<p>If we conceptualize the <span class="math notranslate nohighlight">\(y\)</span>’s as observed values of an IID random sample</p>
<div class="math notranslate nohighlight">
\[
Y_1,Y_2,\ldots,Y_m,
\]</div>
<p>then we obtain the <em>slope estimator</em>, also denoted <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, by putting</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^m (x_i - \bar{x})(Y_i - \overline{Y})}{\sum_{i=1}^m (x_i - \bar{x})^2}.
\]</div>
<p>Likewise, the <em>bias estimator</em>, or <em>intercept estimator</em>, is given by</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \bar{x}.
\]</div>
<p>Our goal over the rest of this section is to compute confidence intervals for the slope coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>We begin with an estimator of the “true” standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> in our linear regression model:</p>
<div class="proof definition admonition" id="rse-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.10 </span></p>
<section class="definition-content" id="proof-content">
<p>For a dataset of size <span class="math notranslate nohighlight">\(m\)</span>, we define the <em>residual standard error</em> to be the quantity</p>
<div class="math notranslate nohighlight">
\[
s_e = \sqrt{\frac{SSE}{m-2}} = \sqrt{\frac{\sum_{i=1}^m (y_i - \hat{y}_i)^2}{m-2}}.
\]</div>
<p>Replacing the <span class="math notranslate nohighlight">\(y_i\)</span>’s with <span class="math notranslate nohighlight">\(Y_i\)</span>’s yields the esimator</p>
<div class="math notranslate nohighlight">
\[
S_e = \sqrt{\frac{\sum_{i=1}^m (Y_i - \hat{Y}_i)^2}{m-2}} = \sqrt{\frac{\sum_{i=1}^m (Y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2}{m-2}}.
\]</div>
</section>
</div><p>The denominator <span class="math notranslate nohighlight">\(m-2\)</span> appears (rather than <span class="math notranslate nohighlight">\(m-1\)</span>) in order to make the square <span class="math notranslate nohighlight">\(S^2_e\)</span> an unbiased estimator of the “true” variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> of the linear regression model.</p>
<div class="proof theorem admonition" id="slope-estimator-prop-thm">
<p class="admonition-title"><span class="caption-number">Theorem 14.9 </span></p>
<section class="theorem-content" id="proof-content">
<ol class="arabic">
<li><p>The slope estimator <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\beta_1\)</span>, i.e.,</p>
<div class="math notranslate nohighlight">
\[
    E(\hat{\beta}_1) = \beta_1.
    \]</div>
</li>
<li><p>Letting <span class="math notranslate nohighlight">\(\sigma\)</span> be the “true” standard deviation of the linear regression model, the standard error of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is</p>
<div class="math notranslate nohighlight">
\[
    \sigma_{\hat{\beta}_1} = \frac{\sigma}{\sqrt{s_{xx}}}.
    \]</div>
<p>Replacing <span class="math notranslate nohighlight">\(\sigma\)</span> with its estimate via the residual standard error <span class="math notranslate nohighlight">\(s_e\)</span>, an estimate of the standard error of <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is</p>
<div class="math notranslate nohighlight">
\[
    \hat{\sigma}_{\hat{\beta}_1} = \frac{s_e}{\sqrt{s_{xx}}}.
    \]</div>
</li>
</ol>
</section>
</div></section>
<section id="analysis-of-variance">
<h2><span class="section-number">14.5. </span>Analysis of variance<a class="headerlink" href="#analysis-of-variance" title="Link to this heading">#</a></h2>
<div class="proof definition admonition" id="categorical-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.11 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(n\geq 1\)</span> be an integer and let <span class="math notranslate nohighlight">\(\btheta^\intercal =(\theta_1,\ldots,\theta_n)\)</span> be a vector such that <span class="math notranslate nohighlight">\(0\leq \theta_j \leq 1\)</span> for each <span class="math notranslate nohighlight">\(j=1,\ldots,n\)</span> and <span class="math notranslate nohighlight">\(\sum_{j=1}^n \theta_j = 1\)</span>. A discrete random variable <span class="math notranslate nohighlight">\(X\)</span> is said to have a <em>categorical distribution</em> with parameter <span class="math notranslate nohighlight">\(\btheta\)</span>, denoted</p>
<div class="math notranslate nohighlight">
\[
X \sim \Cat(\btheta),
\]</div>
<p>if its probability mass function is given by</p>
<div class="math notranslate nohighlight">
\[
p(x;\btheta) = \theta_x
\]</div>
<p>if <span class="math notranslate nohighlight">\(x\in \{1,2,\ldots,n\}\)</span>, and <span class="math notranslate nohighlight">\(p(x;\btheta)=0\)</span> otherwise.</p>
</section>
</div><div class="proof definition admonition" id="anova-defn">
<p class="admonition-title"><span class="caption-number">Definition 14.12 </span></p>
<section class="definition-content" id="proof-content">
<p>An <em>analysis of variance model</em>, or <em>ANOVA model</em>, is a probabilistic graphical model whose underlying graph is of the form</p>
<a class="reference internal image-reference" href="../_images/anova.svg"><img alt="../_images/anova.svg" class="align-center" src="../_images/anova.svg" width="25%" /></a>
<p> </p>
<p>The model has the following parameters:</p>
<ul class="simple">
<li><p>A parameter vector <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> such that <span class="math notranslate nohighlight">\(X \sim \Cat(\btheta)\)</span>.</p></li>
<li><p>A parameter vector <span class="math notranslate nohighlight">\(\bmu \in \bbr^n\)</span>.</p></li>
<li><p>A positive real parameter <span class="math notranslate nohighlight">\(\sigma^2&gt;0\)</span>.</p></li>
</ul>
<p>The link function at <span class="math notranslate nohighlight">\(Y\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
Y \mid X \sim \mathcal{N}(\mu,\sigma^2), \quad \text{where} \quad \mu = \mu(x) = \mu_x,
\]</div>
<p>and <span class="math notranslate nohighlight">\(\bmu^\intercal = (\mu_1,\mu_2,\ldots,\mu_{n})\)</span>.</p>
</section>
</div></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="13-learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="bib.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">15. </span>Bibliography</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics-estimators-and-their-distributions">14.1. Statistics, estimators, and their distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">14.2. Confidence intervals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-tests">14.3. Hypothesis tests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-inference-in-linear-regression">14.4. Statistical inference in linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis-of-variance">14.5. Analysis of variance</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>