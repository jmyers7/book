

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>11. Learning &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"Ber": "\\mathcal{B}er", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "btheta": "\\boldsymbol\\theta", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/learning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12. Statistics and estimators" href="stats-estimators.html" />
    <link rel="prev" title="10. Probabilistic models" href="models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">9. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">10. Probabilistic models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats-estimators.html">12. Statistics and estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="asymptotic.html">13. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="more-samp-dist.html">14. More sampling distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-based-learning-objectives">11.1. Likelihood-based learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-linear-regression-models">11.2. Maximum likelihood estimation for linear regression models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-for-gaussian-mixture-models">11.3. Expectation maximization for Gaussian mixture models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="learning">
<span id="id1"></span><h1><span class="section-number">11. </span>Learning<a class="headerlink" href="#learning" title="Permalink to this heading">#</a></h1>
<p>In the <a class="reference internal" href="models.html#prob-models"><span class="std std-ref">last chapter</span></a>, we studied general probabilistic models and described several specific and important examples. These descriptions included careful identifications of the parameters of the models, but the question was left open concerning exactly <em>how</em> these parameters are chosen in practice. To cut straight to the chase:</p>
<blockquote>
<div><p>The goal is to <em>learn</em> the parameters of a model based on an observed dataset.</p>
</div></blockquote>
<p>The actual implementation of a concrete learning procedure is called a <em>learning algorithm</em> by machine learning researchers and engineers, and they will refer to <em>training</em> or <em>fitting</em> a model. Statisticians refer to learning as <em>parameter estimation</em>. But no matter what you call them, the values of the parameters that these learning procedures seek are very often solutions to some sort of optimization problem. Intuitively, we want to choose parameters to minimize the “distance” between the model probability distribution and the empirical probability distribution of the dataset:</p>
<a class="reference internal image-reference" href="../_images/prob-distance.svg"><img alt="../_images/prob-distance.svg" class="align-center" src="../_images/prob-distance.svg" width="75%" /></a>
<p> </p>
<p>How one precisely defines and measures “distance” (or “discrepancy”) is essentially a matter of choosing an objective function to minimize. Some learning algorithms we will study below are actually posed as maximization problems, but these may be reframed as minimization problems via the usual trick of replacing the objective function with its negative.</p>
<p>So, our first goal in this chapter is to describe objective functions for parameter learning. In some form or fashion, all these objectives will involve the data and model probability functions described in <a class="reference internal" href="models.html#prob-models"><span class="std std-numref">Chapter 10</span></a>, though these functions will be called <em>likelihood functions</em> in this chapter. Thus, all the learning algorithms in this book are <em>likelihood based</em>. For some simple models, the solutions to these optimization problems may be obtained in closed form; for others, the gradient-based optimization algorithms that we studied in <a class="reference internal" href="optim.html#optim"><span class="std std-numref">Chapter 9</span></a> are required to obtain approximate solutions.</p>
<p>Our focus in this chapter is using likelihood-based learning algorithms in a framework inspired by machine learning practice; in the chapters that follow, we will turn toward theoretical and statistical properties of likelihood-based parameter estimators in a more traditional statistics-based context.</p>
<section id="likelihood-based-learning-objectives">
<h2><span class="section-number">11.1. </span>Likelihood-based learning objectives<a class="headerlink" href="#likelihood-based-learning-objectives" title="Permalink to this heading">#</a></h2>
<p>To help motivate likelihood-based learning objectives, let’s begin with a simple example. Suppose that we flip a coin <span class="math notranslate nohighlight">\(m\geq 1\)</span> times and let <span class="math notranslate nohighlight">\(x^{(i)}\)</span> be the number of heads obtained on the <span class="math notranslate nohighlight">\(i\)</span>-th toss; thus, <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is an observed value of a random variable</p>
<div class="math notranslate nohighlight">
\[
X \sim \Ber(\theta).
\]</div>
<p>This is a very simple example of a probabilistic graphical model whose underlying graph consists of only two nodes, one for the parameter <span class="math notranslate nohighlight">\(\theta\)</span> and one for the (observed) random variable <span class="math notranslate nohighlight">\(X\)</span>:</p>
<a class="reference internal image-reference" href="../_images/bern-pgm.svg"><img alt="../_images/bern-pgm.svg" class="align-center" src="../_images/bern-pgm.svg" width="17%" /></a>
<p> </p>
<p>Our observations together form a dataset of size <span class="math notranslate nohighlight">\(m\)</span>:</p>
<div class="math notranslate nohighlight">
\[
x^{(1)},\ldots,x^{(m)} \in \{0,1\}.
\]</div>
<p>Based on this dataset, our goal is to <em>learn</em> an optimal value for <span class="math notranslate nohighlight">\(\theta\)</span> that minimizes the discrepancy between the model distribution and the empirical distribution of the dataset. To do this, it will be convenient to introduce the sum</p>
<div class="math notranslate nohighlight" id="equation-sum-dep-eqn">
<span class="eqno">(11.1)<a class="headerlink" href="#equation-sum-dep-eqn" title="Permalink to this equation">#</a></span>\[
\Sigma x \def x^{(1)} + \cdots + x^{(m)}
\]</div>
<p>which counts the total number of heads seen during the <span class="math notranslate nohighlight">\(m\)</span> flips of the coin. To make this concrete, suppose that <span class="math notranslate nohighlight">\(m=10\)</span> and <span class="math notranslate nohighlight">\(\Sigma x=7\)</span>, so that we see seven heads over ten flips. Then, intuition suggests that <span class="math notranslate nohighlight">\(\theta=0.7\)</span> would be a “more optimal” estimate for the parameter then, say, <span class="math notranslate nohighlight">\(\theta=0.1\)</span>. Indeed, if <span class="math notranslate nohighlight">\(\theta=0.1\)</span>, we would expect it highly unlikely to observe seven heads over ten flips when there is only a one-in-ten chance of seeing a head on a single flip.</p>
<p>We may confirm our hunch by actually computing probabilities. Assuming, as always, that the observations in the dataset are independent, we have</p>
<div class="math notranslate nohighlight" id="equation-likelihood-bern-eqn">
<span class="eqno">(11.2)<a class="headerlink" href="#equation-likelihood-bern-eqn" title="Permalink to this equation">#</a></span>\[
p\big(x^{(1)},\ldots,x^{(m)};\theta\big) = \prod_{i=1}^m \theta^{x^{(i)}}(1-\theta)^{1-x^{(i)}} = \theta^x (1-\theta)^{m-\Sigma x}.
\]</div>
<p>Notice that the value of the joint mass function depends only on the sum <a class="reference internal" href="#equation-sum-dep-eqn">(11.1)</a>. If this sum is <span class="math notranslate nohighlight">\(\Sigma x=7\)</span> and we have <span class="math notranslate nohighlight">\(m=10\)</span> and <span class="math notranslate nohighlight">\(\theta=0.1\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
p\big(x^{(1)},\ldots,x^{(m)};\theta=0.1\big) = 0.1^{7} (1-0.1)^{10-7} = 7.29 \times 10^{-8}.
\]</div>
<p>On the other hand, when <span class="math notranslate nohighlight">\(\theta=0.7\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
p\big(x^{(1)},\ldots,x^{(m)};\theta=0.7\big) = 0.7^{7} (1-0.7)^{10-7} \approx 2.22 \times 10^{-3}.
\]</div>
<p>Thus, it is five orders of magnitude more likely to observe a dataset with <span class="math notranslate nohighlight">\(x=7\)</span> for <span class="math notranslate nohighlight">\(\theta=0.7\)</span> compared to <span class="math notranslate nohighlight">\(\theta=0.1\)</span>. In fact, when <span class="math notranslate nohighlight">\(\Sigma x=7\)</span> and <span class="math notranslate nohighlight">\(m=10\)</span>, the value <span class="math notranslate nohighlight">\(\theta = 0.7\)</span> is a global maximizer of <a class="reference internal" href="#equation-likelihood-bern-eqn">(11.2)</a> as a function of <span class="math notranslate nohighlight">\(\theta\)</span>, which may be verified by inspecting the graph:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.distributions.multivariate_normal</span> <span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c54af72b613e185581ca68c37e08fbfb0f935c4c4e053b3f8d78e3aec888d123.svg" src="../_images/c54af72b613e185581ca68c37e08fbfb0f935c4c4e053b3f8d78e3aec888d123.svg" /></figure>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Warning</strong>: Note that the likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> is <strong>not</strong> a probability density function over <span class="math notranslate nohighlight">\(\theta\)</span>!</p>
</aside>
<p>Note the label along the vertical axis; when the dataset is held fixed, the values of the joint mass function <a class="reference internal" href="#equation-likelihood-bern-eqn">(11.2)</a> as a function of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> are referred to as <em>likelihoods</em>. This function is called the <em>data likelihood function</em> and is denoted</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\big(\theta;x^{(1)},\ldots,x^{(m)}\big) = p\big(x^{(1)},\ldots,x^{(m)};\theta\big).
\]</div>
<p>When the dependence of the likelihood function on the dataset does not need to be explicitly indicated, we shall often simply write <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p>
<p>Thus, we see that the parameter <span class="math notranslate nohighlight">\(\theta = 0.7\)</span> is a solution to the optimization problem that consists of maximizing the likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>. This is a simple example of <em>maximum likelihood estimation</em>, or <em>MLE</em>.</p>
<p>We see from <a class="reference internal" href="#equation-likelihood-bern-eqn">(11.2)</a> that the data likelihood function is a product of probabilities. Thus, if <span class="math notranslate nohighlight">\(m\)</span> is very large, the values of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> will be very small. For example, in the case that <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\Sigma x=70\)</span> (which are still quite small values), we get the following plot:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">70</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/542ded0a37a662a0daaaa44011b2f2b99149417fbd326d0f6ec2291d6d65546c.svg" src="../_images/542ded0a37a662a0daaaa44011b2f2b99149417fbd326d0f6ec2291d6d65546c.svg" /></figure>
</div>
</div>
<p>This often leads to difficulties when implementing MLE in computer algorithms due to numerical round-off. The machine is liable to round very small numbers to <span class="math notranslate nohighlight">\(0\)</span>. For this reason (and others), we often work with the (base-<span class="math notranslate nohighlight">\(e\)</span>) logarithm of the data likelihood function, denoted by</p>
<div class="math notranslate nohighlight">
\[
\ell\big(\theta; x^{(1)},\ldots,x^{(m)}\big) \def \log{\mathcal{L}\big(\theta; x^{(1)},\ldots,x^{(m)}\big)}.
\]</div>
<p>This is called the <em>data log-likelihood function</em>. As with the data likelihood function, if the dataset does not need to be explicitly mentioned, we will often write <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>.</p>
<p>MLE is the optimization problem with the data likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> as the objective function. But it is not hard to prove (see the suggested problems for this section) that the maximizers of the data likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> are the <em>same</em> as the maximizers of the data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>. For our Bernoulli model with <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\Sigma x=70\)</span>, a visual comparison of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> is given in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;log-likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f9d310c677bc3b45e343bc96a8072254bff569955bcd4ed4caf912926f385e73.svg" src="../_images/f9d310c677bc3b45e343bc96a8072254bff569955bcd4ed4caf912926f385e73.svg" /></figure>
</div>
</div>
<p>Notice that the values of <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> are on a much more manageable scale compared to the values of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, and that the two functions have the same global maximizer at <span class="math notranslate nohighlight">\(\theta=0.7\)</span>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The acronym <em>MLE</em> often serves double duty: It stands for the procedure of <em>maximum likelihood estimation</em>, but it also sometimes stands for the results of this procedure, called <em>maximum likelihood estimates</em>.</p>
</aside>
<p>Using the data log-likelihood function as the objective, we may easily compute the MLE in closed form for our Bernoulli model:</p>
<div class="proof theorem admonition" id="bern-mle-1-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.1 </span> (MLE for the Bernoulli model, part 1)</p>
<section class="theorem-content" id="proof-content">
<p>Consider the Bernoulli model described above and suppose that <span class="math notranslate nohighlight">\(0 &lt; \Sigma x &lt; m\)</span>. The (unique) global maximizer <span class="math notranslate nohighlight">\(\theta^\star\)</span> of the data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> over <span class="math notranslate nohighlight">\(\theta \in (0,1)\)</span> is given by <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span>. Thus, <span class="math notranslate nohighlight">\(\theta^\star=\Sigma x/m\)</span> is the maximum likelihood estimate.</p>
</section>
</div><p>To begin the simple proof, first note that</p>
<div class="math notranslate nohighlight" id="equation-data-log-like-bern-eqn">
<span class="eqno">(11.3)<a class="headerlink" href="#equation-data-log-like-bern-eqn" title="Permalink to this equation">#</a></span>\[
\ell(\theta) = \Sigma x \log{\theta} + (m-\Sigma x) \log{(1-\theta)}
\]</div>
<p>from <a class="reference internal" href="#equation-likelihood-bern-eqn">(11.2)</a>.  As you well know, the maximizers of <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> over <span class="math notranslate nohighlight">\((0,1)\)</span> must occur at points where <span class="math notranslate nohighlight">\(\ell'(\theta)=0\)</span>. But</p>
<div class="math notranslate nohighlight">
\[
\ell'(\theta) = \frac{\Sigma x}{\theta} - \frac{m-\Sigma x}{1-\theta},
\]</div>
<p>and a little algebra yields the solution <span class="math notranslate nohighlight">\(\theta = \Sigma x/m\)</span> to the equation <span class="math notranslate nohighlight">\(\ell'(\theta)=0\)</span>. To confirm that <span class="math notranslate nohighlight">\(\theta = \Sigma x/m\)</span> is a global maximizer over <span class="math notranslate nohighlight">\((0,1)\)</span>, note that the second derivatives of both <span class="math notranslate nohighlight">\(\log{\theta}\)</span> and <span class="math notranslate nohighlight">\(\log{(1-\theta)}\)</span> are always negative, and hence <span class="math notranslate nohighlight">\(\ell''(\theta)&lt;0\)</span> as well since <span class="math notranslate nohighlight">\(\Sigma x\)</span> and <span class="math notranslate nohighlight">\(m-\Sigma x\)</span> are positive (this is a manifestation of <a class="reference external" href="https://en.wikipedia.org/wiki/Concave_function">concavity</a>). Thus, <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span> must be the (unique) global maximizer of <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>.</p>
<p>Note that the data likelihood function</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\big(\theta; x^{(1)},\ldots,x^{(m)}\big) = p\big(x^{(1)},\ldots,x^{(m)};\theta\big)
\]</div>
<p>is exactly the <em>data joint probability function</em>, in the language of <a class="reference internal" href="models.html#prob-models"><span class="std std-numref">Chapter 10</span></a>. The latter is the product</p>
<div class="math notranslate nohighlight">
\[
p\big(x^{(1)},\ldots,x^{(m)};\theta\big) = \prod_{i=1}^m p\big(x^{(i)};\theta\big)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
p(x;\theta) = \theta^x (1-\theta)^{1-x}
\]</div>
<p>is the <em>model probability function</em>. As a function of <span class="math notranslate nohighlight">\(\theta\)</span> with <span class="math notranslate nohighlight">\(x\)</span> held fixed, we call</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta; x) \stackrel{\text{def}}{=} p(\theta; x)
\]</div>
<p>the <em>model likelihood function</em> and</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta;x) \stackrel{\text{def}}{=} \log{\mathcal{L}(\theta;x)}
\]</div>
<p>the <em>model log-likelihood function</em>. When the data point <span class="math notranslate nohighlight">\(x\)</span> does not need to be mentioned explicitly, we will write <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> in place of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta;x)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span>. Note that this clashes with our usage of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> to represent the <em>data</em> likelihood and log-likelihood functions when the dataset is not made explicit. You will need to rely on context to clarify which of the two types of likelihood functions (data or model) is meant when we write <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> or <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>.</p>
<p>It will be convenient to describe an optimization problem involving the <em>model</em> likelihood function that is equivalent to MLE. Here, <em>equivalence</em> means that the two optimization problems have the same solutions. This new (but equivalent!) optimization problem is appealing in part because it directly uses the empirical probability distribution of the dataset and thus more closely aligns us with the intuitive scheme described in the introduction to this chapter, that the goal of parameter learning is to minimize the “distance” (or “discrepancy”) between the model distribution and the empirical distribution. This optimization problem is also useful because it opens the door for the <em>stochastic gradient descent algorithm</em> from <a class="reference internal" href="optim.html#optim"><span class="std std-numref">Chapter 9</span></a> when closed form solutions are not available.</p>
<p>To describe the new optimization problem, let’s consider again our Bernoulli model. Let <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> be the empirical mass function of the dataset</p>
<div class="math notranslate nohighlight">
\[
x^{(1)},\ldots,x^{(m)} \in \{0,1\}.
\]</div>
<p>Thus, in general we have</p>
<div class="math notranslate nohighlight">
\[
\hat{p}(x) = \frac{\text{number of data points $x^{(i)}$ that match $x$}}{m}
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\in \bbr\)</span>, but for our particular Bernoulli model, this simplifies to</p>
<div class="math notranslate nohighlight">
\[
\hat{p}(0) = \frac{m-\Sigma x}{m} \quad \text{and} \quad \hat{p}(1) = \frac{\Sigma x}{m},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma x=x^{(1)} + \cdots + x^{(m)}\)</span>. Letting <span class="math notranslate nohighlight">\(\widehat{X}\)</span> be a Bernoulli random variable with <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> as its mass function, we consider the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \stackrel{\text{def}}{=} E \big( \ell\big(\theta; \widehat{X} \big) \big),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> is the model log-likelihood function. Note that</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \ell(\theta;1) \hat{p}(1)+ \ell(\theta; 0) \hat{p}(0) = \frac{1}{m} \left[ \Sigma x \log{\theta}  + (m-\Sigma x)\log{(1-\theta)} \right],
\]</div>
<p>and so by comparison with <a class="reference internal" href="#equation-data-log-like-bern-eqn">(11.3)</a> we see that the stochastic objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> differs from the data log-likelihood function <span class="math notranslate nohighlight">\(\ell\big( \theta; x^{(1)},\ldots,x^{(m)}\big)\)</span> only by a constant factor of <span class="math notranslate nohighlight">\(1/m\)</span>. Therefore, MLE is equivalent to the optimization problem with <span class="math notranslate nohighlight">\(J(\theta)\)</span> as an objective function, where <em>equivalence</em> means that the two problems have the same solutions. Thus:</p>
<div class="proof theorem admonition" id="bern-mle-2-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.2 </span> (MLE for the Bernoulli model, part 2)</p>
<section class="theorem-content" id="proof-content">
<p>Consider the Bernoulli model described above, suppose that <span class="math notranslate nohighlight">\(0 &lt; \Sigma x &lt; m\)</span>, and let <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> be the empirical mass function of a dataset. The (unique) maximum likelihood estimate <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span> is the global maximizer for the optimization problem with the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = E \big( \ell\big(\theta; \widehat{X} \big) \big),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> is the model log-likelihood function and <span class="math notranslate nohighlight">\(\widehat{X} \sim \hat{p}(x)\)</span>.</p>
</section>
</div><p>Then, by combining <a class="reference internal" href="#bern-mle-1-thm">Theorem 11.1</a> and <a class="reference internal" href="#bern-mle-2-thm">Theorem 11.2</a>, we conclude:</p>
<div class="proof theorem admonition" id="three-mle-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.3 </span> (Three ways to obtain an MLE)</p>
<section class="theorem-content" id="proof-content">
<p>The maximum likelihoood estimate for the Bernoulli model may be obtained by maximizing the data likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, the data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>, or the stochastic objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span>.</p>
</section>
</div><p>Our description of likelihood-based learning methods thus far has focused on the simple Bernoulli model. However, from this specific and simple case I am hoping that you can see an outline of a general method able to fit <em>any</em> probabilistic model to data. In particular, <em>all</em> the models we studied in the <a class="reference internal" href="models.html#prob-models"><span class="std std-ref">previous chapter</span></a> have model and data likelihood functions, and thus these general methods apply to them.</p>
<p>To be more precise, we need to distinguish between two types of models. The <a class="reference internal" href="models.html#lin-reg-sec"><span class="std std-ref">linear regression</span></a>, <a class="reference internal" href="models.html#log-reg-sec"><span class="std std-ref">logistic regression</span></a>, and <a class="reference internal" href="models.html#nn-sec"><span class="std std-ref">neural network models</span></a> that we studied in the previous chapter will all be trained as <em>fully-observed discriminative models</em>. This means two things: (1) all stochastic nodes in the underlying graphs are observed, and (2) the likelihood functions are obtained from the <em>conditional</em> probability functions of the models. The <a class="reference internal" href="models.html#gmm-sec"><span class="std std-ref">Gaussian mixture models</span></a> will be trained as <em>partially-observed generative models</em>, which means that the straightfoward MLE algorithm will need to be replaced with the <em>expectation maximization</em> (or <em>EM</em>) algorithm. We will address the EM algorithm separately in <a class="reference internal" href="#em-gmm-sec"><span class="std std-numref">Section 11.3</span></a> below.</p>
<p>To describe the MLE algorithm for the first type of models, we need to define the likelihood-based objective functions. Note that these functions are all defined in terms of the data and model conditional probability functions from <a class="reference internal" href="models.html#prob-models"><span class="std std-numref">Section 10</span></a>.</p>
<div class="proof definition admonition" id="mle-training-objectives-def">
<p class="admonition-title"><span class="caption-number">Definition 11.1 </span> (MLE training objectives for fully-observed discriminative models)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\btheta\in \bbr^{k}\)</span> be the parameter vector of a fully-observed discriminative model and let <span class="math notranslate nohighlight">\(\hat{p}(\bx,y)\)</span> be the empirical mass function of a dataset</p>
<div class="math notranslate nohighlight">
\[
(\bx^{(1)},y^{(1)}),\ldots,(\bx^{(m)},y^{(m)}) \in \bbr^{1\times n} \times \bbr.
\]</div>
<ol class="arabic">
<li><p>Define the <em>data likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) \def p \big(y^{(1)},\ldots,y^{(m)} \mid \bx^{(1)},\ldots,\bx^{(m)}; \ \btheta \big)
    \]</div>
<p>and the <em>model likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\btheta; \ \bx, y) \def p ( y \mid \bx ; \ \btheta) 
    \]</div>
</li>
<li><p>Define the <em>data log-likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \ell\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) \def \log{\mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big)}.
    \]</div>
<p>and the <em>model log-likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \ell(\btheta; \ \bx, y) \def \log{\mathcal{L}(\btheta; \ \bx, y)}.
    \]</div>
</li>
<li><p>Define the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
    J\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) \def E \big( \ell\big(\btheta; \widehat{\bX}, \widehat{Y}\big) \big),
    \]</div>
<p>where <span class="math notranslate nohighlight">\((\widehat{\bX}, \widehat{Y}) \sim \hat{p}(\bx, y)\)</span>.</p>
</li>
</ol>
</section>
</div><p>From independence of the dataset, we obtain the following expressions for these training objectives:</p>
<div class="proof theorem admonition" id="theorem-4">
<p class="admonition-title"><span class="caption-number">Theorem 11.4 </span> (Formulas for MLE training objectives)</p>
<section class="theorem-content" id="proof-content">
<p>Let the notation be as in <a class="reference internal" href="#mle-training-objectives-def">Definition 11.1</a>. We have</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) = \prod_{i=1}^m \mathcal{L}\big( \btheta;\ \bx^{(i)}, y^{(i)} \big)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-log-like-simp-eqn">
<span class="eqno">(11.4)<a class="headerlink" href="#equation-log-like-simp-eqn" title="Permalink to this equation">#</a></span>\[
\ell\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) = \sum_{i=1}^m \ell\big(\btheta; \ \bx^{(i)}, y^{(i)} \big)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-stochastic-simp-eqn">
<span class="eqno">(11.5)<a class="headerlink" href="#equation-stochastic-simp-eqn" title="Permalink to this equation">#</a></span>\[
J\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) = \frac{1}{m} \sum_{i=1}^m \ell\big(\btheta; \ \bx^{(i)}, y^{(i)} \big).
\]</div>
</section>
</div><p>We now state the MLE algorithm for fully-observed discriminative models; note the similarity to <a class="reference internal" href="#three-mle-thm">Theorem 11.3</a>.</p>
<div class="proof definition admonition" id="definition-5">
<p class="admonition-title"><span class="caption-number">Definition 11.2 </span> (Maximum likelihood estimation for fully-observed discriminative models)</p>
<section class="definition-content" id="proof-content">
<p>Let the notation be as in <a class="reference internal" href="#mle-training-objectives-def">Definition 11.1</a>. A <em>maximum likelihood estimate</em> is a parameter vector <span class="math notranslate nohighlight">\(\btheta^\star\)</span> that is a solution to one of the following three equivalent optimization problems:</p>
<ol class="arabic simple">
<li><p>Maximize the data likelihood function</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  \mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big).
  \]</div>
<ol class="arabic simple" start="2">
<li><p>Maximize the data log-likelihood function</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  \ell\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big).
  \]</div>
<ol class="arabic simple" start="3">
<li><p>Maximize the stochastic objective function</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  J\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big).
  \]</div>
</section>
</div><p>In practice, nobody ever maximizes the data likelihood function directly; instead, maximum likelihood estimates are obtained via the other two objective functions in the forms <a class="reference internal" href="#equation-log-like-simp-eqn">(11.4)</a> and <a class="reference internal" href="#equation-stochastic-simp-eqn">(11.5)</a>. As we mentiond above, the stochastic objective function has the advantage of allowing the use of stochastic gradient descent when closed form solutions are not available.</p>
</section>
<section id="maximum-likelihood-estimation-for-linear-regression-models">
<h2><span class="section-number">11.2. </span>Maximum likelihood estimation for linear regression models<a class="headerlink" href="#maximum-likelihood-estimation-for-linear-regression-models" title="Permalink to this heading">#</a></h2>
<p>Linear regression models have the special property that maximum likelihood estimates may be obtained in <em>closed form</em>. Let’s see how.</p>
<p>First, recall from <a class="reference internal" href="models.html#lin-reg-sec"><span class="std std-numref">Section 10.2</span></a> that the underlying graph of a linear regression model is of the form</p>
<a class="reference internal image-reference" href="../_images/lin-reg-00.svg"><img alt="../_images/lin-reg-00.svg" class="align-center" src="../_images/lin-reg-00.svg" width="50%" /></a>
<p> </p>
<p>where <span class="math notranslate nohighlight">\(\bbeta \in \mathbb{R}^{n\times 1}\)</span>, <span class="math notranslate nohighlight">\(\beta_0 \in \bbr\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>. Given a dataset</p>
<div class="math notranslate nohighlight">
\[
(\bx^{(1)},y^{(1)}),\ldots,(\bx^{(m)},y^{(m)}) \in \bbr^{1\times n} \times \bbr,
\]</div>
<p>we may retrieve the data log-likelihood function from <a class="reference internal" href="models.html#lin-reg-sec"><span class="std std-numref">Section 10.2</span></a>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\bbeta, \beta_0, \sigma^2) &amp;= \sum_{i=1}^m \log \left[ \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(- \frac{1}{2\sigma^2} \big( y^{(i)} - \mu^{(i)} \big)^2 \right) \right] \\
&amp;= - \frac{m}{2} \log{2\pi} - \frac{m}{2} \log{\sigma^2} - \frac{1}{2\sigma^2} \sum_{i=1}^m \big( y^{(i)} - \mu^{(i)}\big)^2,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu^{(i)} = \bx^{(i)} \bbeta + \beta_0\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>.</p>
<p>The maximum likelihood estimate for the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> has a particularly simple form:</p>
<div class="proof theorem admonition" id="theorem-6">
<p class="admonition-title"><span class="caption-number">Theorem 11.5 </span> (MLE for the variance of a linear regression model)</p>
<section class="theorem-content" id="proof-content">
<p>The maximum likelihood estimate for the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> of a linear regression model is given by the formula</p>
<div class="math notranslate nohighlight">
\[
(\sigma^2)^\star = \frac{1}{m}\sum_{i=1}^m \big( y^{(i)} - \mu^{(i)}\big)^2.
\]</div>
</section>
</div></section>
<section id="expectation-maximization-for-gaussian-mixture-models">
<span id="em-gmm-sec"></span><h2><span class="section-number">11.3. </span>Expectation maximization for Gaussian mixture models<a class="headerlink" href="#expectation-maximization-for-gaussian-mixture-models" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Probabilistic models</p>
      </div>
    </a>
    <a class="right-next"
       href="stats-estimators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Statistics and estimators</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-based-learning-objectives">11.1. Likelihood-based learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-linear-regression-models">11.2. Maximum likelihood estimation for linear regression models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-for-gaussian-mixture-models">11.3. Expectation maximization for Gaussian mixture models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>