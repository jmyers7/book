

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>10. Information theory &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bpi": "\\boldsymbol\\pi", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bp": "\\mathbf{p}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bD": "\\mathbf{D}", "bH": "\\mathbf{H}", "bI": "\\mathbf{I}", "bK": "\\mathbf{K}", "bS": "\\mathbf{S}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bW": "\\mathbf{W}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calJ": "\\mathcal{J}", "calH": "\\mathcal{H}", "calI": "\\mathcal{I}", "calL": "\\mathcal{L}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "calS": "\\mathcal{S}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/10-info-theory';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Optimization" href="11-optim.html" />
    <link rel="prev" title="9. The halfway point: pivoting toward models and data analysis" href="09-halfway.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-halfway.html">9. The halfway point: pivoting toward models and data analysis</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-optim.html">11. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-models.html">12. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-learning.html">13. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-stats-estimators.html">14. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-asymptotic.html">15. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-CIs.html">16. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-hyp-test.html">17. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-lin-reg.html">18. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">19. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/10-info-theory.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Information theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shannon-information-and-entropy">10.1. Shannon information and entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kullback-leibler-divergence">10.2. Kullback Leibler divergence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-of-information">10.3. Flow of information</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="information-theory">
<span id="id1"></span><h1><span class="section-number">10. </span>Information theory<a class="headerlink" href="#information-theory" title="Permalink to this heading">#</a></h1>
<p>The current chapter describes tools and techniques drawn from the <em>theory of information</em>, which is a charming amalgamation of practical engineering and theoretical mathematics. This theory provides us with a method for measuring a particular form of <em>information</em>, but it also gives us techniques for quantifying related degrees of <em>surprise</em>, <em>uncertainty</em>, and <em>entropy</em>. In the upcoming chapters, our primary use for these measures will be to train and choose probabilistic models, but the theory reaches way beyond into physics, coding theory, computer science, neuroscience, biology, economics, the theory of complex systems, and many other fields.</p>
<p>Though it has its roots in engineering, our primary view on information theory is a mathematical one. For reasons of space and time, we cannot include or describe any of the many fascinating applications to coding theory and the like—for that, we will point the interested reader to one of the standard textbooks on information theory, like <span id="id2">[<a class="reference internal" href="bib.html#id17" title="T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley &amp; Sons, Inc., second edition, 2006.">CT06</a>]</span>, <span id="id3">[<a class="reference internal" href="bib.html#id25" title="D. J. C. MacKay. Information theory, inference and learning algorithms. Cambridge University Press, 2003.">Mac03</a>]</span>, and <span id="id4">[<a class="reference internal" href="bib.html#id26" title="R. B. Ash. Information theory. Courier Corporation, 2012.">Ash12</a>]</span>. Also, one can do little better than reading Claude Shannon’s foundational paper <span id="id5">[<a class="reference internal" href="bib.html#id23" title="C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):379–423, 1948.">Sha48</a>]</span> in which information theory as a mathematical discipline was first described—though it is over three-quarters of a century old, it still reads amazingly well. The survey article <span id="id6">[<a class="reference internal" href="bib.html#id24" title="O. Rioul. This is it: a primer on shannon’s entropy and information. In Information Theory: Poincaré Seminar 2018, 49–86. Springer, 2021.">Rio21</a>]</span> is also an enlightening read.</p>
<p>In the first section of this chapter, we define and study the first two of the four central quantities in information theory called <em>Shannon information</em> and <em>Shannon entropy</em>—after that, in the second section, we define the third quantity called <em>Kullback Leibler (KL) divergence</em>. It is mostly this latter quantity that will be of primary use in the following chapters, since it provides a generalized notion of “distance” between probability distributions. Finally, we end the chapter with the third section where we define and discuss the <em>mutual information</em> shared between two random variables, which may be conceptualized as a generalized correlation measure that vanishes exactly when the variables are independent. This discussion is framed in the context of “flows” of “information” or “influence,” foreshadowing the probabilistic models that we will study in depth in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>.</p>
<section id="shannon-information-and-entropy">
<h2><span class="section-number">10.1. </span>Shannon information and entropy<a class="headerlink" href="#shannon-information-and-entropy" title="Permalink to this heading">#</a></h2>
<p>The whole story begins with a very simple definition:</p>
<div class="proof definition admonition" id="info-content-def">
<p class="admonition-title"><span class="caption-number">Definition 10.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> be a probability measure on a finite sample space <span class="math notranslate nohighlight">\(S\)</span> with mass function <span class="math notranslate nohighlight">\(p(s)\)</span>. The <em>(Shannon) information content</em> of the sample point <span class="math notranslate nohighlight">\(s\in S\)</span>, denoted <span class="math notranslate nohighlight">\(I_P(s)\)</span>, is defined to be</p>
<div class="math notranslate nohighlight">
\[
I_P(s) \def - \log(p(s)).
\]</div>
<p>The information content is also called the <em>surprisal</em>.</p>
<p>If the probability measure <span class="math notranslate nohighlight">\(P\)</span> is clear from context, we will write <span class="math notranslate nohighlight">\(I(s)\)</span> in place of <span class="math notranslate nohighlight">\(I_P(s)\)</span>. If <span class="math notranslate nohighlight">\(\bX\)</span> is a random vector with finite range and probability measure <span class="math notranslate nohighlight">\(P_\bX\)</span>, we will write <span class="math notranslate nohighlight">\(I_\bX(\bx)\)</span> in place of <span class="math notranslate nohighlight">\(I_{P_\bX}(\bx)\)</span>.</p>
</section>
</div><p>On one hand, a sense in which <span class="math notranslate nohighlight">\(I_P(s)\)</span> may be considered a measure of “information” comes from its interpretation as the length of a code word, in the context of <a class="reference external" href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding">Shannon-Fano coding</a>. Another sense comes when we take the average information content of all sample points to obtain something called <em>Shannon entropy</em>; this will be explained after <a class="reference internal" href="#entropy-def">Definition 10.2</a> below.</p>
<p>On the other hand, the intuition for the alternate name <em>surprisal</em> is explained very nicely by simply inspecting the graph of the negative logarithm function:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$p$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;$I(p) = -</span><span class="se">\\</span><span class="s1">log</span><span class="si">{p}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/03eb5dfd2303b2d8864726c15fdf9ce91503182c5f79a5265369652f4ac01249.svg" src="../_images/03eb5dfd2303b2d8864726c15fdf9ce91503182c5f79a5265369652f4ac01249.svg" /></figure>
</div>
</div>
<p>If some outcome is highly likely to occur (large <span class="math notranslate nohighlight">\(p(s)\)</span>), then it is not surprising (small <span class="math notranslate nohighlight">\(I(s)\)</span>). In the other direction, if an outcome <span class="math notranslate nohighlight">\(s\)</span> is highly unlikely to occur (small <span class="math notranslate nohighlight">\(p(s)\)</span>), then it is very surprising (large <span class="math notranslate nohighlight">\(I(s)\)</span>).</p>
<p>It might occur that there are many functions that are equally capable of expressing this same inverse relationship between probability and surprisal—so why the choice of base-<span class="math notranslate nohighlight">\(e\)</span> logarithm? It turns out that if you begin from first principles with a set of “natural axioms” that any notion of <em>surprisal</em> should possess, then you can <em>prove</em> all such surprisal functions must be proportional to negative logarithms; see, for example, the discussion in Section 9 in <span id="id7">[<a class="reference internal" href="bib.html#id24" title="O. Rioul. This is it: a primer on shannon’s entropy and information. In Information Theory: Poincaré Seminar 2018, 49–86. Springer, 2021.">Rio21</a>]</span>. The choice of base <span class="math notranslate nohighlight">\(e\)</span> is then somewhat arbitrary, akin to choosing units. Another popular choice is base <span class="math notranslate nohighlight">\(2\)</span>, facilitating a smooth connection to bit strings in coding theory. (See the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/09-homework.md#problem-2-entropy-in-different-bases">homework</a>.) In base <span class="math notranslate nohighlight">\(e\)</span>, information content is measured in so-called <em>natural units</em>, or <em>nats</em>; in base <span class="math notranslate nohighlight">\(2\)</span>, it is measured in <em>binary units</em>, or <em>bits</em>. (See Section 10 in the aforementioned reference <span id="id8">[<a class="reference internal" href="bib.html#id24" title="O. Rioul. This is it: a primer on shannon’s entropy and information. In Information Theory: Poincaré Seminar 2018, 49–86. Springer, 2021.">Rio21</a>]</span> for more on units.)</p>
<p>Please understand that the terminology <em>information content</em> now has a very specific and precise mathematical meaning. It is designed to “get at” our intuitive understanding of what general “information” is, but you should keep the two separate in your mind: There’s the notion of “information” used in an intuitive and colloquial sense and is generally ill-defined, and then there is the notion of <em>information content</em> precisely defined as above.</p>
<p>With the information content (or surprisal) in hand, we now define <em>entropy</em>:</p>
<div class="proof definition admonition" id="entropy-def">
<p class="admonition-title"><span class="caption-number">Definition 10.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> be a probability measure on a finite sample space <span class="math notranslate nohighlight">\(S\)</span> with mass function <span class="math notranslate nohighlight">\(p(s)\)</span>. The <em>(Shannon) entropy</em> of <span class="math notranslate nohighlight">\(P\)</span>, denoted <span class="math notranslate nohighlight">\(H(P)\)</span>, is defined to be</p>
<div class="math notranslate nohighlight">
\[
H(P) \def \sum_{s\in S} p(s)I_P(s).
\]</div>
<p>The entropy is also called the <em>uncertainty</em>.</p>
<p>If <span class="math notranslate nohighlight">\(\bX\)</span> is a random vector with finite range and probability measure <span class="math notranslate nohighlight">\(P_\bX\)</span>, we will write <span class="math notranslate nohighlight">\(H(\bX)\)</span> in place of <span class="math notranslate nohighlight">\(H(P_\bX)\)</span>. If we write the vector in terms of its component random variables <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_m)\)</span>, then we shall also write <span class="math notranslate nohighlight">\(H(X_1,\ldots,X_m)\)</span> in place of <span class="math notranslate nohighlight">\(H(P_\bX)\)</span> and call this the <em>joint entropy</em> of the random variables <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span>.</p>
</section>
</div><p>There is a notion of <em>entropy</em> for continuous probability measures defined on Euclidean spaces—this latter type of entropy is called <em>differential entropy</em>, which you will briefly encounter in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/09-homework.md#problem-5-differential-entropy">homework</a>.</p>
<p>Since <span class="math notranslate nohighlight">\(I(s) = -\log(p(s))\)</span>, there is an issue in the definition of <span class="math notranslate nohighlight">\(H(P)\)</span> in the case that <span class="math notranslate nohighlight">\(p(s)=0\)</span> for some <span class="math notranslate nohighlight">\(s\in S\)</span>, for then we encounter the indeterminate form <span class="math notranslate nohighlight">\(0 \log(0)\)</span>. By convention, we take this expression to equal <span class="math notranslate nohighlight">\(0\)</span>, which may be justified according to the limit</p>
<div class="math notranslate nohighlight">
\[
\lim_{p \to 0^+} p \log(p) = 0.
\]</div>
<p>In particular, when the probability distribution <span class="math notranslate nohighlight">\(P\)</span> is a so-called <em>Dirac distribution</em> that puts a spike of probability <span class="math notranslate nohighlight">\(1\)</span> on a single sample point and assigns <span class="math notranslate nohighlight">\(0\)</span> probability elsewhere, the entropy is at the minimum value <span class="math notranslate nohighlight">\(H(P)=0\)</span>. As we will see below (in <a class="reference internal" href="#max-entropy-thm">Corollary 10.1</a>), at the other end of the spectrum are the maximum-entropy uniform distributions:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">spike</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">uniform</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">spike</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$p(x)$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Dirac distribution = minimum entropy&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$p(x)$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;uniform distribution = maximum entropy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/3bcec17620b987463b16b1c24f9fbf98b2b8f3603b1a1178dae475caef12f8ab.svg" src="../_images/3bcec17620b987463b16b1c24f9fbf98b2b8f3603b1a1178dae475caef12f8ab.svg" /></figure>
</div>
</div>
<p>The fact that these two types of distributions yield the extreme values of entropy helps explain the alternate name <em>uncertainty</em>. Indeed, imagine drawing a random sample from one of these two distributions—which one has the most uncertain outcome? Which one is most certain?</p>
<p>Notice that the entropy is the average information content, or surprisal, where the averaging weights are drawn from the mass function <span class="math notranslate nohighlight">\(p(s)\)</span>. Since averages of this form will reoccur so often in the current and next few chapters, it will be convenient to introduce new notations for them. So, if <span class="math notranslate nohighlight">\(P\)</span> is a discrete probability measure with mass function <span class="math notranslate nohighlight">\(p(s)\)</span> on a sample space <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(g:S\to \bbr\)</span> is a real-valued function, we will define</p>
<div class="math notranslate nohighlight">
\[
E_P\left[g(s)\right] \def \sum_{s\in S} g(s) p(s).
\]</div>
<p>Alternatively, if we want to explicitly call attention to the mass function <span class="math notranslate nohighlight">\(p(s)\)</span> rather than the probability measure <span class="math notranslate nohighlight">\(P\)</span> itself, we will write</p>
<div class="math notranslate nohighlight">
\[
E_{s\sim p(s)}\left[g(s)\right] \def \sum_{s\in S} g(s) p(s).
\]</div>
<p>We refer to these sums as the <em>mean value</em> or <em>expected value</em> of <span class="math notranslate nohighlight">\(g(s)\)</span>. Note that these are legitimately new usages of the expectation symbol <span class="math notranslate nohighlight">\(E\)</span>, since there is no random variable given <em>a priori</em>. To see the connection with the previous usage of <span class="math notranslate nohighlight">\(E\)</span> for a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> with mass function <span class="math notranslate nohighlight">\(p_X(x)\)</span>, suppose that <span class="math notranslate nohighlight">\(g:\bbr \to \bbr\)</span> and note</p>
<div class="math notranslate nohighlight">
\[
E_{P_X}\left[g(x)\right] = \sum_{x\in \bbr}g(x) p_X(x) = E\left[g(X)\right].
\]</div>
<p>Indeed, the first equality follows from the definition of <span class="math notranslate nohighlight">\(E_{P_X}\left[g(x)\right]\)</span> given above, while the second equality follows from the LotUS. Using this new notation, the definition of entropy may be rewritten as</p>
<div class="math notranslate nohighlight">
\[
H(P) = E_P\left[I_P(s)\right] = E_{s\sim p(s)} \left[ I_P(s)\right].
\]</div>
<p>Since the entropy <span class="math notranslate nohighlight">\(H(P)\)</span> is the average information content, it may sometimes be interpreted as a form of “information.” To see why low (high) uncertainty might be interpreted as low (high) “information” content, imagine that you are to design a random experiment to help answer some question. Then, you certainly <em>do not</em> want to arrange the conditions of the experiment so that the probabilities of the outcomes resemble the Dirac distribution above, with low uncertainty and one outcome practically all but guaranteed—this would convey little information! Instead, the ideal experiment would have the probabilities spread uniformly across all potential outcomes, so that any observed outcome is maximally informative. (For a very convincing and enlightening demonstration of this idea, see the description of the “weighing problem” in Section 4.1 of <span id="id9">[<a class="reference internal" href="bib.html#id25" title="D. J. C. MacKay. Information theory, inference and learning algorithms. Cambridge University Press, 2003.">Mac03</a>]</span>.)</p>
<p>Before moving on with the theory, let’s take a look at some problems:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problems 1 and 2 on the worksheet.</p>
</div>
<p>Now comes a second type of entropy:</p>
<div class="proof definition admonition" id="cross-entropy-def">
<p class="admonition-title"><span class="caption-number">Definition 10.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be two probability measures on a finite sample space <span class="math notranslate nohighlight">\(S\)</span> with mass functions <span class="math notranslate nohighlight">\(p(s)\)</span> and <span class="math notranslate nohighlight">\(q(s)\)</span>. Suppose they satisfy the following condition:</p>
<ul class="simple">
<li><p><em>Absolute continuity</em>. For all <span class="math notranslate nohighlight">\(s\in S\)</span>, if <span class="math notranslate nohighlight">\(q(s)=0\)</span>, then <span class="math notranslate nohighlight">\(p(s) = 0\)</span>. Or equivalently, the support of <span class="math notranslate nohighlight">\(q(s)\)</span> contains the support of <span class="math notranslate nohighlight">\(p(s)\)</span>.</p></li>
</ul>
<p>Then the <em>cross entropy</em> from <span class="math notranslate nohighlight">\(P\)</span> to <span class="math notranslate nohighlight">\(Q\)</span>, denoted <span class="math notranslate nohighlight">\(H_P(Q)\)</span>, is defined by</p>
<div class="math notranslate nohighlight">
\[
H_P(Q) \def E_{s\sim p(s)}\left[ I_Q(s) \right] =  - \sum_{s\in S} p(s)\log(q(s)).
\]</div>
<p>As usual, if <span class="math notranslate nohighlight">\(P_\bX\)</span> and <span class="math notranslate nohighlight">\(P_\bY\)</span> are the probability measures of two random vectors <span class="math notranslate nohighlight">\(\bX\)</span> and <span class="math notranslate nohighlight">\(\bY\)</span> with finite ranges, we will write <span class="math notranslate nohighlight">\(H_\bY(\bX)\)</span> in place of <span class="math notranslate nohighlight">\(H_{P_\bY} (P_\bX)\)</span>.</p>
</section>
</div><p>Notice that the condition of absolute continuity between the two measures guarantees we will never see an expression of the form <span class="math notranslate nohighlight">\(p \log(0)\)</span>, with <span class="math notranslate nohighlight">\(p \neq 0\)</span>. Thus, it is enough to make the cross entropy well-defined by stipulating that we take <span class="math notranslate nohighlight">\(0 \log(0) =0\)</span>, as explained above.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 3 on the worksheet.</p>
</div>
</section>
<section id="kullback-leibler-divergence">
<span id="kl-div-sec"></span><h2><span class="section-number">10.2. </span>Kullback Leibler divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Permalink to this heading">#</a></h2>
<p>Our aim in this section is to devise some sort of method for comparing the “distance” between two probability measures. The technique that we discover will have tight connections with the entropies studied in the previous section, but the first part of this section is largely independent of the previous. The link with entropy will come later.</p>
<p>The types of measures <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> that we shall work with are ones defined on a finite probability space <span class="math notranslate nohighlight">\(S\)</span>, so that they have mass functions <span class="math notranslate nohighlight">\(p(s)\)</span> and <span class="math notranslate nohighlight">\(q(s)\)</span>. (But see the comment immediately below <a class="reference internal" href="#KL-def">Definition 10.4</a>.) The basic measure that we use to compare them is the mean logarithmic relative magnitude.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Of course, the two notions of <em>absolute relative magnitude</em> and <em>logarithmic relative magnitude</em> make sense for any pair of numbers, not necessarily probabilities.</p>
</aside>
<p>Precisely, the <em>absolute relative magnitude</em> of the probability <span class="math notranslate nohighlight">\(p(s)\)</span> to the probability <span class="math notranslate nohighlight">\(q(s)\)</span> ordinarily refers to the ratio <span class="math notranslate nohighlight">\(p(s)/q(s)\)</span>, while the <em>logarithmic relative magnitude</em> refers to the base-<span class="math notranslate nohighlight">\(10\)</span> logarithm of the absolute relative magnitude:</p>
<div class="math notranslate nohighlight">
\[
\log_{10}\left( \frac{p(s)}{q(s)} \right).
\]</div>
<p>The intuition for this number is that it is the <em>order</em> of the absolute relative magnitude; indeed, if we have <span class="math notranslate nohighlight">\(p(s) \approx 10^k\)</span> and <span class="math notranslate nohighlight">\(q(s) \approx 10^l\)</span>, then the logarithmic relative magnitude is roughly the difference <span class="math notranslate nohighlight">\(k-l\)</span>.</p>
<p>Perhaps the most obvious immediate benefit of introducing the logarithm is that it yields a workable number when <span class="math notranslate nohighlight">\(p(s)\)</span> and <span class="math notranslate nohighlight">\(q(s)\)</span> are each on different scales. For example, let’s suppose that the mass functions <span class="math notranslate nohighlight">\(p(s)\)</span> and <span class="math notranslate nohighlight">\(q(s)\)</span> are given by</p>
<div class="math notranslate nohighlight">
\[
p(s) = \binom{10}{s} (0.4)^s(0.6)^{10-s} \quad \text{and} \quad q(s) = \binom{10}{s} (0.9)^s(0.1)^{10-s}
\]</div>
<p>for <span class="math notranslate nohighlight">\(s\in \{0,1,\ldots,10\}\)</span>; these are the mass functions of a <span class="math notranslate nohighlight">\(\Bin(10,0.4)\)</span> and <span class="math notranslate nohighlight">\(\Bin(10,0.9)\)</span> random variable, respectively. We then plot histograms for these mass functions, along with histograms of the absolute and logarithmic relative magnitudes:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;$p(s)$&#39;</span><span class="p">,</span>
          <span class="s1">&#39;$q(s)$&#39;</span><span class="p">,</span>
          <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">frac{p(s)}{q(s)}$&#39;</span><span class="p">,</span>
          <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">log_</span><span class="si">{10}</span><span class="se">\\</span><span class="s1">left(</span><span class="se">\\</span><span class="s1">frac{p(s)}{q(s)}</span><span class="se">\\</span><span class="s1">right)$&#39;</span><span class="p">]</span>
<span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span>
         <span class="n">q</span><span class="p">,</span>
         <span class="n">p</span> <span class="o">/</span> <span class="n">q</span><span class="p">,</span>
         <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">p</span> <span class="o">/</span> <span class="n">q</span><span class="p">)]</span>
<span class="n">ylims</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span>
         <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span>
         <span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mf">0.75e8</span><span class="p">),</span>
         <span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">title</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">ylim</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">titles</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">ylims</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$s$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7620a84208d77685c599dcbf4dc4dbed38ce7cad52834e72edb4730476668177.svg" src="../_images/7620a84208d77685c599dcbf4dc4dbed38ce7cad52834e72edb4730476668177.svg" /></figure>
</div>
</div>
<p>The second row in the figure drives home the point: The absolute relative magnitudes are on such widely different scales that the plot is nearly useless and numerical computations in a machine will likely be unstable.</p>
<p>We obtain a single-number summary of the logarithmic relative magnitudes by averaging with weights drawn from the mass function <span class="math notranslate nohighlight">\(p(s)\)</span>; this yields the number</p>
<div class="math notranslate nohighlight" id="equation-first-kl-eq">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-first-kl-eq" title="Permalink to this equation">#</a></span>\[
\sum_{s\in S} p(s) \log_{10}\left( \frac{p(s)}{q(s)} \right).
\]</div>
<p>Observe that we could have drawn the averaging weights instead from the mass function <span class="math notranslate nohighlight">\(q(s)\)</span> to obtain the single-number summary</p>
<div class="math notranslate nohighlight" id="equation-second-kl-eq">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-second-kl-eq" title="Permalink to this equation">#</a></span>\[
\sum_{s\in S} q(s) \log_{10}\left( \frac{p(s)}{q(s)} \right).
\]</div>
<p>But notice that</p>
<div class="math notranslate nohighlight">
\[
\sum_{s\in S} q(s) \log_{10}\left( \frac{p(s)}{q(s)} \right) = - \sum_{s\in S} q(s) \log_{10}\left( \frac{q(s)}{p(s)} \right),
\]</div>
<p>where the right-hand side is the negative of a number of the form <a class="reference internal" href="#equation-first-kl-eq">(10.1)</a>. (I am <strong>not</strong> saying these two numbers are equal, merely that they have the same functional form!) So, at least up to sign, it doesn’t really matter which of the two numbers <a class="reference internal" href="#equation-first-kl-eq">(10.1)</a> or <a class="reference internal" href="#equation-second-kl-eq">(10.2)</a> that we use to develop our theory. As we will see, our choice of <a class="reference internal" href="#equation-first-kl-eq">(10.1)</a> has the benefit of making the KL divergence nonnegative. Moreover, we can also alter the base of the logarithm in <a class="reference internal" href="#equation-first-kl-eq">(10.1)</a> without altering the core of the theory, since the change-of-base formula for logarithms tells us that the only difference is a multiplicative constant. In the following definition, we select the base-<span class="math notranslate nohighlight">\(e\)</span> natural logarithm to make the link with entropy, though the base-<span class="math notranslate nohighlight">\(2\)</span> binary logarithm is another common choice</p>
<div class="proof definition admonition" id="KL-def">
<p class="admonition-title"><span class="caption-number">Definition 10.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be two probability measures on a finite sample space <span class="math notranslate nohighlight">\(S\)</span> with mass functions <span class="math notranslate nohighlight">\(p(s)\)</span> and <span class="math notranslate nohighlight">\(q(s)\)</span>. Suppose they satisfy the following condition:</p>
<ul class="simple">
<li><p><em>Absolute continuity</em>. For all <span class="math notranslate nohighlight">\(s\in S\)</span>, if <span class="math notranslate nohighlight">\(q(s)=0\)</span>, then <span class="math notranslate nohighlight">\(p(s) = 0\)</span>. Or equivalently, the support of <span class="math notranslate nohighlight">\(q(s)\)</span> contains the support of <span class="math notranslate nohighlight">\(p(s)\)</span>.</p></li>
</ul>
<p>Then the <em>Kullback-Leibler divergence</em> (or just <em>KL divergence</em>) from <span class="math notranslate nohighlight">\(P\)</span> to <span class="math notranslate nohighlight">\(Q\)</span>, denoted <span class="math notranslate nohighlight">\(D(P \parallel Q)\)</span>, is the mean logarithmic relative magnitude of <span class="math notranslate nohighlight">\(P\)</span> to <span class="math notranslate nohighlight">\(Q\)</span>. Precisely, it is given by</p>
<div class="math notranslate nohighlight">
\[
D(P \parallel Q) \def E_{s\sim p(s)} \left[ \log\left( \frac{p(s)}{q(s)} \right)\right] =  \sum_{s\in S} p(s) \log\left( \frac{p(s)}{q(s)} \right).
\]</div>
<p>The KL divergence is also called the <em>relative entropy</em>.</p>
<p>As always, if <span class="math notranslate nohighlight">\(P_\bX\)</span> and <span class="math notranslate nohighlight">\(P_\bY\)</span> are the probability measures of two random vectors <span class="math notranslate nohighlight">\(\bX\)</span> and <span class="math notranslate nohighlight">\(\bY\)</span> with finite ranges, we will write <span class="math notranslate nohighlight">\(D(\bY \parallel \bX)\)</span> in place of <span class="math notranslate nohighlight">\(D(P_\bY \parallel P_\bX)\)</span>.</p>
</section>
</div><p>As with entropy, there is a notion of KL divergence for continuous probability measures on Euclidean spaces—it is called <em>differential KL divergence</em>, and you will encounter it briefly in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/09-homework.md#problem-7-differential-kl-divergence">homework</a>.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>To problem 4 on the worksheet.</p>
</div>
<p>The connection between KL divergence and entropy is given in the next theorem. Its proof is a triviality.</p>
<div class="proof theorem admonition" id="KL-and-entropy-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.1 </span> (KL divergence and entropy)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be two probability measures on a finite sample space <span class="math notranslate nohighlight">\(S\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
D(P\parallel Q) = H_P(Q) - H(P).
\]</div>
</section>
</div><p>We now work towards proving <em>Gibb’s inequality</em>, which is sometimes called the most important inequality in information theory. Its proof goes through a preliminary result known as <em>Jensen’s inequality</em> which, in turn, leverages the <em>concavity</em> of the logarithm function <span class="math notranslate nohighlight">\(h(x) = \log{x}\)</span>. This latter term means that between any two points <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span> along the <span class="math notranslate nohighlight">\(x\)</span>-axis, the graph of <span class="math notranslate nohighlight">\(h(x)\)</span> lies below the secant line:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span>
<span class="n">grid_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span> <span class="o">/</span> <span class="mf">0.8</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid_line</span><span class="p">,</span> <span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid_line</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;$y=h(x) = </span><span class="se">\\</span><span class="s1">log</span><span class="si">{x}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9</span><span class="p">,</span> <span class="s1">&#39;$(a,h(a))$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;$(b,h(b))$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/1e7ec20aa041d1b477f92727aaa787edfc92b664b83638a2fc345945920c60c7.svg" src="../_images/1e7ec20aa041d1b477f92727aaa787edfc92b664b83638a2fc345945920c60c7.svg" /></figure>
</div>
</div>
<p>Note that all <span class="math notranslate nohighlight">\(x\)</span>-values between the endpoints <span class="math notranslate nohighlight">\(x=a\)</span> and <span class="math notranslate nohighlight">\(x=b\)</span> are of the form</p>
<div class="math notranslate nohighlight">
\[
x = (1-t) a + tb
\]</div>
<p>for some <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>. Plugging this <span class="math notranslate nohighlight">\(x\)</span>-value into the function yields a point on the graph with <span class="math notranslate nohighlight">\(y\)</span>-value</p>
<div class="math notranslate nohighlight">
\[
y = h\big((1-t) a + tb\big).
\]</div>
<p>On the other hand, as you may easily check, this same <span class="math notranslate nohighlight">\(x\)</span>-value yields a point on the secant line with <span class="math notranslate nohighlight">\(y\)</span>-value</p>
<div class="math notranslate nohighlight">
\[
y = (1-t)h(a) + th(b).
\]</div>
<p>Thus, concavity of the function means that the inequality</p>
<div class="math notranslate nohighlight" id="equation-convex-secant-eq">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-convex-secant-eq" title="Permalink to this equation">#</a></span>\[
(1-t)h(a) + th(b) \leq h\big((1-t) a + tb\big)
\]</div>
<p>holds for all <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>. Note that since <span class="math notranslate nohighlight">\(h\)</span> is <em>strictly concave</em> (i.e., not linear), equality holds in <a class="reference internal" href="#equation-convex-secant-eq">(10.3)</a> if and only if <span class="math notranslate nohighlight">\(a=b\)</span>.</p>
<p>Now, note that the two scalars <span class="math notranslate nohighlight">\(1-t\)</span> and <span class="math notranslate nohighlight">\(t\)</span> appearing in <a class="reference internal" href="#equation-convex-secant-eq">(10.3)</a> are nonnegative and sum to <span class="math notranslate nohighlight">\(1\)</span>; thus, they define a probability measure on the sample space <span class="math notranslate nohighlight">\(S=\{a,b\}\)</span>, and we may rewrite <a class="reference internal" href="#equation-convex-secant-eq">(10.3)</a> as</p>
<div class="math notranslate nohighlight">
\[
E_{s\sim p(s)}(h(s)) \leq h\big( E_{s\sim p(s)} (s) \big)
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(a) = 1-t\)</span> and <span class="math notranslate nohighlight">\(p(b) = t\)</span>. Jensen’s inequality is nothing but a generalization of these observations to arbitrary concave functions and probability measures:</p>
<div class="proof theorem admonition" id="jensen-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.2 </span> (Jensen’s inequality)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose <span class="math notranslate nohighlight">\(h: \bbr\to \bbr\)</span> is a concave function and <span class="math notranslate nohighlight">\(P\)</span> is a probability measure on a finite subset <span class="math notranslate nohighlight">\(S\subset \bbr\)</span> with mass function <span class="math notranslate nohighlight">\(p(s)\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
E_{s\sim p(s)}(h(s)) \leq h\big( E_{s\sim p(s)} (s) \big).
\]</div>
<p>Provided <span class="math notranslate nohighlight">\(f\)</span> is <em>strictly concave</em>, equality holds if and only if the sample space <span class="math notranslate nohighlight">\(S\)</span> has cardinality <span class="math notranslate nohighlight">\(1\)</span>.</p>
</section>
</div><p>We will not give a proof of the full version of Jensen’s inequality, hoping that the discussion before the statement at least conveys some intuition for why it is true. For a proof, see Section 2.6 in <span id="id10">[<a class="reference internal" href="bib.html#id17" title="T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley &amp; Sons, Inc., second edition, 2006.">CT06</a>]</span>, for example.</p>
<p>We now put Jensen’s inequality to use:</p>
<div class="proof theorem admonition" id="gibbs-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.3 </span> (Gibbs’ inequality)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be two probability measures on a finite probability space <span class="math notranslate nohighlight">\(S\)</span> satisfying the absolute continuity condition in <a class="reference internal" href="#KL-def">Definition 10.4</a>. Then</p>
<div class="math notranslate nohighlight">
\[
D(P \parallel Q) \geq 0,
\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(P=Q\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Suppose that the support of the mass function <span class="math notranslate nohighlight">\(p(s)\)</span> is <span class="math notranslate nohighlight">\(S_p = \{s_1,\ldots,s_n\}\)</span>. By absolute continuity, the support <span class="math notranslate nohighlight">\(S_q\)</span> of the mass function <span class="math notranslate nohighlight">\(q(s)\)</span> contains <span class="math notranslate nohighlight">\(S_p\)</span>, so that <span class="math notranslate nohighlight">\(S_p \subset S_q\)</span>. Now, define</p>
<div class="math notranslate nohighlight">
\[
S' = \left\{ \frac{q(s_1)}{p(s_1)}, \frac{q(s_2)}{p(s_2)}, \ldots, \frac{q(s_n)}{p(s_n)}\right\},
\]</div>
<p>noting that none of the numbers in this set is <span class="math notranslate nohighlight">\(0\)</span> since <span class="math notranslate nohighlight">\(S_p \subset S_q\)</span>. We may view <span class="math notranslate nohighlight">\(S'\)</span> as a new sample space with <span class="math notranslate nohighlight">\(s'_k = q(s_k)/p(s_k)\)</span> for each <span class="math notranslate nohighlight">\(k=1,\ldots,n\)</span>, equipped with a probability mass function <span class="math notranslate nohighlight">\(p(s')\)</span> given by <span class="math notranslate nohighlight">\(p(s'_k) = p(s_k)\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(h(x) = \log{x}\)</span> is strictly concave, from Jensen’s inequality (<a class="reference internal" href="#jensen-thm">Theorem 10.2</a>) we have</p>
<div class="math notranslate nohighlight" id="equation-gibbs-target-eq">
<span class="eqno">(10.4)<a class="headerlink" href="#equation-gibbs-target-eq" title="Permalink to this equation">#</a></span>\[
E_{s' \sim p(s')}\big( h(s') \big) \leq h \big( E_{s' \sim p(s')}(s') \big),
\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(S'\)</span> has cardinality <span class="math notranslate nohighlight">\(1\)</span>. However, the left-hand side is</p>
<div class="math notranslate nohighlight">
\[
E_{s' \sim p(s')}\big( h(s') \big) = \sum_{k=1}^n p(s'_k)\log{s'_k} = - \sum_{k=1}^n p(s_k) \log\left( \frac{p(s_k)}{q(s_k)} \right) = -D(P\parallel Q),
\]</div>
<p>while the right-hand side of <a class="reference internal" href="#equation-gibbs-target-eq">(10.4)</a> is</p>
<div class="math notranslate nohighlight" id="equation-gibbs-target-2-eq">
<span class="eqno">(10.5)<a class="headerlink" href="#equation-gibbs-target-2-eq" title="Permalink to this equation">#</a></span>\[
h \big( E_{s' \sim p(s')}(s') \big) = \log \left( \sum_{k=1}^n p(s_k')s_k' \right) = \log \left( \sum_{k=1}^n q(s_k)\right) \leq 0. 
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(D(P \parallel Q) \geq 0\)</span>.</p>
<p>As we observed, equality holds if and only if <span class="math notranslate nohighlight">\(S'\)</span> has cardinality <span class="math notranslate nohighlight">\(1\)</span>, in which case there is a positive constant <span class="math notranslate nohighlight">\(c\)</span> such that <span class="math notranslate nohighlight">\(p_s(s_k) = cq(s_k)\)</span> for each <span class="math notranslate nohighlight">\(k=1,\ldots,n\)</span>. But then</p>
<div class="math notranslate nohighlight">
\[
1 = \sum_{k=1}^n p(s_k) = c \sum_{k=1}^n q(s_k).
\]</div>
<p>Note that the equality <span class="math notranslate nohighlight">\(D(P\parallel Q)=0\)</span> also implies <span class="math notranslate nohighlight">\(\sum_{k=1}^n q(s_k) = 1\)</span> from <a class="reference internal" href="#equation-gibbs-target-2-eq">(10.5)</a>, and so <span class="math notranslate nohighlight">\(c=1\)</span>. Since then <span class="math notranslate nohighlight">\(S_p = S_q\)</span> as well, we’ve shown that <span class="math notranslate nohighlight">\(P=Q\)</span>. Q.E.D.</p>
</div>
<p>Gibbs’ inequality provides the theoretical justification for viewing the KL divergence as a type of “distance” between two probability distributions. It has two properties that any “distance” must have: It is always nonnegative, and it vanishes only when the two distributions are the same. However, remember that it is <em>not</em> symmetric in the distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, so the KL divergence falls short of being a true distance measure in the <a class="reference external" href="https://en.wikipedia.org/wiki/Metric_space#Definition">precise, mathematical sense</a>. It is better thought of as a “directed distance” function.</p>
<p>Gibbs’ inequality has the following immediate corollary:</p>
<div class="proof corollary admonition" id="max-entropy-thm">
<p class="admonition-title"><span class="caption-number">Corollary 10.1 </span> (Uniform distributions maximize entropy)</p>
<section class="corollary-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(P\)</span> be a probability measures on a finite sample space <span class="math notranslate nohighlight">\(S\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
H(P) \leq \log{|S|},
\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(P\)</span> is uniform.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(Q\)</span> be the uniform distribution on <span class="math notranslate nohighlight">\(S\)</span>, and let <span class="math notranslate nohighlight">\(p(s)\)</span> and <span class="math notranslate nohighlight">\(q(s)\)</span> be the mass functions of <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>, respectively. Then by Gibbs’ inequality, we have</p>
<div class="math notranslate nohighlight">
\[
0 \leq D(P \parallel Q) = H_P(Q) - H(P) = \sum_{s\in S} p(s) \log|S| - H(P) = \log|S| - H(P).
\]</div>
<p>The desired result then follows. Q.E.D.</p>
</div>
<p>So, when <span class="math notranslate nohighlight">\(P\)</span> is uniform, we have <span class="math notranslate nohighlight">\(H(P) = \log|S|\)</span>. It is pleasing to compare this latter equation to the <a class="reference external" href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula">Boltzmann equation</a> for entropy in statistical mechanics. The defining equation for Shannon entropy</p>
<div class="math notranslate nohighlight">
\[
H(P) = - \sum_{s\in S} p(s) \log(p(s))
\]</div>
<p>is the analog of the <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)#Gibbs_entropy_formula">Gibbs equation</a> for Boltzmann entropy.</p>
<p>Let’s finish off our discussion of KL divergence with:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 5 on the worksheet.</p>
</div>
</section>
<section id="flow-of-information">
<span id="cond-entropy-mutual-info-sec"></span><h2><span class="section-number">10.3. </span>Flow of information<a class="headerlink" href="#flow-of-information" title="Permalink to this heading">#</a></h2>
<p>In this section, we use the information-theoretic measures developed in the previous sections to quantify the amount of “information” shared between two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, or the amount of “influence” that they exert on each other. (As in the rest of this chapter, we shall restrict ourselves to random variables with finite ranges.) But to say that the random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> share information must mean that there is some sort of “communication channel” between them—how might we model such a channel?</p>
<p>As we mentioned at the beginning of <a class="reference internal" href="08-more-prob.html#covar-correl-sec"><span class="std std-numref">Section 8.5</span></a>, perhaps the cleanest way to transmit information from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> is via a <em>deterministic link</em> between observed values. This would take the form of a function</p>
<div class="math notranslate nohighlight">
\[
g: \bbr \to \bbr, \quad x \mapsto g(x),
\]</div>
<p>such that <span class="math notranslate nohighlight">\(Y=g(X)\)</span>. Then, an observation <span class="math notranslate nohighlight">\(x\)</span> (of <span class="math notranslate nohighlight">\(X\)</span>) <em>uniquely</em> determines an observation <span class="math notranslate nohighlight">\(y = g(x)\)</span> (of <span class="math notranslate nohighlight">\(Y\)</span>). While these types of deterministic functional dependencies have their place in the theory (see <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>, for instance), they are not the sort of communication channels of primary interest in information theory. Indeed, Shannon’s main goal in the foundational paper <span id="id11">[<a class="reference internal" href="bib.html#id23" title="C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3):379–423, 1948.">Sha48</a>]</span> was to quantify the transfer of information through imperfect “noisy” channels that introduce errors.</p>
<p>So, even though it is too much to ask that an observation <span class="math notranslate nohighlight">\(x\)</span> <em>uniquely</em> determines an observation <span class="math notranslate nohighlight">\(y\)</span>, we still want to keep the general idea that <span class="math notranslate nohighlight">\(x\)</span> is the information carrier that is transmitted through the channel toward <span class="math notranslate nohighlight">\(Y\)</span>. But when <span class="math notranslate nohighlight">\(x\)</span> arrives at <span class="math notranslate nohighlight">\(Y\)</span>, it carries uncertainty with it, introduced by the “noise” in the channel. Uncertainty is modeled mathematically using probabilities, so what we are after is a notion of <em>communcation channel</em> that sends <span class="math notranslate nohighlight">\(x\)</span> to a probability distribution on the <span class="math notranslate nohighlight">\(y\)</span>’s. But this is <em>exactly</em> what conditional distributions are meant to model, and so we are led to replace the deterministic link <span class="math notranslate nohighlight">\(x \mapsto g(x)\)</span> from above with the mapping</p>
<div class="math notranslate nohighlight" id="equation-markov-kern-eq">
<span class="eqno">(10.6)<a class="headerlink" href="#equation-markov-kern-eq" title="Permalink to this equation">#</a></span>\[
x \mapsto p(y | x)
\]</div>
<p>that sends <span class="math notranslate nohighlight">\(x\)</span> to the associated conditional distribution on the <span class="math notranslate nohighlight">\(y\)</span>’s. In contrast to the deterministic link, this might be called a <em>stochastic link</em>. It is a strange mapping, as its domain is a Euclidean space, but its codomain is a <em>set of probability measures</em>! It is an example of something called a general <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_kernel">Markov kernel</a>. The contrast between the two types of links is depicted in:</p>
<a class="reference internal image-reference" href="../_images/markov-kern.svg"><img alt="../_images/markov-kern.svg" class="align-center" src="../_images/markov-kern.svg" width="85%" /></a>
<p> </p>
<p>We are led, then, to define a <em>communication channel</em> as essentially a Markov kernel. But because students at this level might be uncomfortable with a mapping of the form <a class="reference internal" href="#equation-markov-kern-eq">(10.6)</a> that outputs entire probability distributions, we take advantage of the fact that our random variables have finite ranges and “choose bases” to reformulate an alternate definition in terms of matrices and vectors. So, if we suppose for simplicity that the ranges of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have cardinalities <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, respectively, then the conditional distributions may be doubly indexed as <span class="math notranslate nohighlight">\(p(y_j | x_i)\)</span>, for <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span> and <span class="math notranslate nohighlight">\(j=1,\ldots,n\)</span>. For ease of bookkeeping (and other reasons), it is natural to place these distributions into an <span class="math notranslate nohighlight">\(m\times n\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[
\bK = [p(y_j |x _i)].
\]</div>
<p>Holding the row index <span class="math notranslate nohighlight">\(i\)</span> fixed and summing over the columns, note that</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^n p(y_j | x_i) = 1,
\]</div>
<p>since <span class="math notranslate nohighlight">\(p(y|x)\)</span> is a probability mass function in the variable <span class="math notranslate nohighlight">\(y\)</span> with <span class="math notranslate nohighlight">\(x\)</span> held fixed. If we also represent the mass functions <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(p(y)\)</span> as vectors</p>
<div class="math notranslate nohighlight" id="equation-prob-vectors-eq">
<span class="eqno">(10.7)<a class="headerlink" href="#equation-prob-vectors-eq" title="Permalink to this equation">#</a></span>\[
\bpi(X)^\intercal = \begin{bmatrix} p(x_1) &amp; \cdots &amp; p(x_m) \end{bmatrix} \quad \text{and} \quad \bpi(Y)^\intercal = \begin{bmatrix} p(y_1) &amp; \cdots &amp; p(y_n) \end{bmatrix}
\]</div>
<p>then by the Law of Total Probability we have</p>
<div class="math notranslate nohighlight">
\[
\bpi(X)^\intercal \bK = \bpi(Y)^\intercal.
\]</div>
<p>Notice that the entries in the vectors <a class="reference internal" href="#equation-prob-vectors-eq">(10.7)</a> sum to <span class="math notranslate nohighlight">\(1\)</span>. Vectors with nonnegative entries that sum to <span class="math notranslate nohighlight">\(1\)</span> are called <em>probability vectors</em>.</p>
<p>With these preparations completed, we are ready to define (our version of) a <em>Markov kernel</em> and a <em>communication channel</em>:</p>
<div class="proof definition admonition" id="markov-kernel-def">
<p class="admonition-title"><span class="caption-number">Definition 10.5 </span></p>
<section class="definition-content" id="proof-content">
<p>A <em>Markov kernel</em> is a mapping</p>
<div class="math notranslate nohighlight">
\[
\kappa: \{1,2,\ldots,m\} \to \bbr^n
\]</div>
<p>such that each vector <span class="math notranslate nohighlight">\(\kappa(i)\in \bbr^n\)</span> is a probability vector (i.e., a vector with nonnegative entries that sum to <span class="math notranslate nohighlight">\(1\)</span>). The <span class="math notranslate nohighlight">\(m\times n\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bK = \begin{bmatrix} \leftarrow &amp; \kappa(1)^\intercal &amp; \rightarrow \\ \vdots &amp; \vdots &amp; \vdots \\ \leftarrow &amp; \kappa(m)^\intercal &amp; \rightarrow \end{bmatrix}
\end{split}\]</div>
<p>is called the <em>transition matrix</em> of the Markov kernel.</p>
</section>
</div><p>Markov kernels occur in the theory of <em>Markov chains</em>, if you know what those are. In the context of information theory, Markov kernels become our models of communication channels:</p>
<div class="proof definition admonition" id="comm-channel-def">
<p class="admonition-title"><span class="caption-number">Definition 10.6 </span></p>
<section class="definition-content" id="proof-content">
<p>A <em>communication channel</em> is a Markov kernel.</p>
</section>
</div><p>Notice that there is no mention of the random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> in this definition. Indeed, this definition is meant only to isolate and capture the channel itself, whereas the random vectors <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are separate components that are conceptualized as the “sender” and “receiver” hooked to the two ends of the channel.</p>
<p>Our discussion preceding the definitions shows that every pair of random variables <span class="math notranslate nohighlight">\((X,Y)\)</span> (with finite ranges) determines a Markov kernel through the conditional distributions, and hence also a communication channel. Let’s formally record this fact:</p>
<div class="proof theorem admonition" id="conditional-markov-kernel-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.4 </span> (Conditional distributions determine communication channels)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables with finite ranges</p>
<div class="math notranslate nohighlight" id="equation-ranges-eq">
<span class="eqno">(10.8)<a class="headerlink" href="#equation-ranges-eq" title="Permalink to this equation">#</a></span>\[
\{x_1,\ldots,x_m\} \quad \text{and} \quad \{y_1,\ldots,y_n\}.
\]</div>
<p>Then the matrix</p>
<div class="math notranslate nohighlight" id="equation-trans-matrix-var-eq">
<span class="eqno">(10.9)<a class="headerlink" href="#equation-trans-matrix-var-eq" title="Permalink to this equation">#</a></span>\[\begin{split}
\bK = [p(y_j|x_i)] = \begin{bmatrix}
p(y_1|x_1) &amp; \cdots &amp; p(y_n|x_1) \\
\vdots &amp; \ddots &amp; \vdots \\
p(y_1|x_m) &amp; \cdots &amp; p(y_n|x_m)
\end{bmatrix}
\end{split}\]</div>
<p>is the transition matrix of a Markov kernel.</p>
</section>
</div><p>Be sure to notice that the form of the transition matrix <a class="reference internal" href="#equation-trans-matrix-var-eq">(10.9)</a> depends on the enumerations of the ranges of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> in <a class="reference internal" href="#equation-ranges-eq">(10.8)</a>; if we change the numbering of the ranges, the transition matrix will change accordingly. For this reason, it is convenient to represent the transition matrices as tables like the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bK = \left\{\begin{array}{c|cc}
p(y|x) &amp; y=0 &amp; y=1 \\ \hline
x = 0 &amp; 0.75 &amp; 0.25 \\
x = 1 &amp; 0.6 &amp; 0.4
\end{array}\right\}.
\end{split}\]</div>
<p>This is the transition matrix determined by two Bernoulli random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Notice that there is nothing special about the random variables in <a class="reference internal" href="#conditional-markov-kernel-thm">Theorem 10.4</a>, besides that they have finite ranges. Indeed, <em>every</em> conditional distribution determines a Markov kernel and thus a communication channel. This is just a new framework in which to conceptualize conditional distributions—they are the mechanism that one random variable uses to send “information” or exert “influence” on another.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do Problem 6 on the worksheet.</p>
</div>
<p>The induced communication channel in <a class="reference internal" href="#conditional-markov-kernel-thm">Theorem 10.4</a> has a directionality to it, from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>. However, there is also the communication channel in the other direction, induced by the conditional distributions <span class="math notranslate nohighlight">\(p(x|y)\)</span>. The mathematical mechanism that allows us to reverse the direction of “flow” is exactly Bayes’ theorem, since it states that</p>
<div class="math notranslate nohighlight">
\[
p(x|y) = \frac{p(y|x)p(x)}{\sum_{x^\star \in \bbr} p(y|x^\star)p(x^\star)}.
\]</div>
<p>So, as long as we know the marginal mass functions <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(p(y)\)</span>, Bayes’ theorem gives us a way to use one communication channel in one direction to obtain the other:</p>
<a class="reference internal image-reference" href="../_images/comm-bayes.svg"><img alt="../_images/comm-bayes.svg" class="align-center" src="../_images/comm-bayes.svg" width="75%" /></a>
<p> </p>
<p>We now seek a way to measure the amount of “information” that passes through a communication channel induced by a pair of random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as in <a class="reference internal" href="#conditional-markov-kernel-thm">Theorem 10.4</a>. Our identification of such a measure begins by studying the case in which it is natural to believe that <em>no</em> information is passed, exactly when the communication channel is <em>constant</em>:</p>
<a class="reference internal image-reference" href="../_images/markov-kern-constant.svg"><img alt="../_images/markov-kern-constant.svg" class="align-center" src="../_images/markov-kern-constant.svg" width="85%" /></a>
<p> </p>
<p>But this turns out to have an interpretation in terms of a familiar previous concept:</p>
<div class="proof theorem admonition" id="ind-markov-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.5 </span> (Independence <span class="math notranslate nohighlight">\(=\)</span> constant Markov kernels)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables with finite ranges</p>
<div class="math notranslate nohighlight">
\[
\{x_1,\ldots,x_m\} \quad \text{and} \quad \{y_1,\ldots,y_n\}.
\]</div>
<p>Then the induced communication channel</p>
<div class="math notranslate nohighlight">
\[
\kappa: \{1,2,\ldots,m\} \to \bbr^n, \quad \kappa(i)^\intercal = \begin{bmatrix} p(y_1|x_i) &amp; \cdots &amp; p(y_n|x_i) \end{bmatrix},
\]</div>
<p>is constant if and only if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. In this case, <span class="math notranslate nohighlight">\(\kappa(i) = \bpi(Y)\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. By the Conditional Criteria for Independence (see <a class="reference internal" href="07-random-vectors.html#conditional-ind-rvs-thm">Theorem 7.9</a>), we have that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent if and only if <span class="math notranslate nohighlight">\(p(y|x) = p(y)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. In particular, if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, then we must have <span class="math notranslate nohighlight">\(p(y_j|x_i) = p(y_j)\)</span> for each <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, and so the transition matrix is of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bK = \begin{bmatrix} p(y_1) &amp; \cdots &amp; p(y_n) \\ \vdots &amp; \ddots &amp; \vdots \\ p(y_1) &amp; \cdots &amp; p(y_n) \end{bmatrix},
\end{split}\]</div>
<p>showing that the Markov kernel is constant with <span class="math notranslate nohighlight">\(\kappa(i) = \bpi(Y)\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>. Conversely, if the Markov kernel is constant so that the transition matrix is of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bK = \begin{bmatrix}
p(y_1|x_1) &amp; \cdots &amp; p(y_n|x_1) \\
\vdots &amp; \ddots &amp; \vdots \\
p(y_1|x_m) &amp; \cdots &amp; p(y_n|x_m)
\end{bmatrix} = \begin{bmatrix} \leftarrow &amp; \bc^\intercal &amp; \rightarrow \\ \vdots &amp; \vdots &amp; \vdots \\ \leftarrow &amp; \bc^\intercal &amp; \rightarrow \end{bmatrix}
\end{split}\]</div>
<p>for some <span class="math notranslate nohighlight">\(\bc = [c_j] \in \bbr^n\)</span>, then by the Law of Total Probability</p>
<div class="math notranslate nohighlight">
\[
\bpi(Y)^\intercal = \bpi(X)^\intercal \bK ,
\]</div>
<p>we must have</p>
<div class="math notranslate nohighlight">
\[
p(y_j) = \sum_{i=1}^m p(x_i)c_{j} = c_j \sum_{i=1}^m p(x_i) = c_j
\]</div>
<p>for each <span class="math notranslate nohighlight">\(j\)</span>. This shows <span class="math notranslate nohighlight">\(\bpi(Y) = \bc\)</span>, and so</p>
<div class="math notranslate nohighlight">
\[
p(y_j) = c_j = p(y_j|x_i)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Thus, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. Q.E.D.</p>
</div>
<p>By the Mass/Density Criteria for Independence (see <a class="reference internal" href="07-random-vectors.html#mass-density-ind-thm">Theorem 7.7</a>), the induced communication channel in <a class="reference internal" href="#ind-markov-thm">Theorem 10.5</a> is constant if and only if the joint mass function of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> factors into the marginals:</p>
<div class="math notranslate nohighlight">
\[
p(x,y) = p(x)p(y),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. This suggests that a measure of “information” passed from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> should be the degree to which the joint mass function <em>does not</em> equal the product of the marginals. But we already have a method for measuring the discrepancy between two distributions! This motivates:</p>
<div class="proof definition admonition" id="mutual-info-def">
<p class="admonition-title"><span class="caption-number">Definition 10.7 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables with finite ranges. The <em>mutual information</em> shared between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, denoted <span class="math notranslate nohighlight">\(I(X,Y)\)</span>, is the KL divergence</p>
<div class="math notranslate nohighlight">
\[
I(X, Y) \def D( P_{XY} \parallel P_{X} \otimes P_{Y}) = \sum_{(x,y)\in \bbr^2} p(x,y) \log\left( \frac{p(x,y)}{p(x)p(y)}\right).
\]</div>
</section>
</div><p>The notation <span class="math notranslate nohighlight">\(P_X \otimes P_Y\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Product_measure"><em>product measure</em></a> on the Cartesian product of the ranges of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>; by definition, this is the measure that has mass function given by</p>
<div class="math notranslate nohighlight">
\[
p_{P_X \otimes P_Y}(x,y) \def p(x)p(y).
\]</div>
<p>When the marginal distributions <span class="math notranslate nohighlight">\(P_X\)</span> and <span class="math notranslate nohighlight">\(P_Y\)</span> are expressed in terms of probability vectors, then the product measure is realized as the <a class="reference external" href="https://en.wikipedia.org/wiki/Outer_product"><em>outer product</em></a> of the vectors, which is also called their <a class="reference external" href="https://en.wikipedia.org/wiki/Tensor_product"><em>tensor product</em></a> by some people (like me).</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do Problem 7 on the worksheet.</p>
</div>
<p>Note that the mutual information <span class="math notranslate nohighlight">\(I(X,Y)\)</span> is always nonnegative, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-mutual-ineq-eq">
<span class="eqno">(10.10)<a class="headerlink" href="#equation-mutual-ineq-eq" title="Permalink to this equation">#</a></span>\[
I(X,Y) \geq 0.
\]</div>
<p>This is because mutual information is a KL divergence, and Gibbs’ inequality therefore applies (see <a class="reference internal" href="#gibbs-thm">Theorem 10.3</a>). As a sanity check, let’s make sure that equality holds in <a class="reference internal" href="#equation-mutual-ineq-eq">(10.10)</a> when (and only when) the communication channel carries no “information”:</p>
<div class="proof theorem admonition" id="info-independence-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.6 </span> (Independence <span class="math notranslate nohighlight">\(=\)</span> zero mutual information)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables with finite ranges</p>
<div class="math notranslate nohighlight">
\[
\{x_1,\ldots,x_m\} \quad \text{and} \quad \{y_1,\ldots,y_n\}.
\]</div>
<p>Then the following statements are equivalent:</p>
<ol class="arabic">
<li><p>The induced communication channel</p>
<div class="math notranslate nohighlight">
\[
    \kappa: \{1,2,\ldots,m\} \to \bbr^n, \quad \kappa(i)^\intercal = \begin{bmatrix} p(y_1|x_i) &amp; \cdots &amp; p(y_n|x_i) \end{bmatrix},
    \]</div>
<p>is constant.</p>
</li>
<li><p>The random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p></li>
<li><p>The mutual information <span class="math notranslate nohighlight">\(I(X,Y) =0\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The equivalence of the first two statements is exactly <a class="reference internal" href="#ind-markov-thm">Theorem 10.5</a>. The equivalence of the second and third follows from the Mass/Density Criteria for Independence (see <a class="reference internal" href="07-random-vectors.html#mass-density-ind-thm">Theorem 7.7</a>) and <a class="reference internal" href="#gibbs-thm">Theorem 10.3</a>. Q.E.D.</p>
</div>
<p>The mutual information between two random variables is <em>defined</em> to be a particular KL divergence, which links it up with one of the information-theoretic quantities from the previous sections. But it <em>also</em> can be expressed in terms of marginal and joint entropies:</p>
<div class="proof theorem admonition" id="other-info-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.7 </span> (Mutual information and entropy)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables with finite ranges. Then:</p>
<div class="math notranslate nohighlight">
\[
I(X,Y) = H(X) + H(Y) - H(X,Y).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof is a computation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
I(x,y) &amp;= \sum_{x\in \bbr}\sum_{y \in \bbr} p(x,y) \log\left( \frac{p(x,y)}{p(x)p(y)} \right) \\
&amp;= - \sum_{x\in \bbr}\sum_{y \in \bbr} p(x,y) \log \left(p(x)\right) - \sum_{x\in \bbr}\sum_{y \in \bbr} p(x,y) \log\left(p(y)\right) \\
&amp;\quad + \sum_{x\in \bbr}\sum_{y \in \bbr} p(x,y) \log\left(p(x,y)\right)  \\
&amp;= - \sum_{x \in \bbr} p(x) \log\left( p(x) \right) - \sum_{y \in \bbr} p(y) \log\left( p(y)\right) - H(X,Y)  \\
&amp;= H(X) + H(Y) - H(X, Y),
\end{align*}\]</div>
<p>as desired. Q.E.D.</p>
</div>
<p>Since the joint entropy <span class="math notranslate nohighlight">\(H(X,Y)\)</span> is symmetric in the random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, i.e., <span class="math notranslate nohighlight">\(H(X,Y) = H(Y,X)\)</span>, we get the following corollary:</p>
<div class="proof corollary admonition" id="symm-info-thm">
<p class="admonition-title"><span class="caption-number">Corollary 10.2 </span> (Symmetry of mutual information)</p>
<section class="corollary-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables with finite ranges. Then <span class="math notranslate nohighlight">\(I(X,Y) = I(Y,X)\)</span>.</p>
</section>
</div><p>We end this section by stating a fundamental inequality that provides further evidence that the abstract notion of <em>mutual information</em> accords with our intuition for transfer or flow of “information” or “influence.” Indeed, suppose that we have a triple of random variables <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(Z\)</span>, and that we consider the “flows” going from one to the other in that order:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is a very simple example of a <em>probabilistic graphical model</em> that we will study in more detail in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>.</p>
</aside>
<a class="reference internal image-reference" href="../_images/markov.svg"><img alt="../_images/markov.svg" class="align-center" src="../_images/markov.svg" width="40%" /></a>
<p> </p>
<p>For a concrete running example, suppose we consider testing for the fictional(!) disease <em>hydromechanical trepidation syndrome</em> (or <em>HTS</em>) that we met back in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_03.ipynb">programming assignment</a> for <a class="reference internal" href="03-rules-of-prob.html#rules-prob"><span class="std std-numref">Chapter 3</span></a>. Suppose <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are (Bernoulli) indicator variables for the presence of the disease and the result of the test:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = \left\{\begin{array}{cl}
0 &amp; : \text{do not have disease} \\
1 &amp; : \text{have disease}
\end{array}\right\} \quad\quad \text{and} \quad\quad Y = \left\{\begin{array}{cl}
0 &amp; : \text{test negative} \\
1 &amp; : \text{test positive}
\end{array} \right\}.
\end{split}\]</div>
<p>Suppose, furthermore, that depending on the result of the test, the subject may leave on their annual vacation—this means that we have a third indicator variable:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Z = \left\{\begin{array}{cl}
0 &amp; : \text{do not go on vacation} \\
1 &amp; : \text{go on vacation}
\end{array}\right\}.
\end{split}\]</div>
<p>Note that the natural of flow of “influence” would indeed go in the order <span class="math notranslate nohighlight">\(X \to Y \to Z\)</span> displayed above.</p>
<p>It is also natural to assume that the <em>only</em> way that the presence of the disease <span class="math notranslate nohighlight">\(X\)</span> influences a subject’s decision <span class="math notranslate nohighlight">\(Z\)</span> to leave on vacation flows through the result of the test <span class="math notranslate nohighlight">\(Y\)</span>. There should be <em>no</em> direct influence from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Z\)</span>, which would be indicated by a third directed edge in the graph:</p>
<a class="reference internal image-reference" href="../_images/markov-02.svg"><img alt="../_images/markov-02.svg" class="align-center" src="../_images/markov-02.svg" width="40%" /></a>
<p> </p>
<p>Another way to understand the absence of a <em>direct</em> influence from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> (that bypasses <span class="math notranslate nohighlight">\(Y\)</span>) is to say that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are <em>conditionally independent</em> given <span class="math notranslate nohighlight">\(Y\)</span>. This means that if a subject knows the result of their test (<span class="math notranslate nohighlight">\(Y=0\)</span> or <span class="math notranslate nohighlight">\(Y=1\)</span>), then whether they leave on vacation (<span class="math notranslate nohighlight">\(Z=0\)</span> or <span class="math notranslate nohighlight">\(Z=1\)</span>) is independent of whether they actually have the disease (<span class="math notranslate nohighlight">\(X=0\)</span> or <span class="math notranslate nohighlight">\(X=1\)</span>). Formally:</p>
<div class="proof definition admonition" id="cond-ind-def">
<p class="admonition-title"><span class="caption-number">Definition 10.8 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(Z\)</span> be three random variables.</p>
<ol class="arabic">
<li><p>If the variables are jointly discrete, then we shall say <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are <em>conditionally independent</em> given <span class="math notranslate nohighlight">\(Y\)</span> if</p>
<div class="math notranslate nohighlight">
\[
    p(x,z|y) = p(x|y)p(z|y)
    \]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span>, and <span class="math notranslate nohighlight">\(z\)</span>.</p>
</li>
<li><p>If the variables are jointly continuous, then we shall say <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are <em>conditionally independent</em> given <span class="math notranslate nohighlight">\(Y\)</span> if</p>
<div class="math notranslate nohighlight">
\[
    f(x,z|y) = f(x|y)f(z|y)
    \]</div>
<p>for all <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(y\)</span>, and <span class="math notranslate nohighlight">\(z\)</span>.</p>
</li>
</ol>
</section>
</div><p>So, in probabilistic models of the form</p>
<a class="reference internal image-reference" href="../_images/markov.svg"><img alt="../_images/markov.svg" class="align-center" src="../_images/markov.svg" width="40%" /></a>
<p> </p>
<p>we always assume that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>With this language, we now state the following fundamental inequality.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The name of this inequality is inspired by engineering applications. However, it is purely a result in abstract information theory. For a proof, see Section 2.8 in <span id="id12">[<a class="reference internal" href="bib.html#id17" title="T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley &amp; Sons, Inc., second edition, 2006.">CT06</a>]</span> or the solutions to the exercises in Section 8 of <span id="id13">[<a class="reference internal" href="bib.html#id25" title="D. J. C. MacKay. Information theory, inference and learning algorithms. Cambridge University Press, 2003.">Mac03</a>]</span>.</p>
</aside>
<div class="proof theorem admonition" id="data-processing-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.8 </span> (Data Processing Inequality)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(Z\)</span> are three random variables with finite ranges, and suppose that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(Y\)</span>. Then</p>
<div class="math notranslate nohighlight" id="equation-data-processing-eq">
<span class="eqno">(10.11)<a class="headerlink" href="#equation-data-processing-eq" title="Permalink to this equation">#</a></span>\[
I(X,Z) \leq I(X,Y),
\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent given <span class="math notranslate nohighlight">\(Z\)</span>.</p>
</section>
</div><p>In plain language, the inequality states that the amount of information that flows from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> along the graph</p>
<a class="reference internal image-reference" href="../_images/markov.svg"><img alt="../_images/markov.svg" class="align-center" src="../_images/markov.svg" width="40%" /></a>
<p> </p>
<p>can be no more than the amount of “information” that flows directly from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>. Indeed, if this were <em>not</em> the case, then we must imagine that somehow additional “information” is “created” in the link from <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(Z\)</span>. But intuition suggests that that is not possible. Or, from another point of view, the inequality says that what <span class="math notranslate nohighlight">\(Z\)</span> “knows” about <span class="math notranslate nohighlight">\(X\)</span> is not more than what <span class="math notranslate nohighlight">\(Y\)</span> “knows” about <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>In our disease/test/vacation model, the inequality <a class="reference internal" href="#equation-data-processing-eq">(10.11)</a> is telling us that whether a subject decides to leave on vacation is not any better a test for the disease than the test itself. That is, unless equality holds in <a class="reference internal" href="#equation-data-processing-eq">(10.11)</a>, in which case we have a model of the form</p>
<a class="reference internal image-reference" href="../_images/markov-03.svg"><img alt="../_images/markov-03.svg" class="align-center" src="../_images/markov-03.svg" width="40%" /></a>
<p> </p>
<p>In this case, whether a subject leaves on vacation <em>is</em> just as good a test for the disease as the test itself; intuitively, all the “information” that <span class="math notranslate nohighlight">\(Y\)</span> carries about <span class="math notranslate nohighlight">\(X\)</span> reaches <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>You will have an opportunity to work with the disease/test/vacation probabilistic model in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/10-homework.md#problem-9-the-data-processing-inequality">homework</a> for this chapter.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="09-halfway.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>The halfway point: pivoting toward models and data analysis</p>
      </div>
    </a>
    <a class="right-next"
       href="11-optim.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#shannon-information-and-entropy">10.1. Shannon information and entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kullback-leibler-divergence">10.2. Kullback Leibler divergence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flow-of-information">10.3. Flow of information</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>