

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>10. Optimization &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bA": "\\mathbf{A}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bS": "\\mathbf{S}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/10-optim';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Probabilistic graphical models" href="11-models.html" />
    <link rel="prev" title="9. Information theory" href="09-info-theory.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-info-theory.html">9. Information theory</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-models.html">11. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-learning.html">12. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-stats-estimators.html">13. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-asymptotic.html">14. Large sample theory and more sampling distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/10-optim.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">10.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curvature-and-derivatives-in-higher-dimensions">10.2. Curvature and derivatives in higher dimensions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">10.3. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">10.4. Stochastic gradient descent</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="optimization">
<span id="optim"></span><h1><span class="section-number">10. </span>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h1>
<p>As with most traditional textbooks in this field, the first half of this book has been almost exclusively concerned with building up the foundations of abstract probability theory. With this task completed, at this point, most textbooks would turn toward applications of this theory to statistics. We, however, will take an excursion over the next three chapters through applications of probability theory to <em>machine learning</em> before proceeding to traditional statistics.</p>
<p>Our excursion begins where we left off in <a class="reference internal" href="06-theory-to-practice.html#theory-to-practice"><span class="std std-numref">Chapter 6</span></a>. In that earlier chapter, we first encountered <em>probabilistic models</em>, but of a very simple type consisting of just a single random variable that depends on a few parameters. Given a real-world dataset and a candidate probabilistic model, the goal is to locate parameter values that minimize the “distance” or discrepancy between the probability distribution proposed by the model and the empirical distribution of the dataset. In the very simple case considered in <a class="reference internal" href="06-theory-to-practice.html#theory-to-practice"><span class="std std-numref">Chapter 6</span></a> consisting of a normal distribution parametrized by its mean and variance, there were intuitively “obvious” parameter values given by the empirical mean and variance of the dataset.</p>
<p>In the <a class="reference internal" href="11-models.html#prob-models"><span class="std std-ref">next chapter</span></a>, we will discuss much more complex types of probabilistic models in which multiple random (and deterministic) variables are connected through “webs of influence.” The number of parameters in these complex models will be correspondingly larger, and in general there will not be such “obvious” choices of parameter values as there were for the simple univariate models considered in <a class="reference internal" href="06-theory-to-practice.html#theory-to-practice"><span class="std std-numref">Chapter 6</span></a>. Instead, a systematic method for finding these parameter values is needed.</p>
<p>One such method was actually foreshadowed above when we mentioned that our goal is to <strong>minimize</strong> the discrepancy between the model and empirical distributions, denoted <span class="math notranslate nohighlight">\(p(x;\btheta)\)</span> and <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> respectively, where <span class="math notranslate nohighlight">\(\btheta\)</span> is the vector of parameters:</p>
<a class="reference internal image-reference" href="../_images/prob-distance.svg"><img alt="../_images/prob-distance.svg" class="align-center" src="../_images/prob-distance.svg" width="75%" /></a>
<p> </p>
<p>Provided that we can cook up a function that measures this discrepancy precisely, our parameter search thus becomes an <em>optimization problem</em> in which we seek minimizers of the discrepancy function. (These latter functions go by many names, such as <em>loss</em>, <em>cost</em>, and <em>risk functions</em>.) At least in the sense that we use it in this book, the <em>learning</em> in <em>machine learning</em> thus amounts to <em>optimization</em>.</p>
<p>This explains and justifies the presence of the current chapter, which is entirely devoted to solving optimization problems. Many of the optimization problems we will encounter do not have closed-form solutions, and so we will need to study methods for approximating solutions. All the methods studied in this chapter are versions of an iterative method called <em>gradient descent</em>. For a warm-up, we will study a simple single-variable version of this method in <a class="reference internal" href="#univariate-grad-desc-sec"><span class="std std-numref">Section 10.1</span></a> before proceeding to the “real” versions in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Sections 10.3</span></a> and <a class="reference internal" href="#sgd-sec"><span class="std std-numref">10.4</span></a>. We will put these methods to use in <a class="reference internal" href="12-learning.html#learning"><span class="std std-numref">Chapter 12</span></a> after learning about general probabilistic models in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>.</p>
<p>One more thing, before beginning: The inclusion of gradient-based optimization algorithms and their applications to parameter estimation is what distinguishes this book from a traditional book on mathematical statistics. This material is often included in texts on machine learning, but it is not in any text on statistics (that I know of). However, we are just <em>barely</em> scratching the surface of optimization and machine learning. If you are new to these fields and want to learn more, I suggest beginning with the fifth chapter of <span id="id1">[<a class="reference internal" href="bib.html#id10" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT Press, 2016.">GBC16</a>]</span> for a quick overview. After this, you can move on to <span id="id2">[<a class="reference internal" href="bib.html#id11" title="M. Hardt and B. Recht. Patterns, predictions, and actions. Foundations of machine learning. Princeton University Press, 2022.">HR22</a>]</span>, before tackling the massive, encyclopedic texts <span id="id3">[<a class="reference internal" href="bib.html#id8" title="K. P. Murphy. Probabilistic machine learning. An introduction. MIT Press, 2022.">Mur22</a>]</span> and <span id="id4">[<a class="reference internal" href="bib.html#id9" title="K. P. Murphy. Probabilistic machine learning. Advanced topics. MIT Press, 2023.">Mur23</a>]</span>.</p>
<section id="gradient-descent-in-one-variable">
<span id="univariate-grad-desc-sec"></span><h2><span class="section-number">10.1. </span>Gradient descent in one variable<a class="headerlink" href="#gradient-descent-in-one-variable" title="Permalink to this heading">#</a></h2>
<p>In this section, we describe the single-variable version of the gradient descent algorithm to help motivate the general algorithm in arbitrary dimensions. To begin, consider the <em>optimization problem</em> of locating the minimum values of the polynomial function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \theta^4 - 6\theta^3 + 11\theta^2 - 7\theta + 4.
\]</div>
<p>This function is called the <em>objective function</em> of the optimization problem. Its graph is displayed in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.distributions.multivariate_normal</span> <span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">11</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">7</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/613cb21dd92442536ae31405deca30c94f8bd5420d49326a7641207fffc861e9.svg" src="../_images/613cb21dd92442536ae31405deca30c94f8bd5420d49326a7641207fffc861e9.svg" /></figure>
</div>
</div>
<p>From the graph, we see that the objective function has minimums of approximately <span class="math notranslate nohighlight">\(J(0.5)\)</span> and <span class="math notranslate nohighlight">\(J(2.7)\)</span>. By definition, a number <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a <em>local minimizer</em> of <span class="math notranslate nohighlight">\(J(\theta)\)</span> provided that</p>
<div class="math notranslate nohighlight">
\[
J(\theta^\star) \leq J(\theta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\theta\)</span> in a neighborhood of <span class="math notranslate nohighlight">\(\theta^\star\)</span>; if this inequality holds for <em>all</em> <span class="math notranslate nohighlight">\(\theta\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is called a <em>global minimizer</em> of <span class="math notranslate nohighlight">\(J(\theta)\)</span>. If we flip the inequality the other direction, then we obtain the definitions of <em>local</em> and <em>global maximizers</em>. Collectively, local and global minimizers and maximizers of <span class="math notranslate nohighlight">\(J(\theta)\)</span> are called <em>extremizers</em>, and the values <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span> of the function where <span class="math notranslate nohighlight">\(\theta^\star\)</span> is an extremizer are called <em>extrema</em> or <em>extreme values</em>. Using this terminology, we would therefore say that <span class="math notranslate nohighlight">\(0.5\)</span> is (approximately) a local minimizer of <span class="math notranslate nohighlight">\(J(\theta)\)</span>, while <span class="math notranslate nohighlight">\(2.7\)</span> is (approximately) a global minimizer.</p>
<p>Let’s see how the single-variable version of the <em>gradient descent (GD) algorithm</em> would solve this optimization problem. In this context, the GD algorithm is called the <em>optimizer</em>. This algorithm depends on an initial guess for a minimizer, as well as two parameters called the <em>learning rate</em> and the <em>number of gradient steps</em>. We will state the algorithm first, and then walk through some intuition for why it works:</p>
<div class="proof algorithm admonition" id="single-var-gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 10.1 </span> (Single-variable gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta - \alpha J'(\theta)\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</section>
</div><p>Beginning from an initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span> for a minimizer, the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop in the GD algorithm outputs a sequence of approximations <span class="math notranslate nohighlight">\(\theta_1,\ldots,\theta_t,\ldots,\theta_N\)</span> for a minimizer. The last value <span class="math notranslate nohighlight">\(\theta_N\)</span> in the sequence is taken as the output of the algorithm; if the algorithm converges to a minimizer, then we should have <span class="math notranslate nohighlight">\(\theta_N \approx \theta^\star\)</span>.</p>
<p>The equation</p>
<div class="math notranslate nohighlight" id="equation-update-rule-eqn">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-update-rule-eqn" title="Permalink to this equation">#</a></span>\[\theta_t := \theta_{t-1} - \alpha J'(\theta_{t-1})\]</div>
<p>in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is called the <em>update rule</em>; we say that the new parameter <span class="math notranslate nohighlight">\(\theta_t\)</span> is obtained by taking a <em>gradient step</em> from <span class="math notranslate nohighlight">\(\theta_{t-1}\)</span>. The first update occurs when <span class="math notranslate nohighlight">\(t=1\)</span>, yielding</p>
<div class="math notranslate nohighlight">
\[
\theta_1 := \theta_{0} - \alpha J'(\theta_{0}).
\]</div>
<p>To understand the intuition for this rule, consider the two cases that the derivative <span class="math notranslate nohighlight">\(J'(\theta_0)\)</span> is positive or negative:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J_prime</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="mi">18</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">22</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="mi">7</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">+</span> <span class="mf">0.4</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">12.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="sa">r</span><span class="s2">&quot;$J&#39;(\theta_0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="mf">3.3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">-</span> <span class="mf">3.3</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">3.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.9</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="sa">r</span><span class="s2">&quot;$J&#39;(\theta_0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/0d2e5d39c89d3959ae6c1e4c25b7e04d070d2f62579e06781af035c369f48275.svg" src="../_images/0d2e5d39c89d3959ae6c1e4c25b7e04d070d2f62579e06781af035c369f48275.svg" /></figure>
</div>
</div>
<p>In this plot, we’ve drawn the tangent lines to the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> at two initial values <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and <span class="math notranslate nohighlight">\(\theta_0=3.3\)</span>. Since the derivatives are the slopes of these tangent lines, the sign of the derivative is negative when <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and positive when <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span>. In the first case, we have</p>
<div class="math notranslate nohighlight" id="equation-first-update-eqn">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-first-update-eqn" title="Permalink to this equation">#</a></span>\[\theta_1 = -0.4 - \alpha J'(-0.4) &gt; -0.4\]</div>
<p>since <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, while in the second case we have</p>
<div class="math notranslate nohighlight" id="equation-second-update-eqn">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-second-update-eqn" title="Permalink to this equation">#</a></span>\[\theta_1 = 3.3 - \alpha J'(3.3) &lt; 3.3.\]</div>
<p>But notice that the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = -0.4\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 0.5\)</span>, and so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-first-update-eqn">(10.2)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than the initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span>, provided that the (scaled) negative derivative</p>
<div class="math notranslate nohighlight" id="equation-neg-derivative-eqn">
<span class="eqno">(10.4)<a class="headerlink" href="#equation-neg-derivative-eqn" title="Permalink to this equation">#</a></span>\[-\alpha J'(\theta_0)\]</div>
<p>is not too large (in magnitude) causing the new <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” the minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Similarly, the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>, so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-second-update-eqn">(10.3)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than <span class="math notranslate nohighlight">\(\theta_0\)</span>, again provided that the (scaled) negative derivative <a class="reference internal" href="#equation-neg-derivative-eqn">(10.4)</a> is not too large in magnitude.</p>
<p>From these considerations, we conclude the following:</p>
<div class="proof observation admonition" id="gd-obs">
<p class="admonition-title"><span class="caption-number">Observation 10.1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>The negative derivative <span class="math notranslate nohighlight">\(-J'(\theta)\)</span> always “points downhill.”</p></li>
<li><p>When the gradient descent algorithm works, it locates a minimizer by following the negative derivative “downhill.”</p></li>
</ul>
</section>
</div><p>The sense in which the negative derivative “points downhill” is made precise by our observation that it is positive if the point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> sits on a decreasing portion of the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>, and it is negative if <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> is on an increasing portion of the graph. The role of the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is to scale down the magnitude of the negative derivative so that the gradient step in the update rule does not cause <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” a nearby minimizer.</p>
<p>Let’s run the GD algorithm four times, with various settings of the parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the gradient descent function</span>
<span class="k">def</span> <span class="nf">GD</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    
    <span class="c1"># initialize lists to track objective values and thetas</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_thetas</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># begin gradient descent loop</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>

        <span class="c1"># compute objective with current theta</span>
        <span class="n">objective</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        
        <span class="c1"># compute gradients</span>
        <span class="n">objective</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="c1"># append current objective and theta to running lists</span>
        <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">running_thetas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        
        <span class="c1"># take a step and update the theta</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">g</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">theta</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">theta</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>

        <span class="c1"># zero out the gradient to prepare for the next iteration</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="c1"># output tensors instead of lists</span>
    <span class="n">running_thetas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">running_thetas</span><span class="p">)</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">running_thetas</span><span class="p">,</span> <span class="n">running_objectives</span>

<span class="c1"># plot objective function</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>

<span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">2e-1</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/2b8e184507d1de3e23a4c517c94c7fb89c5c4700b32c4939bccc45f93556c2d5.svg" src="../_images/2b8e184507d1de3e23a4c517c94c7fb89c5c4700b32c4939bccc45f93556c2d5.svg" /></figure>
</div>
</div>
<p>In all four plots, the large magenta dot represents the initial point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span>, while the smaller dots represent the points</p>
<div class="math notranslate nohighlight">
\[
(\theta_1,J(\theta_1)), (\theta_2, J(\theta_2)),\ldots, (\theta_N,J(\theta_N)),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of gradient steps in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop in the GD algorithm. In the first row, the algorithm appears to be converging in both cases to the nearest minimizer to the initial guesses. In the second row, the learning rate is (relatively) large, causing the first gradient steps to “overshoot” the nearest minimizers to the initial guesses. However, the algorithm still appears to converge in both cases.</p>
<p>It is possible for the GD algorithm to diverge, especially if the learning rate is too large. For example, suppose that we set the learning rate to <span class="math notranslate nohighlight">\(\alpha = 0.2\)</span> and use <span class="math notranslate nohighlight">\(\theta_0 = 3.5\)</span> as our initial guess. Then three steps of gradient descent produce the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-1</span><span class="p">,}</span>

<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">55</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/125a1146393dabdcf6ba8e9e5e7d3be332ddfa906fbeb384c4ef611b9d85ec23.svg" src="../_images/125a1146393dabdcf6ba8e9e5e7d3be332ddfa906fbeb384c4ef611b9d85ec23.svg" /></figure>
</div>
</div>
<p>We see already that <span class="math notranslate nohighlight">\(J(\theta_3) \approx 10^7\)</span>; in fact, we have <span class="math notranslate nohighlight">\(J(\theta_t) \to \infty\)</span> as <span class="math notranslate nohighlight">\(t\to\infty\)</span> for these particular parameters. Of course, one can often prevent divergence by simply using a smaller learning rate, but sometimes a large <em>initial</em> learning rate is desirable to help the algorithm quickly find the neighborhood of a minimizer. So, what we desire is a scheme to shrink the learning rate from (relatively) large values to (relatively) smaller ones as the algorithm runs. This scheme is called <em>learning rate decay</em> or <em>rate decay</em>.</p>
<div class="proof algorithm admonition" id="single-variable-gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 10.2 </span> (Single-variable gradient descent with rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta - \alpha (1 - \gamma)^t J'(\theta)\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</section>
</div><p>Setting <span class="math notranslate nohighlight">\(\gamma=0\)</span> results in <em>no</em> rate decay. In our diverging example above, setting <span class="math notranslate nohighlight">\(\gamma=0.2\)</span> results in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-1</span><span class="p">,}</span>

<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/6ee684e4fbf4ff61c523b1d21e543e40323701b70a7460192631f2e48e8f0617.svg" src="../_images/6ee684e4fbf4ff61c523b1d21e543e40323701b70a7460192631f2e48e8f0617.svg" /></figure>
</div>
</div>
<p>We have carried out <span class="math notranslate nohighlight">\(N=8\)</span> gradient steps, and it appears that the algorithm has successfully located the minimizer <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>.</p>
<p>The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> and the decay rate <span class="math notranslate nohighlight">\(\gamma\)</span> are often chosen by experimentation.</p>
<div class="admonition-tip admonition">
<p class="admonition-title">Tip</p>
<p>When using the gradient descent algorithm to solve an optimization problem, try beginning with a learning and decay rate around <span class="math notranslate nohighlight">\(\alpha \approx 0.01\)</span> and <span class="math notranslate nohighlight">\(\gamma \approx 0.1\)</span>, respectively.</p>
</div>
<p>These values may be tuned by the analyst by closely monitoring the values of the objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> as the algorithm runs. This is easy in the single-variable case, since one can plot the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>. In the multi-variable case, however, the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> may live in many more dimensions than we can visualize, so the analyst might track the values of the objective function against the number of gradient steps. For example, with our polynomial objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> from above and</p>
<div class="math notranslate nohighlight">
\[
\theta_0 = -0.5, \quad \alpha = 0.01, \quad \gamma = 0.1,
\]</div>
<p>we would plot the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,}</span>

<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)),</span> <span class="n">running_objectives</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/454d924fc9672553262b1758f7f58232fb65e481ce2443b3d46e6d13c8658042.svg" src="../_images/454d924fc9672553262b1758f7f58232fb65e481ce2443b3d46e6d13c8658042.svg" /></figure>
</div>
</div>
<p>One may use this plot to decide on the total number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps; simply choose <span class="math notranslate nohighlight">\(N\)</span> large enough to reach a point where the plot “plateaus” or “levels out,” indicating that the algorithm is converging on a minimizer. Alternatively, the analyst may build an automatic stopping condition into the algorithm that halts when the magnitude between successive objective values is less than some chosen threshold, say</p>
<div class="math notranslate nohighlight">
\[
|J(\theta_t) - J(\theta_{t-1}) | &lt; \epsilon,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> is a small number.</p>
</section>
<section id="curvature-and-derivatives-in-higher-dimensions">
<h2><span class="section-number">10.2. </span>Curvature and derivatives in higher dimensions<a class="headerlink" href="#curvature-and-derivatives-in-higher-dimensions" title="Permalink to this heading">#</a></h2>
<p>As you certainly remember from elementary calculus, if <span class="math notranslate nohighlight">\(\theta^\star\)</span> is an extremizer of a differentiable function <span class="math notranslate nohighlight">\(J:\bbr \to \bbr\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> must be a <em>stationary point</em> in the sense that</p>
<div class="math notranslate nohighlight" id="equation-stationary-eqn">
<span class="eqno">(10.5)<a class="headerlink" href="#equation-stationary-eqn" title="Permalink to this equation">#</a></span>\[
J'(\theta^\star)=0.
\]</div>
<p>The name arises from the observation that small (first-order infinitesimal) perturbations of <span class="math notranslate nohighlight">\(\theta^\star\)</span> do not change the value <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span>, i.e., the value <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span> remains <em>stationary</em> under small perturbations. In certain very favorable situations, we may be able to solve the stationarity equation <a class="reference internal" href="#equation-stationary-eqn">(10.5)</a> for <span class="math notranslate nohighlight">\(\theta^\star\)</span> to obtain a formula in closed form. In this case, the iterative gradient descent algorithm is not be needed.</p>
<p>You certainly also remember that the stationarity equation is only a <em>necessary</em> condition for <span class="math notranslate nohighlight">\(\theta^\star\)</span> to be an extremizer; sufficient conditions may be obtained by considering the <em>local curvature</em> of the graph of <span class="math notranslate nohighlight">\(J\)</span> near <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Furthermore, not only do curvature considerations help us identify extremizers, such considerations also help us gauge the speed of convergence of iterative algorithms like gradient descent, as we will see later in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Section 10.3</span></a>.</p>
<p>Our goal in this section is twofold: First, we briefly recall how local curvature helps us identify extremizers in the single-variable case. The relevant tools are the first and second derivatives. Then, we generalize these derivatives to higher dimensions obtaining gadgets called <em>gradient vectors</em> and <em>Hessian matrices</em>. We finish by indicating how the local curvature in higher dimensions may be computed using these new tools and, in particular, how we may use them to identify extremizers.</p>
<p>So, let’s begin with the familiar routine from single-variable calculus called the <em>Second Derivative Test</em>. Given a point <span class="math notranslate nohighlight">\(\theta^\star\)</span> and a twice-differentiable function <span class="math notranslate nohighlight">\(J:\bbr \to \bbr\)</span>, the test splits into two cases:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(J'(\theta^\star) = 0\)</span> and <span class="math notranslate nohighlight">\(J''(\theta^\star) &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a local minimizer.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(J'(\theta^\star) = 0\)</span> and <span class="math notranslate nohighlight">\(J''(\theta^\star) &lt; 0\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a local maximizer.</p></li>
</ol>
<p>Provided that the second derivative of <span class="math notranslate nohighlight">\(J\)</span> not only exists but is also <em>continuous</em>, then intuition for the Second Derivative Test may be explained via local curvature. Indeed, in the first case, positivity of the second derivative means that the graph of <span class="math notranslate nohighlight">\(J\)</span> is <em>convex</em> near <span class="math notranslate nohighlight">\(\theta^\star\)</span>, while in the second case negativity of the second derivative means that the graph is <em>concave</em>. The nature of the curvature helps us distinguish between minimizers and maximizers:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">functions</span><span class="p">,</span> <span class="n">axes</span><span class="p">)):</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;$J &#39;(0) = 0$, $J&#39;&#39;(0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;convex $\Rightarrow$ minimizer&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;$J &#39;(0) = 0$, $J &#39;&#39;(0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;concave $\Rightarrow$ maximizer&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c8432360b89662057cceaa0f1abfe218c6c1a875c7869d04e603918bd0c1c50a.svg" src="../_images/c8432360b89662057cceaa0f1abfe218c6c1a875c7869d04e603918bd0c1c50a.svg" /></figure>
</div>
</div>
<p>To see <em>why</em> second derivatives encode local curvature, let’s suppose <span class="math notranslate nohighlight">\(J''(\theta^\star) &gt;0\)</span> at some point <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Then continuity of <span class="math notranslate nohighlight">\(J''\)</span> means that there is a number <span class="math notranslate nohighlight">\(\dev&gt;0\)</span> such that <span class="math notranslate nohighlight">\(J''(\theta)&gt;0\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span> in the open interval <span class="math notranslate nohighlight">\(I = (\theta^\star - \dev, \theta^\star + \dev)\)</span> centered at <span class="math notranslate nohighlight">\(\theta^\star\)</span>. But the second derivative is the first derivative of the first derivative, and thus positivity of <span class="math notranslate nohighlight">\(J''\)</span> over <span class="math notranslate nohighlight">\(I\)</span> means that <span class="math notranslate nohighlight">\(J'\)</span> is increasing over <span class="math notranslate nohighlight">\(I\)</span>. But the first derivative <span class="math notranslate nohighlight">\(J'\)</span> measures the slope of the graph of <span class="math notranslate nohighlight">\(J\)</span>, and thus these slopes must increase as we move from left to right over <span class="math notranslate nohighlight">\(I\)</span>. But this must mean that <span class="math notranslate nohighlight">\(J\)</span> is convex!</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>For the simple form of Taylor’s Theorem used here, see Theorem 1 in Chapter 20 of <span id="id5">[]</span>. It does <em>not</em> require that the second derivative is continuous.</p>
</aside>
<p>These curvature considerations make it intuitively clear why the sign of the second derivative <span class="math notranslate nohighlight">\(J''(\theta^\star)\)</span> (provided it is not zero) is enough to tell us whether we have located a minimizer or maximizer at a stationary point <span class="math notranslate nohighlight">\(\theta^\star\)</span>. A more rigorous argument is based on Taylor’s Theorem, which says that for any pair of distinct numbers <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\theta^\star\)</span> we may write</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = J(\theta^\star) + J'(\theta^\star) (\theta-\theta^\star) + \frac{1}{2} J''(\theta^\star) (\theta-\theta^\star)^2 + o( (\theta-\theta^\star)^2 )
\]</div>
<p>where the asymptotic “little-oh” notation <span class="math notranslate nohighlight">\(o( (\theta-\theta^\star)^2)\)</span> indicates a remainder term that goes to <span class="math notranslate nohighlight">\(0\)</span> <em>faster</em> than <span class="math notranslate nohighlight">\((\theta-\theta^\star)^2\)</span> does as <span class="math notranslate nohighlight">\(\theta \to \theta^\star\)</span>. But then we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{\theta \to \theta^\star} \frac{J(\theta) - J(\theta^\star)}{(\theta-\theta^\star)^2} = \frac{1}{2} J''(\theta^\star)
\]</div>
<p>provided <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a stationary point. From this we conclude that <span class="math notranslate nohighlight">\(J(\theta) &gt; J(\theta^\star)\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span> near <span class="math notranslate nohighlight">\(\theta^\star\)</span> if <span class="math notranslate nohighlight">\(J''(\theta^\star)\)</span> is positive, which shows that <span class="math notranslate nohighlight">\(\theta^\star\)</span> is indeed a local minimizer. Similar considerations apply if instead <span class="math notranslate nohighlight">\(J''(\theta^\star)\)</span> is negative.</p>
<p>How might these considerations and concepts generalize to higher dimensions?</p>
<p>To answer this question, let’s first increment the dimension by one, supposing that we have a twice-differentiable function</p>
<div class="math notranslate nohighlight">
\[
J:\bbr^2 \to \bbr, \quad \btheta \mapsto J(\btheta),
\]</div>
<p>by which we mean all second-order partial derivatives of <span class="math notranslate nohighlight">\(J\)</span> exist. Actually, to obtain the best results relating curvature to second derivatives, we shall assume moreover that the second-order partial derivatives are also <em>continuous</em>, though this isn’t strictly needed for several of the definitions and results below. Functions with continuous first- and second-order partial derivatives are said to be of <em>class <span class="math notranslate nohighlight">\(C^2\)</span></em> in the mathematical literature.</p>
<p>For example, let’s suppose that the graph of <span class="math notranslate nohighlight">\(J\)</span> is an upside down paraboloid:</p>
<a class="reference internal image-reference" href="../_images/paraboloid-plot.png"><img alt="../_images/paraboloid-plot.png" class="align-center" src="../_images/paraboloid-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>At any given point on this surface (like the one above the black dot) there is not a <em>single</em> slope and curvature, but rather <em>infinitely many</em> slopes and curvatures in all the different directions that one may step in the plane <span class="math notranslate nohighlight">\(\bbr^2\)</span>. These different directions may be represented as <em>directional vectors</em> in the plane; here are three examples:</p>
<a class="reference internal image-reference" href="../_images/directional-plot.png"><img alt="../_images/directional-plot.png" class="align-center" src="../_images/directional-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>Taking advantage of the very special circumstance that our graph is embedded as a surface in <span class="math notranslate nohighlight">\(\bbr^3\)</span>, we may visualize the slopes and curvatures in these three directions by first intersecting the graph with three vertical planes:</p>
<a class="reference internal image-reference" href="../_images/sectional-plot.png"><img alt="../_images/sectional-plot.png" class="align-center" src="../_images/sectional-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>The intersections of these vertical planes and the surface yield curves called <em>sections</em>. For the planes displayed in the plot above, the sections are a trio of downward opening parabolas. The slopes and curvatures on the surface in the three directions are then the slopes and curvatures of these sections. It is thus of interest to obtain <em>formulas</em> for these sectional curves.</p>
<p>To obtain them, let’s take a step back from our specific example and consider a general function</p>
<div class="math notranslate nohighlight">
\[
J:\bbr^n \to \bbr, \quad \btheta \mapsto J(\btheta),
\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note that we are <em>not</em> requiring our directional vectors to have unit length!</p>
</aside>
<p>of class <span class="math notranslate nohighlight">\(C^2\)</span>. Let’s suppose that we have an arbitrary point <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> at which we want to compute the directional slopes and curvatures. The direction will be represented by a <em>directional vector</em>, which is simply a vector <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span>. As we let <span class="math notranslate nohighlight">\(r\in \bbr\)</span> vary, the vector sum</p>
<div class="math notranslate nohighlight">
\[
r \bv + \btheta
\]</div>
<p>traces out a line in <span class="math notranslate nohighlight">\(\bbr^n\)</span> through <span class="math notranslate nohighlight">\(\btheta\)</span> and in the direction of <span class="math notranslate nohighlight">\(\bv\)</span>. The mapping</p>
<div class="math notranslate nohighlight">
\[
r\mapsto J(r\bv + \btheta)
\]</div>
<p>then traces out a <span class="math notranslate nohighlight">\(1\)</span>-dimensional curve on the graph of <span class="math notranslate nohighlight">\(J\)</span> embedded as a hypersurface in <span class="math notranslate nohighlight">\(\bbr^{n+1}\)</span>. Generalizing from our considerations above, we then expect that the first and second derivatives of this mapping should yield the desired directional slopes and curvatures. This motivates the following:</p>
<div class="proof definition admonition" id="directional-der-def">
<p class="admonition-title"><span class="caption-number">Definition 10.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^2 \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point, and <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span> a directional vector. We define the <em>directional first derivative of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> in the direction <span class="math notranslate nohighlight">\(\bv\)</span></em> to be</p>
<div class="math notranslate nohighlight">
\[
J_\bv'(\btheta) \def \frac{\text{d}}{\text{d}r} \bigg|_{r=0} J(r\bv + \btheta),
\]</div>
<p>while we define the <em>directional second derivative</em> to be</p>
<div class="math notranslate nohighlight">
\[
J_\bv''(\btheta) \def \frac{\text{d}^2}{\text{d}r^2} \bigg|_{r=0} J(r\bv + \btheta).
\]</div>
</section>
</div><p>The familiar relations between these directional derivatives and partial deriatives pass through the gadgets defined in the following box. The first is familiar to us from our course in multi-variable calculus:</p>
<div class="proof definition admonition" id="grad-vec-def">
<p class="admonition-title"><span class="caption-number">Definition 10.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^2 \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point. We define the <em>gradient vector</em> to be</p>
<div class="math notranslate nohighlight">
\[
\nabla (J(\btheta)) \def \begin{bmatrix} \displaystyle \frac{\partial J}{\partial \theta_i}(\btheta) \end{bmatrix}_i \in \bbr^n,
\]</div>
<p>while we define the the <em>Hessian matrix</em> to be</p>
<div class="math notranslate nohighlight">
\[
\text{Hess}(J(\btheta)) \def \begin{bmatrix} \displaystyle \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}(\btheta) \end{bmatrix}_{ij} \in \bbr^{n\times n}.
\]</div>
</section>
</div><p>Note that since <span class="math notranslate nohighlight">\(J\)</span> is of class <span class="math notranslate nohighlight">\(C^2\)</span>, the Hessian matrix is symmetric.</p>
<p>The following important theorem expresses the relations between the first and second directional derivatives and the gradient vector and Hessian matrix.</p>
<div class="proof theorem admonition" id="directional-der-grad-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.1 </span> (Slopes, curvatures, and partial derivatives)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point, and <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span> a directional (unit) vector.</p>
<ol class="arabic">
<li><p>We have</p>
<div class="math notranslate nohighlight">
\[
    J_{\bv}'(\btheta) = \bv^\intercal \nabla(J(\btheta)).
    \]</div>
</li>
<li><p>We have</p>
<div class="math notranslate nohighlight">
\[
    J_{\bv}''(\btheta) = \bv^\intercal \text{Hess}(J(\btheta)) \bv.
    \]</div>
</li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proofs are simple exercises using the multi-variable Chain Rule. Indeed, note that</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d}}{\text{d}r} J(r \bv + \btheta) = \sum_{i=1}^n v_i \frac{\partial J}{\partial \theta_i} (r\bv + \btheta).
\]</div>
<p>Plugging in <span class="math notranslate nohighlight">\(r=0\)</span> to both sides of this last equality then yields (1.). On the other hand, differentiating both sides with respect to <span class="math notranslate nohighlight">\(r\)</span> (and using the Chain Rule a second time) gives</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d}^2}{\text{d}r^2} J(r \bv + \btheta) = \sum_{i=1}^n v_i \frac{\text{d}}{\text{d}r}\frac{\partial J}{\partial \theta_i} (r\bv + \btheta)  = \sum_{i,j=1}^n v_i v_j \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}(r\bv + \btheta).
\]</div>
<p>Plugging in <span class="math notranslate nohighlight">\(r=0\)</span> to both ends of this last sequence of equations yields (2.). Q.E.D.</p>
</div>
<p>When the directional vector <span class="math notranslate nohighlight">\(\bv\)</span> is a <em>unit</em> vector, the value of the directional first derivative <span class="math notranslate nohighlight">\(J'_\bv(\btheta)\)</span> is interpreted as the (instantaneous) rate of change of <span class="math notranslate nohighlight">\(J\)</span> in the direction indicated by <span class="math notranslate nohighlight">\(\bv\)</span>. Likewise, if <span class="math notranslate nohighlight">\(\bv\)</span> is unit vector, then the value of the directional second derivative <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is interpreted as the local curvature of <span class="math notranslate nohighlight">\(J\)</span> in the direction indicated by <span class="math notranslate nohighlight">\(\bv\)</span>.</p>
<p>For our purposes, the most important property of the gradient vector <span class="math notranslate nohighlight">\(\nabla (J(\btheta))\)</span> is that it points in the direction of <em>maximum</em> rate of change. This is a direct consequence of the fact that the gradient vector is orthogonal to the <em>level (hyper)surfaces</em> of <span class="math notranslate nohighlight">\(J\)</span>, otherwise called <em>contours</em>. By definition, such a <em>level surface</em> is the <span class="math notranslate nohighlight">\((n-1)\)</span>-dimensional set of solutions <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> to an equation</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = c
\]</div>
<p>for fixed <span class="math notranslate nohighlight">\(c\in \bbr\)</span>. In the case that <span class="math notranslate nohighlight">\(n=2\)</span>, these level surfaces are actually <em>level curves</em>; for our upside down paraboloid above, they are the blue ellipses in the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">15</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">grid_1d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">grid_1d</span><span class="p">,</span> <span class="n">grid_1d</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">8</span> <span class="o">/</span> <span class="mf">3.5</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.75</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.75</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.5</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/0f0016e76513e2ab569f93343a5d28bbc18a05a94d2fdb4dc22f4e45d8498ed3.svg" src="../_images/0f0016e76513e2ab569f93343a5d28bbc18a05a94d2fdb4dc22f4e45d8498ed3.svg" /></figure>
</div>
</div>
<p>In the case that <span class="math notranslate nohighlight">\(n=2\)</span>, the fact that the gradient vector points in the direction of maximum rate of change is often described by saying that it points in the direction of the steepest ascent, visualizing the graph of <span class="math notranslate nohighlight">\(J\)</span> as a series of hills and valleys. In our contour plot above, we have drawn a gradient vector with its tail on a level curve. The straight line is the tangent line to the level curve, and it appears that the gradient is indeed orthogonal to this latter line.</p>
<p>It is intuitively clear from the equation</p>
<div class="math notranslate nohighlight">
\[
J_{\bv}'(\btheta) = \bv^\intercal \nabla(J(\btheta))
\]</div>
<p>in <a class="reference internal" href="#directional-der-grad-thm">Theorem 10.1</a> that the gradient is orthogonal to level surfaces. Indeed, if <span class="math notranslate nohighlight">\(\bv\)</span> is a tangent vector to the level surface passing through <span class="math notranslate nohighlight">\(\btheta\)</span>, then <span class="math notranslate nohighlight">\(J\)</span> should <em>not</em> change (at least up to first order) as we step in the direction of <span class="math notranslate nohighlight">\(\bv\)</span> since (by definition) the function <span class="math notranslate nohighlight">\(J\)</span> is constant along its level surfaces. Thus, we have <span class="math notranslate nohighlight">\(J_\bv'(\btheta)=0\)</span>, and so <span class="math notranslate nohighlight">\(\bv^\intercal \nabla(J(\btheta))=0\)</span>. This shows <span class="math notranslate nohighlight">\(\nabla(J(\btheta))\)</span> is indeed orthogonal to the level surface passing through <span class="math notranslate nohighlight">\(\btheta\)</span>. (For a more rigorous argument, see the proposition on page 23 of <span id="id6">[<a class="reference internal" href="bib.html#id13" title="V. Guillemin and A. Pollack. Differential topology. AMS Chelsea Publishing, 2010.">GP10</a>]</span>.)</p>
<p>Using these observations, we can easily prove the fundamental fact that the gradient vector “points uphill”, while its negative “points downhill”:</p>
<div class="proof theorem admonition" id="grad-uphill-def">
<p class="admonition-title"><span class="caption-number">Theorem 10.2 </span> (Gradient vectors point uphill)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point. Assuming it is nonzero, the gradient vector <span class="math notranslate nohighlight">\(\nabla(J(\btheta))\)</span> points in the direction of maximum rate of change at <span class="math notranslate nohighlight">\(\btheta\)</span>, while the negative gradient vector <span class="math notranslate nohighlight">\(-\nabla(J(\btheta))\)</span> points in the direction of minimum rate of change.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(\be_1,\ldots,\be_{n-1}\)</span> be an orthonormal basis of the tangent space of the level surface through <span class="math notranslate nohighlight">\(\btheta\)</span>. Letting <span class="math notranslate nohighlight">\(\bg = \nabla(J(\btheta)) / || \nabla(J(\btheta)) || \)</span> be the normalized gradient vector, from our considerations above we conclude that</p>
<div class="math notranslate nohighlight" id="equation-onb-eqn">
<span class="eqno">(10.6)<a class="headerlink" href="#equation-onb-eqn" title="Permalink to this equation">#</a></span>\[
\be_1,\ldots,\be_{n-1},\bg
\]</div>
<p>is an orthonormal basis of <span class="math notranslate nohighlight">\(\bbr^n\)</span>. Thus, given a unit directional vector <span class="math notranslate nohighlight">\(\bv\)</span>, there are unique scalars <span class="math notranslate nohighlight">\(\alpha_1,\ldots,\alpha_{n-1},\beta\in \bbr\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\bv = \alpha_1 \be_1 + \cdots + \alpha_{n-1} \be_{n-1} + \beta \bg.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\bv\)</span> is a unit vector and <a class="reference internal" href="#equation-onb-eqn">(10.6)</a> is an orthonormal basis, we must have</p>
<div class="math notranslate nohighlight">
\[
\alpha_1^2 + \cdots + \alpha_{n-1}^2 + \beta^2 = 1.
\]</div>
<p>(This latter equation is a consequence of <a class="reference external" href="https://en.wikipedia.org/wiki/Parseval%27s_identity#Generalization_of_the_Pythagorean_theorem">Parseval’s identity</a>.) From <a class="reference internal" href="#directional-der-grad-thm">Theorem 10.1</a>, we get that</p>
<div class="math notranslate nohighlight">
\[
J'_{\bv}(\btheta) = \bv^\intercal \nabla(J(\btheta)) = \beta || \nabla(J(\btheta)) || .
\]</div>
<p>This latter quantity will be maximized when <span class="math notranslate nohighlight">\(\beta=1\)</span> and minimized when <span class="math notranslate nohighlight">\(\beta=-1\)</span>. The desired results follow. Q.E.D.</p>
</div>
<p>Observe that the part about the negative gradient vector <span class="math notranslate nohighlight">\(-\nabla(J(\btheta))\)</span> “pointing downhill” is exactly the higher-dimensional version of the observation in <a class="reference internal" href="#gd-obs">Observation 10.1</a> regarding the negative derivative of a single-variable function. Just like its single-variable cousin, this will be key to the general multi-variable gradient descent algorithm that we will discuss in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Section 10.3</span></a> below.</p>
<p>Let’s now turn toward minimizers and maximizers of a multi-variable function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span>. They are defined just like in the single-variable case: A point <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a <em>local minimizer</em> if</p>
<div class="math notranslate nohighlight">
\[
J(\btheta^\star) \leq J(\btheta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\btheta\)</span> in a neighborhood of <span class="math notranslate nohighlight">\(\btheta^\star\)</span>; if this inequality holds for <em>all</em> <span class="math notranslate nohighlight">\(\btheta\)</span>, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is called a <em>global minimizer</em>. Flipping the inequality the other direction gives us the definitions of <em>local</em> and <em>global maximizers</em>. Collectively, local and global minimizers and maximizers are called <em>extremizers</em>.</p>
<p>As I hope you remember from multi-variable calculus, the stationarity equation</p>
<div class="math notranslate nohighlight">
\[
\nabla(J(\btheta^\star)) = 0
\]</div>
<p>is a <em>necessary</em> condition for <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to be an extremizer of <span class="math notranslate nohighlight">\(J\)</span>. As in the single-variable case, one might hope to classify such a stationary point as a minimizer or maximizer based on the local curvature of <span class="math notranslate nohighlight">\(J\)</span>. Indeed, it makes intuitive sense that if <span class="math notranslate nohighlight">\(J\)</span> is convex (concave) in <em>all</em> directions at <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> should be a local minimizer (maximizer). But from <a class="reference internal" href="#directional-der-grad-thm">Theorem 10.1</a>, the local directional curvatures at <span class="math notranslate nohighlight">\(\btheta^\star\)</span> are measured by the numbers</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal \text{Hess}(J(\btheta^\star)) \bv
\]</div>
<p>as <span class="math notranslate nohighlight">\(\bv\)</span> cycles through all unit vectors in <span class="math notranslate nohighlight">\(\bbr^n\)</span>. Thus, if these numbers are <em>always</em> positive (negative), then we would expect the stationary point <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer (maximizer). By the way, matrices with these special properties have names:</p>
<div class="proof definition admonition" id="definite-def">
<p class="admonition-title"><span class="caption-number">Definition 10.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(H\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> real matrix.</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(\bv^\intercal H \bv &gt;0\)</span> for all nonzero <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span>, then <span class="math notranslate nohighlight">\(H\)</span> is called <em>positive definite</em>. If instead of strict inequality <span class="math notranslate nohighlight">\(&gt;\)</span> we have <span class="math notranslate nohighlight">\(\geq\)</span>, then <span class="math notranslate nohighlight">\(H\)</span> is called <em>positive semidefinite</em>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\bv^\intercal H \bv &lt;0\)</span> for all nonzero <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span>, then <span class="math notranslate nohighlight">\(H\)</span> is called <em>negative definite</em>. If instead of strict inequality <span class="math notranslate nohighlight">\(&lt;\)</span> we have <span class="math notranslate nohighlight">\(\leq\)</span>, then <span class="math notranslate nohighlight">\(H\)</span> is called <em>negative semidefinite</em>.</p></li>
</ol>
</section>
</div><p>So, the question becomes: If we know that the Hessian matrix is positive (negative) definite at a stationary point <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, is <span class="math notranslate nohighlight">\(\btheta^\star\)</span> necessarily a local minimizer (maximizer)?</p>
<p>The answer is <em>yes</em>!</p>
<p>The justification is essentially a repeat of the argument in the single-variable case. Indeed, there is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional analog of Taylor’s Theorem that states the following: Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>. For any pair of distinct points <span class="math notranslate nohighlight">\(\btheta\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star\)</span> we may write</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = J(\btheta^\star) + (\btheta - \btheta^\star)^\intercal \nabla(J(\btheta^\star)) + \frac{1}{2} (\btheta - \btheta^\star)^\intercal \text{Hess}(J(\btheta^\star)) (\btheta - \btheta^\star) + o( ||\btheta - \btheta^\star||^2).
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\lim_{\btheta \to \btheta^\star} \frac{J(\btheta) - J(\btheta^\star)}{||\btheta - \btheta^\star||^2} = \bv^\intercal \text{Hess}(J(\btheta^\star)) \bv
\]</div>
<p>provided <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a stationary point and where <span class="math notranslate nohighlight">\(\bv = (\btheta - \btheta^\star) / || \btheta - \btheta^\star ||\)</span>. From this we conclude that <span class="math notranslate nohighlight">\(J(\btheta) &gt; J(\btheta^\star)\)</span> if the Hessian matrix is positive definite, which shows that <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer. Similar considerations apply regarding local maximizers if instead the Hessian matrix is negative definite.</p>
<p>In very favorable situations, the Hessian matrix <span class="math notranslate nohighlight">\(\text{Hess}(J(\btheta))\)</span> is positive definite for <em>all</em> <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span>. In this case, the function <span class="math notranslate nohighlight">\(J\)</span> is <em>globally</em> convex and has at most one stationary point which must be a global minimizer.</p>
<p>It is worth summarizing our findings in the form of a theorem:</p>
<div class="proof theorem admonition" id="second-der-test-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.3 </span> (Second Derivative Test)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star \in \bbr^n\)</span> a point.</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(\nabla (J(\btheta_\star)) =0 \)</span> and <span class="math notranslate nohighlight">\(\text{Hess}(J(\btheta_\star))\)</span> is positive definite, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\nabla (J(\btheta_\star)) =0 \)</span> and <span class="math notranslate nohighlight">\(\text{Hess}(J(\btheta_\star))\)</span> is negative definite, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local maximizer.</p></li>
</ol>
</section>
</div><p>Since we are assuming the second-order partial derivatives of all our functions are continuous, all our Hessian matrices are symmetric. This means that a very important and powerful theorem from advanced linear algebra may be brought to bear:</p>
<div class="proof theorem admonition" id="spectral-thm">
<p class="admonition-title"><span class="caption-number">Theorem 10.4 </span> (Eigenvalue criterion for (semi)definiteness)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(H\in \bbr^{n\times n}\)</span> be symmetric. Then the eigenvalues of <span class="math notranslate nohighlight">\(H\)</span> are real, and:</p>
<ol class="arabic simple">
<li><p>The matrix <span class="math notranslate nohighlight">\(H\)</span> is positive definite (semidefinite) if and only if all its eigenvalues are positive (nonnegative).</p></li>
<li><p>The matrix <span class="math notranslate nohighlight">\(H\)</span> is negative definite (semidefinite) if and only if all its eigenvalues are negative (nonpositive).</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. By the <a class="reference external" href="https://en.wikipedia.org/wiki/Spectral_theorem">Spectral Theorem</a> (see also Theorem 5.8 in Chapter 7 of <span id="id7">[<a class="reference internal" href="bib.html#id12" title="M. Artin. Algebra. Prentice Hall, 1991.">Art91</a>]</span>), there exists an orthonormal basis <span class="math notranslate nohighlight">\(\be_1,\ldots,\be_n\)</span> of <span class="math notranslate nohighlight">\(\bbr^n\)</span> consisting of eigenvectors of <span class="math notranslate nohighlight">\(H\)</span> and, furthermore, the associated eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_n\)</span> are all real. Thus, given <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span>, we may write</p>
<div class="math notranslate nohighlight">
\[
\bv = \sum_{i=1}^n \alpha_i \be_i
\]</div>
<p>for some scalars <span class="math notranslate nohighlight">\(\alpha_1,\ldots,\alpha_n\in \bbr\)</span>. But then</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal H \bv = \sum_{i,j=1}^n \alpha_i \alpha_j \be_i^\intercal H \be_j = \sum_{i=1}^n \alpha_i^2 \lambda_i,
\]</div>
<p>where the last equality follows from the eigenvalue/eigenvector equations <span class="math notranslate nohighlight">\(H \be_j = \lambda _j \be_j\)</span> and the fact <span class="math notranslate nohighlight">\(\be_1,\ldots,\be_n\)</span> is orthonormal. The desired results then follow. Q.E.D.</p>
</div>
<p>A rough measure of the complexity of the local curvature at a minimizer is given by the ratio of the largest directional curvature to the smallest directional curvature. To be more precise, let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and suppose <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer with positive definite Hessian matrix <span class="math notranslate nohighlight">\(H = \text{Hess}(J(\btheta^\star))\)</span>. By <a class="reference internal" href="#spectral-thm">Theorem 10.4</a>, all the eigenvalues of <span class="math notranslate nohighlight">\(H\)</span> are real and positive; let’s suppose that they are linearly ordered as</p>
<div class="math notranslate nohighlight" id="equation-ordering-eqn">
<span class="eqno">(10.7)<a class="headerlink" href="#equation-ordering-eqn" title="Permalink to this equation">#</a></span>\[
0 &lt; \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n.
\]</div>
<p>By the Spectral Theorem (see the proof of <a class="reference internal" href="#spectral-thm">Theorem 10.4</a>), we may choose an orthonormal basis of <span class="math notranslate nohighlight">\(\bbr^n\)</span> consisting of eigenvectors <span class="math notranslate nohighlight">\(\be_1,\ldots,\be_n\)</span>. Thus, if <span class="math notranslate nohighlight">\(\bv\)</span> is a unit vector, we may write</p>
<div class="math notranslate nohighlight">
\[
\bv = \alpha_1 \be_1 + \cdots + \alpha_n \be_n
\]</div>
<p>for some unique scalars <span class="math notranslate nohighlight">\(\alpha_1,\ldots,\alpha_n\)</span> with</p>
<div class="math notranslate nohighlight" id="equation-sum-to-one-eqn">
<span class="eqno">(10.8)<a class="headerlink" href="#equation-sum-to-one-eqn" title="Permalink to this equation">#</a></span>\[
\alpha_1^2 + \cdots + \alpha_n^2 =1.
\]</div>
<p>Then, the curvature in the direction of <span class="math notranslate nohighlight">\(\bv\)</span> is given by</p>
<div class="math notranslate nohighlight" id="equation-objective-curv-eqn">
<span class="eqno">(10.9)<a class="headerlink" href="#equation-objective-curv-eqn" title="Permalink to this equation">#</a></span>\[
\bv^\intercal H \bv = \alpha_1^2 \lambda_1 + \cdots + \alpha_n^2 \lambda_n.
\]</div>
<p>Using the ordering <a class="reference internal" href="#equation-ordering-eqn">(10.7)</a> and <a class="reference internal" href="#equation-sum-to-one-eqn">(10.8)</a>, it is easy to show that the curvature <a class="reference internal" href="#equation-objective-curv-eqn">(10.9)</a> is maximized when <span class="math notranslate nohighlight">\(\bv = \be_n\)</span>, in which case the curvature is the largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_n\)</span>. Similarly, it is easy to show that the curvature is minimized when <span class="math notranslate nohighlight">\(\bv = \be_1\)</span>, in which case the curvature is the smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span>. The ratio of these two curvatures is denoted</p>
<div class="math notranslate nohighlight">
\[
\kappa(H) \def \frac{\lambda_n}{\lambda_1}
\]</div>
<p>and is called the <em>condition number</em> of <span class="math notranslate nohighlight">\(H\)</span>. When <span class="math notranslate nohighlight">\(\kappa(H)\)</span> is large (in which case <span class="math notranslate nohighlight">\(H\)</span> is called <em>ill-conditioned</em>), the curvatures vary widely as we look in all different directions; conversely, when <span class="math notranslate nohighlight">\(\kappa(H)\)</span> is near <span class="math notranslate nohighlight">\(1\)</span>, the directional curvatures are all nearly the same. As we will see in the next section, ill-conditioned Hessian matrices inflate an important upper-bound on the speed of convergence of gradient descent. In other words, ill-conditioned Hessian matrices <em>may</em> signal slow convergence of gradient descent.</p>
</section>
<section id="gradient-descent-in-multiple-variables">
<span id="multivariate-grad-desc-sec"></span><h2><span class="section-number">10.3. </span>Gradient descent in multiple variables<a class="headerlink" href="#gradient-descent-in-multiple-variables" title="Permalink to this heading">#</a></h2>
<p>With the gradient vector taking the place of the derivative, it is easy to generalize the single-variable gradient descent algorithm from <a class="reference internal" href="#single-variable-gd-alg">Algorithm 10.2</a> to multiple variables:</p>
<div class="proof algorithm admonition" id="gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 10.3 </span> (Multi-variable gradient descent with rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A function <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\to \mathbb{R}\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span>, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^n\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta - \alpha(1-\gamma)^t \nabla (J(\btheta))\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ol>
</section>
</div><p>Just like the single-variable version, beginning from an initial guess <span class="math notranslate nohighlight">\(\btheta_0\)</span> for a minimizer, the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop in the algorithm outputs a sequence of approximations <span class="math notranslate nohighlight">\(\btheta_1,\ldots,\btheta_t,\ldots,\btheta_N\)</span> for a minimizer. The last value <span class="math notranslate nohighlight">\(\btheta_N\)</span> in the sequence is taken as the output of the algorithm; if the algorithm converges to a minimizer, then we should have <span class="math notranslate nohighlight">\(\btheta_N \approx \btheta^\star\)</span>.</p>
<p>For an example, let’s consider the polynomial objective function</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = J(\theta_1,\theta_2) = (\theta_1^2 + 10 \theta_2^2)\big((\theta_1-1)^2 + 10(\theta_2-1)^2 \big)
\]</div>
<p>in two dimensions. This function has two minimizers</p>
<div class="math notranslate nohighlight">
\[
\btheta^\star = (0, 0), (1,1),
\]</div>
<p>as well as a “saddle point” at <span class="math notranslate nohighlight">\((0.5, 0.5)\)</span> where the gradient <span class="math notranslate nohighlight">\(\nabla (J(\btheta))\)</span> vanishes. A contour plot of its level curves looks like:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the objective function</span>
<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">theta_1</span><span class="p">,</span> <span class="n">theta_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">theta</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta_1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">theta_2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">theta_1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># plot contours of objective function</span>
<span class="n">linspace_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">linspace_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.25</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">linspace_x</span><span class="p">,</span> <span class="n">linspace_y</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/fff2799da5f1d697d03009732ed819b1480d58d72d2ccd8fa09bb3b836cf2e87.svg" src="../_images/fff2799da5f1d697d03009732ed819b1480d58d72d2ccd8fa09bb3b836cf2e87.svg" /></figure>
</div>
</div>
<p>Let’s run the GD algorithm four times beginning with <em>no</em> rate decay, and track the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> plotted over the contours of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the objective function</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">4e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/4d506150ec9d163a55e35d2b00b4d6b2668a52ce44e4c1b62f1a1a387bae3918.svg" src="../_images/4d506150ec9d163a55e35d2b00b4d6b2668a52ce44e4c1b62f1a1a387bae3918.svg" /></figure>
</div>
</div>
<p>The large magenta dots in the plots indicate the initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span>, while the smaller dots indicate the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> for <span class="math notranslate nohighlight">\(t&gt;0\)</span>. The algorithm appears to be converging nicely to the minimizer <span class="math notranslate nohighlight">\(\btheta^\star = (1,1)\)</span> in the upper-left plot, while in the other three plots, the algorithm finds a neighborhood of a minimizer, but then oscillates back and forth and never appears to settle down. This is due jointly to the elliptical (non-circular) shape of the contours, the choice of initial values, and poorly chosen learning rates.</p>
<p>In particular, since the gradient is orthogonal to contours, we see that the negative gradient (which the GD algorithm is following) does <em>not</em> point directly toward the minimizers. The elliptical nature of the contours creates local curvatures at the minimizers that are quite different depending on which direction you look. From the previous section, we know that the local curvatures are encoded in the Hessian matrix; this suggests that studying the Hessian matrix might lead to insights into the convergence properties of gradient descent.</p>
<p>To begin this study, let’s start more generally with a function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star\)</span> a point. As we saw in the previous section, for <span class="math notranslate nohighlight">\(\btheta\)</span> near <span class="math notranslate nohighlight">\(\btheta^\ast\)</span>, we have the local degree-<span class="math notranslate nohighlight">\(2\)</span> Taylor approximation</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \approx J(\btheta^\star) + (\btheta - \btheta^\star)^\intercal \nabla(J(\btheta^\star)) + (\btheta - \btheta^\star)^\intercal \text{Hess}(J(\btheta^\star)) (\btheta - \btheta^\star).
\]</div>
<p>Thus, if we want to study (an approximation of) the local geometry of the graph of <span class="math notranslate nohighlight">\(J\)</span> near <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, we may as well replace <span class="math notranslate nohighlight">\(J(\btheta)\)</span> with the Taylor polynomial on the right-hand side of this approximation. We therefore assume that</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{2}\btheta^\intercal A \btheta + \bb^\intercal \btheta + c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(A \in \bbr^{n\times n}\)</span> is a symmetric matrix, <span class="math notranslate nohighlight">\(\bb\in \bbr^n\)</span> is a vector, and <span class="math notranslate nohighlight">\(c\in \bbr\)</span> is a scalar. As you may easily compute, the gradient and Hessian matrices are given by</p>
<div class="math notranslate nohighlight">
\[
\nabla(J(\btheta)) = A \btheta + \bb \quad \text{and} \quad \text{Hess}(J(\btheta)) = A.
\]</div>
<p>Assuming that the rate decay <span class="math notranslate nohighlight">\(\gamma=0\)</span>, the update rule in the GD algorithm is given by</p>
<div class="math notranslate nohighlight">
\[
\btheta_{t+1} = \btheta_t - \alpha(A\btheta_t + \bb).
\]</div>
<p>Then, if <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is any stationary point (like a local minimizer), we may rewrite this update rule as</p>
<div class="math notranslate nohighlight">
\[
\btheta_{t+1} - \btheta^\star = (I - \alpha A)(\btheta_t - \btheta^\star)
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the <span class="math notranslate nohighlight">\(n\times n\)</span> identity matrix. This leads us to the closed form of the update rule given by</p>
<div class="math notranslate nohighlight" id="equation-gd-closed-eqn">
<span class="eqno">(10.10)<a class="headerlink" href="#equation-gd-closed-eqn" title="Permalink to this equation">#</a></span>\[
\btheta_t - \btheta^\star = (I - \alpha A)^t (\btheta_0 - \btheta^\star)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 0\)</span>.</p>
<p>Choosing the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is a balancing act: We want it large enough to obtain quick convergence, but small enough to avoid oscillations like in the plots above. To find the optimal <span class="math notranslate nohighlight">\(\alpha\)</span> in our current situation, let’s suppose that <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is indeed a local minimizer with positive definite Hessian matrix <span class="math notranslate nohighlight">\(\text{Hess}(J(\btheta^\star)) = A\)</span>. Suppose we linearly order the eigenvalues of <span class="math notranslate nohighlight">\(A\)</span> as</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \lambda_1 \leq \cdots \leq \lambda_n.
\]</div>
<p>The eigenvalues of the matrix <span class="math notranslate nohighlight">\(I - \alpha A\)</span> are <span class="math notranslate nohighlight">\(1 - \alpha \lambda_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. As long as we choose the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-lr-eqn">
<span class="eqno">(10.11)<a class="headerlink" href="#equation-lr-eqn" title="Permalink to this equation">#</a></span>\[
0 &lt; \alpha \leq 1 / \lambda_n,
\]</div>
<p>these latter eigenvalues are all nonnegative with</p>
<div class="math notranslate nohighlight" id="equation-new-order-eqn">
<span class="eqno">(10.12)<a class="headerlink" href="#equation-new-order-eqn" title="Permalink to this equation">#</a></span>\[
0 \leq 1 - \alpha \lambda_n \leq \cdots \leq 1 - \alpha \lambda_1 &lt; 1.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(I-\alpha A\)</span> is symmetric, its operator norm is equal to its largest eigenvalue, <span class="math notranslate nohighlight">\(1-\alpha \lambda_1\)</span>. In particular, from <a class="reference internal" href="#equation-gd-closed-eqn">(10.10)</a> we obtain the upper bound</p>
<div class="math notranslate nohighlight">
\[
||\btheta_t - \btheta^\star|| \leq ||I - \alpha A||^t ||\btheta_0 - \btheta^\star || = (1-\alpha \lambda_1)^t ||\btheta_0 - \btheta^\star ||.
\]</div>
<p>As we saw in <a class="reference internal" href="#equation-new-order-eqn">(10.12)</a>, our choice of learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> such that <a class="reference internal" href="#equation-lr-eqn">(10.11)</a> holds implies <span class="math notranslate nohighlight">\(1-\alpha \lambda_1&lt;1\)</span>, and therefore this last displayed inequality shows that convergence is guaranteed as <span class="math notranslate nohighlight">\(t\to \infty\)</span>. However, we may speed up the convergence by choosing <span class="math notranslate nohighlight">\(\alpha\)</span> to be the maximum value in the range given by <a class="reference internal" href="#equation-lr-eqn">(10.11)</a>, i.e., <span class="math notranslate nohighlight">\(\alpha = 1/\lambda_n\)</span>. In this case, we have</p>
<div class="math notranslate nohighlight">
\[
||\btheta_t - \btheta^\star|| \leq ( 1- 1/\kappa(A))^t ||\btheta_0 - \btheta^\star ||
\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa(A)\)</span> is the condition number of <span class="math notranslate nohighlight">\(A\)</span>. This shows that the fastest rate of convergence guaranteed by our arguments is controlled by the condition number of the Hessian matrix. In particular, if the Hessian matrix is ill-conditioned (i.e., if the condition number is large), then we cannot guarantee quick convergence (at least using the present arguments).</p>
<p>We may dampen these oscillations and encourage the algorithm to converge by adding learning rate decay. Here are four plots with the same initial values and learning rates, but with <span class="math notranslate nohighlight">\(\gamma = 0.05\)</span> and <span class="math notranslate nohighlight">\(N\)</span> increased to <span class="math notranslate nohighlight">\(40\)</span> to account for the learning rate decay:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the objective function</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">5e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0.05$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f3e7ebe0f96d5874b23fe2c61b5a59114307dd28b5fcf4306791f9b3cc3c0344.svg" src="../_images/f3e7ebe0f96d5874b23fe2c61b5a59114307dd28b5fcf4306791f9b3cc3c0344.svg" /></figure>
</div>
</div>
<p>Now, the learning rate <span class="math notranslate nohighlight">\(\alpha = 0.005\)</span> in the first plot appears to be much too small causing the gradient steps to shrink too fast before the algorithm converges. On the other hand, the algorithm in the other three plots appears to be nicely converging to minimizers. We have effectively “dampened out” the wild oscillations in the first four plots above.</p>
<p>Here are the values of the objective function in all four runs, plotted against the number of gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">5e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)),</span> <span class="n">running_objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0.05$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/e6fddb84f83e3f343784588b7c40cc5b4e25a2b6c1e318f6cfce0ed0cc3f0585.svg" src="../_images/e6fddb84f83e3f343784588b7c40cc5b4e25a2b6c1e318f6cfce0ed0cc3f0585.svg" /></figure>
</div>
</div>
<p>Notice the initial “overshoot” in the plot in the bottom left, causing the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> to <em>increase</em> after the first gradient step. Recall also that the initial value <span class="math notranslate nohighlight">\(\btheta_0\)</span> in the bottom right plot is near the saddle point <span class="math notranslate nohighlight">\((0.5,0.5)\)</span>, causing <span class="math notranslate nohighlight">\(\nabla J(\btheta_0) \approx 0\)</span>. This accounts for the small initial changes in the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> indicated by the (nearly) horizontal stretch early in the run of the algorithm.</p>
<p>Of course, an objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}^2 \to \mathbb{R}\)</span> defined on a <span class="math notranslate nohighlight">\(2\)</span>-dimensional input space is still not a realistic example of the objective functions encountered in the real world. In two dimensions, we have the ability to plot the algorithm’s progress through <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> on a contour plot, as we did above. In dimensions <span class="math notranslate nohighlight">\(n\geq 4\)</span> we lose this visual aid, though one may plot input variables two at a time in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. But no matter the input dimension, we may always plot the objective values against the number of gradient steps as a diagnostic plot for convergence.</p>
</section>
<section id="stochastic-gradient-descent">
<span id="sgd-sec"></span><h2><span class="section-number">10.4. </span>Stochastic gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this heading">#</a></h2>
<p>The special types of objective functions that we will see in <a class="reference internal" href="12-learning.html#learning"><span class="std std-numref">Chapter 12</span></a> are so-called <em>stochastic objective functions</em> of the form</p>
<div class="math notranslate nohighlight" id="equation-stoch-obj-eqn">
<span class="eqno">(10.13)<a class="headerlink" href="#equation-stoch-obj-eqn" title="Permalink to this equation">#</a></span>\[J(\btheta) = E\big( g(\mathbf{X};\btheta) \big)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional random vector, <span class="math notranslate nohighlight">\(\btheta \in \mathbb{R}^k\)</span> is a <span class="math notranslate nohighlight">\(k\)</span>-dimensional <em>parameter vector</em>, and <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n+k} \to \mathbb{R}\)</span> is a function. In many important cases, the probability distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is discrete and does not depend on <span class="math notranslate nohighlight">\(\btheta\)</span>, so that</p>
<div class="math notranslate nohighlight" id="equation-object-expect-eqn">
<span class="eqno">(10.14)<a class="headerlink" href="#equation-object-expect-eqn" title="Permalink to this equation">#</a></span>\[J(\btheta) = \sum_{\mathbf{x}\in \mathbb{R}^n} g(\mathbf{x};\btheta)p(\mathbf{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> is the mass function of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact, the mass function <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> will often be an <em>empirical</em> mass function of an observed multivariate dataset</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(m)}\in \mathbb{R}^n,
\]</div>
<p>where we use a superscript with parentheses to index the data vectors rather than a subscript. Then, from <a class="reference internal" href="#equation-object-expect-eqn">(10.14)</a> and the definition of the mass function, we get</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m g\big(\mathbf{x}^{(i)}; \btheta \big).
\]</div>
<p>Provided that the function is differentiable with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-batch-eqn">
<span class="eqno">(10.15)<a class="headerlink" href="#equation-batch-eqn" title="Permalink to this equation">#</a></span>\[\nabla_\btheta J(\btheta) = \frac{1}{m} \sum_{i=1}^m \nabla_\btheta g\big(\mathbf{x}^{(i)}; \btheta \big)\]</div>
<p>where we write <span class="math notranslate nohighlight">\(\nabla_\btheta\)</span> to emphasize that the gradient is computed with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. In this context, the gradient descent algorithm applied to <a class="reference internal" href="#equation-batch-eqn">(10.15)</a> is given a new name:</p>
<div class="proof definition admonition" id="batch-gd-def">
<p class="admonition-title"><span class="caption-number">Definition 10.4 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>batch gradient descent algorithm</em> is the gradient descent algorithm applied to a stochastic objective function of the form <a class="reference internal" href="#equation-batch-eqn">(10.15)</a>.</p>
</section>
</div><p>Let’s take a look at a simple example. Suppose that we define</p>
<div class="math notranslate nohighlight" id="equation-quadratic-eqn">
<span class="eqno">(10.16)<a class="headerlink" href="#equation-quadratic-eqn" title="Permalink to this equation">#</a></span>\[g: \bbr^4 \to \bbr, \quad g(\btheta,\btheta) = |\btheta - \btheta|^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\btheta,\btheta\in \bbr^2\)</span> and the vertical bars represent the usual Euclidean norm. We create a bivariate dataset by drawing a random sample of size <span class="math notranslate nohighlight">\(1{,}024\)</span> drawn from a <span class="math notranslate nohighlight">\(\mathcal{N}_2(\boldsymbol0,I)\)</span> distribution. A scatter plot of the dataset looks like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/e995dd41d9108f1db7b456761e4337af9da4eb00042ed31823811225222d91d5.svg" src="../_images/e995dd41d9108f1db7b456761e4337af9da4eb00042ed31823811225222d91d5.svg" /></figure>
</div>
</div>
<p>Then, two runs of the batch gradient descent algorithm produce the following plots of the objective function versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">tracking</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># define data loader</span>
    <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>
    
    <span class="c1"># initialize lists and a dictionary to track objectives and parameters</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># begin looping through epochs</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        
        <span class="c1"># initialize a list to track per-step objectives. this will only be used if</span>
        <span class="c1"># tracking is set to &#39;epoch&#39;</span>
        <span class="n">per_step_objectives</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># begin gradient descent loop</span>
        <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            
            <span class="c1"># compute objective with current parameters</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

            <span class="c1"># if we are tracking per gradient step, then add objective value and parameters to the </span>
            <span class="c1"># running lists. otherwise, we are tracking per epoch, so add the objective value to</span>
            <span class="c1"># the list of per-step objectives</span>
            <span class="k">if</span> <span class="n">tracking</span> <span class="o">==</span> <span class="s1">&#39;gd_step&#39;</span><span class="p">:</span>
                <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">running_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">per_step_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        
            <span class="c1"># compute gradients    </span>
            <span class="n">objective</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># take a gradient step and update the parameters</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">g</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">parameter</span><span class="o">.</span><span class="n">grad</span>
                    <span class="n">parameter</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>
            
            <span class="c1"># zero out the gradients to prepare for the next iteration</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">parameter</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># if we hit the maximum number of gradient steps, break out of the inner `for`</span>
            <span class="c1"># loop</span>
            <span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">step_count</span> <span class="o">==</span> <span class="n">max_steps</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="c1"># if we are tracking per epoch, then add the average per-step objective to the</span>
        <span class="c1"># list of running objectives. also, add the current parameters to the list of running</span>
        <span class="c1"># parameters</span>
        <span class="k">if</span> <span class="n">tracking</span> <span class="o">==</span> <span class="s1">&#39;epoch&#39;</span><span class="p">:</span>
            <span class="n">per_step_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">per_step_objectives</span><span class="p">)</span>
            <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">per_step_objectives</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">running_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        
        <span class="c1"># if we hit the maximum number of gradient steps, break out of the outer `for`</span>
        <span class="c1"># loop</span>
        <span class="k">if</span> <span class="n">step_count</span> <span class="o">==</span> <span class="n">max_steps</span><span class="p">:</span>
            <span class="k">break</span>
            
    <span class="c1"># output tensors instead of lists</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">running_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span>

<span class="c1"># define the objective function</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Parameters must be a dictionary of tensors.&#39;</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">parameters</span><span class="p">))</span>

<span class="c1"># get grids for contour plots</span>
<span class="n">linspace</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">linspace</span><span class="p">,</span> <span class="n">linspace</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">grid_1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">grid_2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">z_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">:</span>
    <span class="n">z_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">}))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="n">z_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">grid_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># gradient descent parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">]}</span>
<span class="n">parameters_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">objectives_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># run gradient descent</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                                                 <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                                                 <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                                                 <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;gd_step&#39;</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                 <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                                 <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">parameters_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">)</span>
    <span class="n">objectives_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

<span class="c1"># plot the objective function</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">objectives</span> <span class="o">=</span> <span class="n">objectives_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7e6c0540b4f96d54fb58cc44c5bc7b6b7af329b522ed32807be3789c022f56df.svg" src="../_images/7e6c0540b4f96d54fb58cc44c5bc7b6b7af329b522ed32807be3789c022f56df.svg" /></figure>
</div>
</div>
<p>If we track the parameters <span class="math notranslate nohighlight">\(\btheta = (\theta_1,\theta_2)\)</span> during the runs, we get the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">parameters_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, gradient steps$=</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/72ba65cfea7492452fc4fb88281bbdae20c32e9efb715b6f63e29d65b1ceeb7f.svg" src="../_images/72ba65cfea7492452fc4fb88281bbdae20c32e9efb715b6f63e29d65b1ceeb7f.svg" /></figure>
</div>
</div>
<p>In both cases, notice that the algorithm is nicely converging toward the minimizer at <span class="math notranslate nohighlight">\(\btheta^\star \approx (0,0)\)</span>.</p>
<p>One of the drawbacks of the batch algorithm is that it needs the <em>entire</em> dataset in order to take just a single gradient step. This isn’t an issue for our small toy dataset of size <span class="math notranslate nohighlight">\(m=1{,}024\)</span>, but for the large datasets that you may encounter in the real world, this can be a serious hindrance to fast convergence.</p>
<p>One method for dealing with this bottleneck is to use <em>mini-batches</em> of the data to compute gradient steps. To do so, we begin by randomly partitioning the dataset into subsets <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_\ell\)</span> called <em>mini-batches</em>:</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-eqn">
<span class="eqno">(10.17)<a class="headerlink" href="#equation-mini-batch-eqn" title="Permalink to this equation">#</a></span>\[B_1 \cup B_2 \cup \cdots \cup B_p = \{\btheta^{(1)},\btheta^{(2)},\ldots,\btheta^{(m)}\}.\]</div>
<p>Supposing that the <span class="math notranslate nohighlight">\(j\)</span>-th mini-batch <span class="math notranslate nohighlight">\(B_j\)</span> has size <span class="math notranslate nohighlight">\(\ell_j\)</span>, we would then expect from <a class="reference internal" href="#equation-batch-eqn">(10.15)</a> that</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-grad-eqn">
<span class="eqno">(10.18)<a class="headerlink" href="#equation-mini-batch-grad-eqn" title="Permalink to this equation">#</a></span>\[\nabla J(\btheta) \approx \frac{1}{\ell_j} \sum_{x^{(i)} \in B_j} \nabla_\btheta g\big(\btheta^{(i)}; \btheta\big).\]</div>
<p>Very often, the mini-batch sizes <span class="math notranslate nohighlight">\(\ell_1,\ell_2,\ldots,\ell_p\)</span> are chosen to be equal to a common value <span class="math notranslate nohighlight">\(\ell\)</span>, except (possibly) for one to compensate for the fact that <span class="math notranslate nohighlight">\(m\)</span> may not be evenly divisible by <span class="math notranslate nohighlight">\(\ell\)</span>. For example, if <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\ell=30\)</span>, then we would have four mini-batches, three of size <span class="math notranslate nohighlight">\(\ell=30\)</span> and the fourth of size <span class="math notranslate nohighlight">\(10\)</span>.</p>
<p>As you are about to see, the mini-batch version of the GD algorithm loops over the mini-batches <a class="reference internal" href="#equation-mini-batch-eqn">(10.17)</a> and computes gradient steps as in <a class="reference internal" href="#equation-mini-batch-grad-eqn">(10.18)</a>. A single loop through <em>all</em> the mini-batches, covering the <em>entire</em> dataset, is called an <em>epoch</em>. As the vanilla version of the GD algorithm takes the number of gradient steps as a parameter, the new version of the algorithm takes the number of epochs as a parameter. This new version is called the <em>stochastic gradient descent (SGD) algorithm</em>:</p>
<div class="proof algorithm admonition" id="sgd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 10.4 </span> (Stochastic gradient descent with rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A dataset <span class="math notranslate nohighlight">\(\btheta^{(1)},\ldots,\btheta^{(m)}\in \mathbb{R}^n\)</span>, a stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m g \big(\btheta^{(i)};\btheta \big), \quad \btheta \in \mathbb{R}^n,
\]</div>
<p>where <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n+k}\to \mathbb{R}\)</span> is a differentiable function, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^k\)</span> for a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span> of <span class="math notranslate nohighlight">\(J\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span>, a mini-batch size <span class="math notranslate nohighlight">\(\ell\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of epochs.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ul>
<li><p>Randomly partition the dataset into mini-batches <a class="reference internal" href="#equation-mini-batch-eqn">(10.17)</a> of size <span class="math notranslate nohighlight">\(\ell\)</span>.</p></li>
<li><p>For each mini-batch <span class="math notranslate nohighlight">\(B_j\)</span>, do:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta - \frac{\alpha(1-\gamma)^t}{\ell} \sum_{x^{(i)} \in B_j} \nabla_\btheta g\big(\btheta^{(i)}; \btheta\big)^T\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ul>
</section>
</div><p>Notice that the dataset is randomly partitioned into mini-batches inside each iteration of the per-epoch <code class="docutils literal notranslate"><span class="pre">for</span></code> loop; and remember that there may be one mini-batch of size <span class="math notranslate nohighlight">\(\neq \ell\)</span> if the size of the dataset <span class="math notranslate nohighlight">\(m\)</span> is not divisible by <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>It is possible to select a mini-batch size of <span class="math notranslate nohighlight">\(\ell=1\)</span>, so that the algorithm computes a gradient step per data point. Some references refer to this algorithm as just <em>stochastic gradient descent</em>. In our example <a class="reference internal" href="#equation-quadratic-eqn">(10.16)</a> from above, a step size of <span class="math notranslate nohighlight">\(\ell=1\)</span> yields the following plots of objective values versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># SGD parameters</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">],</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">160</span><span class="p">]}</span>
<span class="n">parameters_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">objectives_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># run SGD</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                                                 <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                                                 <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                                                 <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;gd_step&#39;</span><span class="p">,</span>
                                                 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                 <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                                 <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">parameters_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">)</span>
    <span class="n">objectives_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

<span class="c1"># plot the objective function</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">objectives</span> <span class="o">=</span> <span class="n">objectives_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/16fdce16e836f8abd6442a2273fa812edabfbcc6a1436386e204b82f439a5853.svg" src="../_images/16fdce16e836f8abd6442a2273fa812edabfbcc6a1436386e204b82f439a5853.svg" /></figure>
</div>
</div>
<p>The plots are very noisy, especially for large numbers of gradient steps. However, a slight downward trend in objective values is detectable, indicating that the algorithm is locating the minimizer. The trace of the algorithm through parameter space is shown in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">parameters_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/42cb29889507cb5ffb6a7be2e950497fe2246eeed08c245c7af34d63d366b376.svg" src="../_images/42cb29889507cb5ffb6a7be2e950497fe2246eeed08c245c7af34d63d366b376.svg" /></figure>
</div>
</div>
<p>The traces are very noisy, especially in the first row with the large learning rate <span class="math notranslate nohighlight">\(\alpha=0.1\)</span>. Nevertheless, it is clear that the algorithm has found the neighborhood of the minimizer at <span class="math notranslate nohighlight">\((0,0)\)</span>. We might try to tame the noise in these plots by increasing the decay rate, but according to our implementation, that would be equivalent to simply decreasing the learning rate since none of these four runs of the algorithm completes a full epoch. Indeed, notice that the power <span class="math notranslate nohighlight">\(t\)</span> in the expression <span class="math notranslate nohighlight">\((1-\gamma)^t\)</span> in the <a class="reference internal" href="#sgd-alg">statement</a> of the algorithm counts the number of epochs.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># mini-batch gradient descent parameters</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
               <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">],</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">]}</span>
<span class="n">parameters_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">objectives_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># run mini-batch gradient descent</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                             <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                             <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                             <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;gd_step&#39;</span><span class="p">,</span>
                             <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                             <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">parameters_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">)</span>
    <span class="n">objectives_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

<span class="c1"># plot the objective function</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">objectives</span> <span class="o">=</span> <span class="n">objectives_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$\ell=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">$, $\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/d5e4b152accb4d51576b2bfc42ba843dedc988baf291b26a4d869ea50a0d7211.svg" src="../_images/d5e4b152accb4d51576b2bfc42ba843dedc988baf291b26a4d869ea50a0d7211.svg" /></figure>
</div>
</div>
<p>Mini-batch GD parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">parameters_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    <span class="n">total_data_points</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">max_steps</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$\ell=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">$, $\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c1562abffc281be3bb11e710d9a3b058274804ce8a45630eb741af0da675a2e5.svg" src="../_images/c1562abffc281be3bb11e710d9a3b058274804ce8a45630eb741af0da675a2e5.svg" /></figure>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="09-info-theory.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Information theory</p>
      </div>
    </a>
    <a class="right-next"
       href="11-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Probabilistic graphical models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">10.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curvature-and-derivatives-in-higher-dimensions">10.2. Curvature and derivatives in higher dimensions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">10.3. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">10.4. Stochastic gradient descent</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>