

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>13. Learning &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bpi": "\\boldsymbol\\pi", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bp": "\\mathbf{p}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bD": "\\mathbf{D}", "bH": "\\mathbf{H}", "bI": "\\mathbf{I}", "bK": "\\mathbf{K}", "bS": "\\mathbf{S}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bW": "\\mathbf{W}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calJ": "\\mathcal{J}", "calH": "\\mathcal{H}", "calI": "\\mathcal{I}", "calL": "\\mathcal{L}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "calS": "\\mathcal{S}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/13-learning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14. Statistics and general parameter estimation" href="14-stats-estimators.html" />
    <link rel="prev" title="12. Probabilistic graphical models" href="12-models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-halfway.html">9. The halfway point: pivoting toward models and data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-info-theory.html">10. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-optim.html">11. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-models.html">12. Probabilistic graphical models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-stats-estimators.html">14. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-asymptotic.html">15. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-CIs.html">16. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-hyp-test.html">17. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-lin-reg.html">18. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">19. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/13-learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-look-at-likelihood-based-learning-objectives">13.1. A first look at likelihood-based learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-mle">13.2. General MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-linear-regression">13.3. MLE for linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-logistic-regression">13.4. MLE for logistic regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-neural-networks">13.5. MLE for neural networks</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="learning">
<span id="id1"></span><h1><span class="section-number">13. </span>Learning<a class="headerlink" href="#learning" title="Permalink to this heading">#</a></h1>
<section id="a-first-look-at-likelihood-based-learning-objectives">
<span id="likelihood-learning-sec"></span><h2><span class="section-number">13.1. </span>A first look at likelihood-based learning objectives<a class="headerlink" href="#a-first-look-at-likelihood-based-learning-objectives" title="Permalink to this heading">#</a></h2>
<p>To help motivate the learning objectives obtained in this section, let’s begin with a simple example. Suppose that we have an observed dataset</p>
<div class="math notranslate nohighlight">
\[
x_1,x_2,\ldots,x_m \in \{0,1\}
\]</div>
<p>drawn from a random variable <span class="math notranslate nohighlight">\(X \sim \Ber(\theta)\)</span> with unknown parameter <span class="math notranslate nohighlight">\(\theta \in [0,1]\)</span>. This is a very simple example of a probabilistic graphical model whose underlying graph consists of only two nodes, one for the parameter <span class="math notranslate nohighlight">\(\theta\)</span> and one for the (observed) random variable <span class="math notranslate nohighlight">\(X\)</span>:</p>
<a class="reference internal image-reference" href="../_images/bern-pgm.svg"><img alt="../_images/bern-pgm.svg" class="align-center" src="../_images/bern-pgm.svg" width="18%" /></a>
<p> </p>
<p>The probability measure <span class="math notranslate nohighlight">\(P_\theta\)</span> proposed by the model has mass function</p>
<div class="math notranslate nohighlight" id="equation-bern-model-eq">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-bern-model-eq" title="Permalink to this equation">#</a></span>\[
p(x;\theta) = \theta^x (1-\theta)^{1-x},
\]</div>
<p>for <span class="math notranslate nohighlight">\(x\in \{0,1\}\)</span>, while the dataset has its empirical probability measure <span class="math notranslate nohighlight">\(\hat{P}\)</span> with mass function <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> defined as</p>
<div class="math notranslate nohighlight" id="equation-bern-empirical-eq">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-bern-empirical-eq" title="Permalink to this equation">#</a></span>\[\begin{split}
\hat{p}(x) = \frac{\text{frequency of $x$ in the dataset}}{m} = \begin{cases}
\displaystyle\frac{\Sigma x}{m} &amp; : x=1, \\
\displaystyle\frac{m - \Sigma x}{m} &amp; : x=0,
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma x \def x_1 + x_2 + \cdots + x_m\)</span>. The goal, of course, is to model the observed dataset with our univariate PGM, but the parameter <span class="math notranslate nohighlight">\(\theta\)</span> is unknown. An “optimal” value for the parameter will minimize the discrepancy (or “distance”) between the two distributions <span class="math notranslate nohighlight">\(\hat{P}\)</span> and <span class="math notranslate nohighlight">\(P_\theta\)</span>. We seek to “learn” this optimal value from the dataset.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Technically, according to <a class="reference internal" href="10-info-theory.html#KL-def">Definition 10.4</a>, in order to discuss the KL divergence we must require that the empirical distribution is absolutely continuous with respect to the model distribution, in the sense that <span class="math notranslate nohighlight">\(p(x;\theta)=0\)</span> implies <span class="math notranslate nohighlight">\(\hat{p}(x)=0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>. This may create some minor headaches that require addressing special cases in proofs. For an example, see the proof of <a class="reference internal" href="#bern-mle-thm">Theorem 13.2</a> below.</p>
</aside>
<p>Of course, by now we know that “distance” means KL divergence, so the goal is to locate the minimizer</p>
<div class="math notranslate nohighlight">
\[
\theta^\star = \argmin_{\theta\in [0,1]} D(\hat{P} \parallel P_\theta).
\]</div>
<p>But from <a class="reference internal" href="10-info-theory.html#KL-and-entropy-thm">Theorem 10.1</a>, the KL divergence may be expressed as a difference of two entropies,</p>
<div class="math notranslate nohighlight">
\[
D(\hat{P} \parallel P_\theta) = H_{\hat{P}}(P_\theta) - H(\hat{P}),
\]</div>
<p>and since the entropy <span class="math notranslate nohighlight">\(H(\hat{P})\)</span> does not depend on <span class="math notranslate nohighlight">\(\theta\)</span> it may be dropped from the optimization objective, and we see that we are equivalently searching for the minimizer of cross entropy:</p>
<div class="math notranslate nohighlight">
\[
\theta^\star = \argmin_{\theta\in [0,1]} H_{\hat{P}}(P_\theta).
\]</div>
<p>Let’s unpack this cross entropy, using <a class="reference internal" href="#equation-bern-model-eq">(13.1)</a> and <a class="reference internal" href="#equation-bern-empirical-eq">(13.2)</a>. By definition, we have</p>
<div class="math notranslate nohighlight" id="equation-cross-ent-stoch-eq">
<span class="eqno">(13.3)<a class="headerlink" href="#equation-cross-ent-stoch-eq" title="Permalink to this equation">#</a></span>\[
H_{\hat{P}}(P_\theta) = E_{x \sim \hat{p}(x)} \left[ I_{P_\theta}(x) \right],
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
I_{P_\theta}(x) = -\log\left[ p(x;\theta) \right]
\]</div>
<p>is the surprisal function (see <a class="reference internal" href="10-info-theory.html#info-content-def">Definition 10.1</a>). Because we want to think of the data as being fixed and the parameter as variable, it will be convenient to define the <em>model surprisal function</em> to be</p>
<div class="math notranslate nohighlight">
\[
\calI(\theta;x) \def -\log\left[ p(x;\theta) \right],
\]</div>
<p>with the parameter <span class="math notranslate nohighlight">\(\theta\)</span> written first, similar to our convention regarding likelihood functions in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>. In fact, if we define the <em>model likelihood function</em> of our univariate Bernoulli model to be</p>
<div class="math notranslate nohighlight">
\[
\calL(\theta;x) \def p(x;\theta),
\]</div>
<p>then the model surprisal function is nothing but the negative logarithm of the model likelihood function.</p>
<p>Now, note that</p>
<div class="math notranslate nohighlight" id="equation-bern-like-factor-eq">
<span class="eqno">(13.4)<a class="headerlink" href="#equation-bern-like-factor-eq" title="Permalink to this equation">#</a></span>\[
p(x_1,\ldots,x_m; \theta) = \prod_{i=1}^m p(x_i;\theta),
\]</div>
<p>since the dataset is assumed drawn from an IID random sample. If we define the left-hand side to be the <em>data likelihood function</em>,</p>
<div class="math notranslate nohighlight">
\[
\calL(\theta;x_1,\ldots,x_m) \def p(x_1,\ldots,x_m; \theta),
\]</div>
<p>then we may rewrite <a class="reference internal" href="#equation-bern-like-factor-eq">(13.4)</a> in terms of likelihood functions as</p>
<div class="math notranslate nohighlight" id="equation-bern-like-factor-2-eq">
<span class="eqno">(13.5)<a class="headerlink" href="#equation-bern-like-factor-2-eq" title="Permalink to this equation">#</a></span>\[
\calL(\theta;x_1,\ldots,x_m) = \prod_{i=1}^m \calL(\theta; x_i).
\]</div>
<p>(Notice that this is identical in concept to the factorizations for the PGMs studied in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>.) Finally, if we define the <em>data surprisal function</em> to be</p>
<div class="math notranslate nohighlight">
\[
\calI(\theta;x_1,\ldots,x_n) \def - \log\left[ p(x_1,\ldots,x_m; \theta) \right] = - \log\left[ \calL(\theta;x_1,\ldots,x_m)\right],
\]</div>
<p>then we may apply the negative logarithm to both sides of <a class="reference internal" href="#equation-bern-like-factor-2-eq">(13.5)</a> to get the fundamental equation</p>
<div class="math notranslate nohighlight" id="equation-data-model-surprise-bern-eq">
<span class="eqno">(13.6)<a class="headerlink" href="#equation-data-model-surprise-bern-eq" title="Permalink to this equation">#</a></span>\[
\calI(\theta;x_1,\ldots,x_n) = \sum_{i=1}^m \calI(\theta;x_i)
\]</div>
<p>expressing the data surprisal function as a sum of model surprisal functions.</p>
<p>Let’s now bring back the cross entropy expressed above as <a class="reference internal" href="#equation-cross-ent-stoch-eq">(13.3)</a>. Using the model surprisal function, we may write:</p>
<div class="math notranslate nohighlight">
\[
H_{\hat{P}}(P_\theta) = E_{x \sim \hat{p}(x)} \left[ \calI(\theta; x) \right] = \sum_{x\in \bbr} \hat{p}(x) \calI(\theta;x) = \frac{1}{m} \sum_{i=1}^m \calI(\theta;x) = \frac{1}{m} \calI(\theta;x_1,\ldots,x_m).
\]</div>
<p>So, putting everything together, we get that</p>
<div class="math notranslate nohighlight">
\[
D(\hat{P} \parallel P_\theta) + H(\hat{P}) = H_{\hat{P}}(P_\theta) = E_{x \sim \hat{p}(x)} \left[ \calI(\theta; x) \right] \propto \calI(\theta; x_1,\ldots,x_m),
\]</div>
<p>where the constant of proportionality is the (positive) number <span class="math notranslate nohighlight">\(1/m\)</span>. Moreover, since the negative logarithm function is strictly decreasing, minimizing the data surprisal function with respect to <span class="math notranslate nohighlight">\(\theta\)</span> is equivalent to maximizing the data likelihood function with respect to <span class="math notranslate nohighlight">\(\theta\)</span>. If we combine all of our observations into a single theorem, we get:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>As mentioned in the margin note above, in this theorem we are implicitly restricting our attention to those parameters <span class="math notranslate nohighlight">\(\theta\)</span> for which the empirical distribution <span class="math notranslate nohighlight">\(\hat{P}\)</span> is absolutely continuous with respect to the model distribution <span class="math notranslate nohighlight">\(P_\theta\)</span>.</p>
</aside>
<div class="proof theorem admonition" id="equiv-obj-bern-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.1 </span> (Equivalent learning objectives for the univariate Bernoulli model)</p>
<section class="theorem-content" id="proof-content">
<p>Let</p>
<div class="math notranslate nohighlight">
\[
x_1,x_2,\ldots,x_m \in \{0,1\}
\]</div>
<p>be an observed dataset corresponding to a Bernoulli random variable <span class="math notranslate nohighlight">\(X\sim \Ber(\theta)\)</span> with unknown <span class="math notranslate nohighlight">\(\theta\)</span>. Let <span class="math notranslate nohighlight">\(P_\theta\)</span> be the model distribution of <span class="math notranslate nohighlight">\(X\)</span> and let <span class="math notranslate nohighlight">\(\hat{P}\)</span> be the empirical distribution of the dataset. The following optimization objectives are equivalent:</p>
<ol class="arabic simple">
<li><p>Minimize the KL divergence <span class="math notranslate nohighlight">\(D(\hat{P} \parallel P_\theta)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>Minimize the cross entropy <span class="math notranslate nohighlight">\(H_{\hat{P}}(P_\theta)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>Minimize the data surprisal function <span class="math notranslate nohighlight">\(\calI(\theta; x_1,\ldots,x_m)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p>Maximize the data likelihood function <span class="math notranslate nohighlight">\(\calL(\theta; x_1,\ldots,x_m)\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</section>
</div><p>Though these optimization objectives are all equivalent to each other, they have different interpretations, conceptualizations, and advantages:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Minimizing the KL divergence between the empirical and model distributions has an immediate and concrete interpretation as minimizing the “distance” between these two distributions.</p></li>
<li><p>As a function of <span class="math notranslate nohighlight">\(\theta\)</span>, the cross entropy <span class="math notranslate nohighlight">\(J(\theta) = H_{\hat{P}}(P_\theta)\)</span> may be viewed as a stochastic objective function, since it is exactly the mean of the model surprisal function. This opens the door for applications of the stochastic gradient descent algorithm studied in <a class="reference internal" href="11-optim.html#sgd-sec"><span class="std std-numref">Section 11.4</span></a>.</p></li>
<li><p>The third optimization objective seeks the model probability distribution according to which the data is <em>least surprising</em>.</p></li>
<li><p>The fourth optimization objective seeks the model probability distribution according to which the data is <em>most likely</em>.</p></li>
</ol>
</div></blockquote>
<p>Due to the equivalence with the fourth optimization objective, all these optimization objectives are referred to as <em>likelihood-based learning objectives</em>. The optimization process is then called <em>maximum likelihood estimation</em> (<em>MLE</em>), and the value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\theta^\star_\text{MLE} &amp;\def \argmax_{\theta \in [0,1]} \calL(\theta; x_1,\ldots,x_m) \\
&amp;= \argmin_{\theta \in [0,1]} \calI(\theta; x_1,\ldots,x_m) \\
&amp;= \argmin_{\theta \in [0,1]} H_{\hat{P}}(P_\theta) \\
&amp;= \argmin_{\theta \in [0,1]} D(\hat{P} \parallel P_\theta)
\end{align*}\]</div>
<p>is called the <em>maximum likelihood estimate</em> (also <em>MLE</em>). But in actual real-world practice, nobody <em>ever</em> maximizes the likelihood function directly due to numerical instability (and other reasons), and instead one of the other three learning objectives is used.</p>
<p>It will turn out that a version of <a class="reference internal" href="#equiv-obj-bern-thm">Theorem 13.1</a> holds for all probabilistic graphical models with discrete model distributions, not just our univariate Bernoulli model. But for the Bernoulli model, the MLE may be computed in closed form:</p>
<div class="proof theorem admonition" id="bern-mle-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.2 </span> (MLE for the univariate Bernoulli model)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\( x_1,x_2,\ldots,x_m \in \{0,1\}\)</span> be an observed dataset corresponding to a Bernoulli random variable <span class="math notranslate nohighlight">\(X\sim \Ber(\theta)\)</span> with unknown <span class="math notranslate nohighlight">\(\theta\)</span>. Then the (unique) maximum likelihood estimate <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE}\)</span> is the ratio <span class="math notranslate nohighlight">\( \Sigma x/m\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We first address the special cases that <span class="math notranslate nohighlight">\(\Sigma x =0\)</span> or <span class="math notranslate nohighlight">\(m\)</span>. In the first case, the data likelihood function is given by</p>
<div class="math notranslate nohighlight">
\[
\calL(\theta; x_1,\ldots,x_m ) = \theta^{\Sigma x} (1-\theta)^{m-\Sigma x} = (1-\theta)^m.
\]</div>
<p>But the latter expression is maximized at <span class="math notranslate nohighlight">\(\theta^\star=0\)</span>, and so <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE} = \Sigma x/m\)</span>, as claimed. A similar argument shows that if <span class="math notranslate nohighlight">\(\Sigma x = m\)</span>, then the likelihood function is maximized at <span class="math notranslate nohighlight">\(\theta^\star = 1\)</span>, and so <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE} = \Sigma x / m\)</span> again.</p>
<p>So, we may assume that <span class="math notranslate nohighlight">\(0 &lt; \Sigma x &lt; m\)</span>. In this case, the maximizer of the likelihood function must occur in the open interval <span class="math notranslate nohighlight">\((0,1)\)</span>. Thus, by <a class="reference internal" href="#equiv-obj-bern-thm">Theorem 13.1</a>, the parameter <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE}\)</span> is equivalently the global minimizer of the data surprisal function</p>
<div class="math notranslate nohighlight">
\[
\calI(\theta;x_1,\ldots,x_m ) = -\Sigma x \log{\theta} - (m-\Sigma x) \log{(1-\theta)}.
\]</div>
<p>But minimizers of this function can only occur at points <span class="math notranslate nohighlight">\(\theta^\star \in (0,1)\)</span> where</p>
<div class="math notranslate nohighlight" id="equation-sur-station-eq">
<span class="eqno">(13.7)<a class="headerlink" href="#equation-sur-station-eq" title="Permalink to this equation">#</a></span>\[
\frac{\partial}{\partial \theta}\Bigg|_{\theta = \theta^\star} \calI(\theta; x_1,\ldots,x_m) = 0.
\]</div>
<p>But</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \theta} \calI (\theta; x_1,\ldots,x_m) = -\frac{\Sigma x}{\theta} + \frac{m-\Sigma x}{1-\theta},
\]</div>
<p>and a little algebra yields the solution <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span> to the stationarity equation <a class="reference internal" href="#equation-sur-station-eq">(13.7)</a>. To confirm that <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span> is a global minimizer over <span class="math notranslate nohighlight">\((0,1)\)</span>, note that the second derivatives of both <span class="math notranslate nohighlight">\(-\log{\theta}\)</span> and <span class="math notranslate nohighlight">\(-\log{(1-\theta)}\)</span> are always positive, and hence the data surprisal function is strictly convex. Thus, <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE} = \Sigma x/m\)</span> must indeed be the (unique) MLE. Q.E.D.</p>
</div>
<p>Though the <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE}\)</span> is available in closed form for our univariate Bernoulli model, it is still amusing to search for <span class="math notranslate nohighlight">\(\theta^\star\)</span> by running stochastic gradient descent on the stochastic objective function given by cross entropy:</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \def H_{\hat{P}}(P_\theta) = E_{x\sim \hat{p}(x)} \left[ \calI(\theta;x) \right].
\]</div>
<p>To create the following figure, we generated a sequence of <span class="math notranslate nohighlight">\(128\)</span> observations</p>
<div class="math notranslate nohighlight">
\[
x_1,x_2,\ldots,x_{128} \in \{0,1\}
\]</div>
<p>with <span class="math notranslate nohighlight">\(\Sigma x = 87\)</span>. Then, a run of mini-batch gradient descent yields the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">sqrt</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;/Users/johnmyers/code/math_stats_ml/src/math_stats_ml&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">gd</span> <span class="kn">import</span> <span class="n">GD</span><span class="p">,</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">plot_sgd</span>
<span class="c1">#from math_stats_ml.gd import GD, SGD, plot_sgd, plot_gd</span>

<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="k">as</span> <span class="nn">clr</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="mf">0.65</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">theta</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">X</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">Sigmax</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Sigmax</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">Sigmax</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">))</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.05</span><span class="p">])}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="n">epoch_step_nums</span> <span class="o">=</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">epoch_step_nums</span>
<span class="n">objectives</span> <span class="o">=</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="n">epoch_step_nums</span><span class="p">]</span>
<span class="n">running_parameters</span> <span class="o">=</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
<span class="n">running_parameters</span> <span class="o">=</span> <span class="n">running_parameters</span><span class="p">[</span><span class="n">epoch_step_nums</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">objectives</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;cross entropy&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">epoch_step_nums</span><span class="p">,</span> <span class="n">objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mini-batch gradient descent</span><span class="se">\n</span><span class="s1">$k=$</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">, $</span><span class="se">\\</span><span class="s1">alpha =$</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">, $</span><span class="se">\\</span><span class="s1">beta=$0, $N = $</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7230b6593a2e16bc200b2abeee391d5bdaf43ecdf84a408cac36ce8840ddc2f4.svg" src="../_images/7230b6593a2e16bc200b2abeee391d5bdaf43ecdf84a408cac36ce8840ddc2f4.svg" /></figure>
</div>
</div>
<p>The blue curve in the left-hand plot is the graph of the <em>exact</em> cross entropy function <span class="math notranslate nohighlight">\(H_{\hat{P}}(P_\theta)\)</span>. The magenta points—which represent a selection of outputs of the algorithm—do not fall <em>precisely</em> on this graph since they are <em>approximations</em> to the cross entropy, obtained as realizations of the expression on the right-hand side of</p>
<div class="math notranslate nohighlight">
\[
H_{\hat{P}}(P_\theta) \approx \frac{1}{8} \sum_{x\in B} I_{P_\theta}(x),
\]</div>
<p>where <span class="math notranslate nohighlight">\(B\)</span> is a mini-batch of data of size <span class="math notranslate nohighlight">\(k=8\)</span>. (This was discussed right after we introduced <a class="reference internal" href="11-optim.html#sgd-alg">Algorithm 11.4</a> in <a class="reference internal" href="11-optim.html#optim"><span class="std std-numref">Chapter 11</span></a>.) On the right-hand size of the figure, we have plotted the (approximate) cross entropy versus gradient steps, a type of plot familiar from <a class="reference internal" href="11-optim.html#optim"><span class="std std-numref">Chapter 11</span></a>. The magenta dots on the two sides of the figure correspond to each other; they represent the (approximate) cross entropies every 16 gradient steps (<span class="math notranslate nohighlight">\(=1\)</span> epoch). Notice that the algorithm appears to be converging to the true value <span class="math notranslate nohighlight">\(\theta^\star_\text{MLE} = 87/128 \approx 0.68\)</span> given by <a class="reference internal" href="#bern-mle-thm">Theorem 13.2</a>.</p>
</section>
<section id="general-mle">
<h2><span class="section-number">13.2. </span>General MLE<a class="headerlink" href="#general-mle" title="Permalink to this heading">#</a></h2>
<p>In broad concept, maximum likelihood estimation works for all the probabilistic graphical models that we studied in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>, though there are some variations between the different models.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>One should also further distinguish between the cases that the PGM contains hidden (or latent) variables, or whether all variables are visible. We shall only focus on the latter case (so-called <em>fully-observed models</em>) since the training process for models with hidden variables requires a different set of algorithms. (For example, the <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation maximation algorithm</a>.)</p>
</aside>
<p>First, we must distinguish between training a model as a <em>generative model</em> versus a <em>discriminative model</em>. Along with every PGM comes the joint distribution over <em>all</em> random variables, and for a generative model, the learning process trains the model with the goal to learn the parameters of the <em>entire</em> joint distribution, while for a discriminative model, the learning process aims at learning the parameters of only a conditional distribution. Of the types of models explicitly studied in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>—linear regression models, logistic regression models, and neural networks—all three are trained as discriminative models, aiming to learn the parameters of the conditional distributions of the response variable <span class="math notranslate nohighlight">\(Y\)</span> given the predictor vector <span class="math notranslate nohighlight">\(\bX\)</span>. On the other hand, both the univariate Bernoulli model in the previous section and the Naive Bayes model—studied in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_12.ipynb">programming assignment</a> for <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>—are trained as generative models.</p>
<p>We begin our discussion with the case of generative models, since it is essentially just a recapitulation of our discussion of the univariate Bernoulli model in the previous section. If such a model consists of <span class="math notranslate nohighlight">\(n\)</span> random variables, say <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_n\)</span>, then we will write them as an <span class="math notranslate nohighlight">\(n\)</span>-dimensional random vector</p>
<div class="math notranslate nohighlight">
\[
\bX = (X_1,X_2,\ldots,X_n).
\]</div>
<p>For simplicity, we will assume that <span class="math notranslate nohighlight">\(\bX\)</span> is discrete, so that if <span class="math notranslate nohighlight">\(\btheta\)</span> is the parameter vector for the model, we have a joint mass function <span class="math notranslate nohighlight">\(p(\bx;\btheta)\)</span>. Then the same definitions given in the previous section for the univariate Bernoulli model apply here:</p>
<div class="proof definition admonition" id="gen-model-functions-def">
<p class="admonition-title"><span class="caption-number">Definition 13.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Consider a PGM trained as a generative model containing the random variables <span class="math notranslate nohighlight">\(\bX = (X_1,X_2,\ldots,X_n)\)</span>, and let <span class="math notranslate nohighlight">\(\btheta\)</span> be the parameter vector.</p>
<ol class="arabic">
<li><p>For fixed <span class="math notranslate nohighlight">\(\bx\in \bbr^n\)</span>, the <em>model likelihood function</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \calL(\btheta;\bx) \def p(\bx; \btheta),
    \]</div>
<p>thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>. The <em>model surprisal function</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \calI(\btheta;\bx) \def -\log \left[ \calL(\btheta;\bx) \right] = -\log \left[p(\bx; \btheta) \right],
    \]</div>
<p>also thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</li>
<li><p>For a fixed, observed dataset <span class="math notranslate nohighlight">\(\bx_1,\bx_2,\ldots,\bx_m\in \bbr^n\)</span>, the <em>data likelihood function</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \calL(\btheta;\bx_1,\ldots,\bx_m) \def p(\bx_1,\ldots,\bx_m; \btheta),
    \]</div>
<p>thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>. The <em>data surprisal function</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \calI(\btheta;\bx_1,\ldots,\bx_m) \def -\log \left[ \calL(\btheta;\bx_1,\ldots,\bx_m) \right] = -\log \left[p(\bx_1,\ldots,\bx_m; \btheta) \right],
    \]</div>
<p>also thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</li>
</ol>
</section>
</div><p>Sometimes, if mentioning the specific observation <span class="math notranslate nohighlight">\(\bx\)</span> or the observed dataset <span class="math notranslate nohighlight">\(\bx_1,\bx_2,\ldots,\bx_m\)</span> is not important, we will write the functions in the definition as</p>
<div class="math notranslate nohighlight">
\[
\calL_\text{model}(\btheta), \quad \calI_\text{model}(\btheta), \quad \calL_\text{data}(\btheta), \quad \calI_\text{data}(\btheta).
\]</div>
<p>Since observed datasets are assumed to be observations of IID random samples, we have:</p>
<div class="proof theorem admonition" id="likelihood-sur-decompose-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.3 </span> (Data likelihood/surprisal <span class="math notranslate nohighlight">\(=\)</span> product/sum of model likelihood/surprisal)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a PGM trained as a generative model containing the random variables <span class="math notranslate nohighlight">\(\bX = (X_1,X_2,\ldots,X_n)\)</span>, and let <span class="math notranslate nohighlight">\(\btheta\)</span> be the parameter vector. If</p>
<div class="math notranslate nohighlight">
\[
\bx_1,\bx_2,\ldots,\bx_m\in \bbr^n
\]</div>
<p>is an observed dataset, then we have</p>
<div class="math notranslate nohighlight">
\[
\calL_\text{data}(\btheta) = \prod_{i=1}^m \calL(\btheta; \bx_i) \quad \text{and} \quad \calI_\text{data}(\btheta) = \sum_{i=1}^m \calI(\btheta; \bx_i).
\]</div>
</section>
</div><p>Now, we state a version of <a class="reference internal" href="#equiv-obj-bern-thm">Theorem 13.1</a> that holds for generative models:</p>
<div class="proof theorem admonition" id="equiv-obj-gen-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.4 </span> (Equivalent learning objectives for generative PGMs)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a PGM trained as a generative model containing the random variables <span class="math notranslate nohighlight">\(\bX = (X_1,X_2,\ldots,X_n)\)</span>, and let <span class="math notranslate nohighlight">\(\btheta\)</span> be the parameter vector. Let</p>
<div class="math notranslate nohighlight">
\[
\bx_1,\bx_2,\ldots,\bx_m \in \bbr^n
\]</div>
<p>be an observed dataset, let <span class="math notranslate nohighlight">\(P_\btheta\)</span> be the model joint probability distribution, and let <span class="math notranslate nohighlight">\(\hat{P}\)</span> be the empirical distribution of the dataset. The following optimization objectives are equivalent:</p>
<ol class="arabic simple">
<li><p>Minimize the KL divergence <span class="math notranslate nohighlight">\(D(\hat{P} \parallel P_\btheta)\)</span> with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
<li><p>Minimize the cross entropy <span class="math notranslate nohighlight">\(H_{\hat{P}}(P_\btheta)\)</span> with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
<li><p>Minimize the data surprisal function <span class="math notranslate nohighlight">\(\calI(\btheta; \bx_1,\ldots,\bx_m)\)</span> with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
<li><p>Maximize the data likelihood function <span class="math notranslate nohighlight">\(\calL(\btheta; \bx_1,\ldots,\bx_m)\)</span> with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ol>
</section>
</div><p>The proof of the equivalence of these training objectives is the same as the proof in the special case of the univariate Bernoulli model in the previous section. The optimization process which seeks a solution to these (equivalent) optimization problems is called <em>maximum likelihood estimatation</em> (<em>MLE</em>), and any solution is called a <em>maximum likelihood estimate</em> (also <em>MLE</em>) and is denoted <span class="math notranslate nohighlight">\(\btheta_\text{MLE}^\star\)</span>.</p>
<p>We now turn toward discriminative models, which include all those models explicitly studied in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>. In this case, we must further distinguish between the models with discrete response variable <span class="math notranslate nohighlight">\(Y\)</span> versus a continuous one. Linear regression models are examples of the latter type, while we also briefly encountered an example of a neural network model with continuous <span class="math notranslate nohighlight">\(Y\)</span> in the worksheet to <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a>. For both of these models, the response variable <span class="math notranslate nohighlight">\(Y\)</span> was actually <em>normally</em> distributed (conditionally), so this will be the only case of continuous <span class="math notranslate nohighlight">\(Y\)</span> that we consider in this book.</p>
<div class="proof definition admonition" id="disc-model-functions-def">
<p class="admonition-title"><span class="caption-number">Definition 13.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Consider a PGM trained as a discriminative model with predictor vector <span class="math notranslate nohighlight">\(\bX\)</span>, response variable <span class="math notranslate nohighlight">\(Y\)</span>, and parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
<ol class="arabic">
<li><p>For fixed <span class="math notranslate nohighlight">\(\bx\in \bbr^n\)</span>, the <em>model likelihood function</em> is given either by</p>
<div class="math notranslate nohighlight">
\[
    \calL(\btheta; \ y\mid \bx) \def p(y\mid \bx;\  \btheta) \quad \text{or} \quad \calL(\btheta; \ y\mid \bx) \def f(y\mid \bx;\  \btheta),
    \]</div>
<p>depending on whether <span class="math notranslate nohighlight">\(Y\)</span> is (conditionally) discrete or continuous. The model likelihood function is thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>. The <em>model surprisal function</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \calI(\btheta; \ y\mid \bx) \def -\log \left[ \calL(y\mid \btheta; \ \bx) \right],
    \]</div>
<p>also thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</li>
<li><p>For a fixed, observed dataset</p>
<div class="math notranslate nohighlight">
\[
    (\bx_1,y_1),(\bx_2,y_2),\ldots,(\bx_m,y_m)\in \bbr^n \times \bbr,
    \]</div>
<p>the <em>data likelihood function</em> is given either by</p>
<div class="math notranslate nohighlight">
\[
    \calL(\btheta; \ y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m) \def p(y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m; \ \btheta)
    \]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
    \calL(\btheta; \ y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m) \def f(y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m; \ \btheta)
    \]</div>
<p>depending on whether <span class="math notranslate nohighlight">\(Y\)</span> is (conditionally) discrete or continuous. The data likelihood function is thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>. The <em>data surprisal function</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \calI(\btheta;\ y\mid \bx_1,\ldots,\bx_m) \def -\log \left[ \calL(\btheta; \ y\mid \bx_1,\ldots,\bx_m) \right],
    \]</div>
<p>also thought of as a function of <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</li>
</ol>
</section>
</div><p>As with generative models, if mentioning the specific observation or the observed dataset is not important, we will write the functions in the definition as</p>
<div class="math notranslate nohighlight">
\[
\calL_\text{model}(\btheta), \quad \calI_\text{model}(\btheta), \quad \calL_\text{data}(\btheta), \quad \calI_\text{data}(\btheta).
\]</div>
<p>From independence, we also get the analog of <a class="reference internal" href="#likelihood-sur-decompose-thm">Theorem 13.3</a>:</p>
<div class="proof theorem admonition" id="likelihood-sur-decompose-disc-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.5 </span> (Data likelihood/surprisal <span class="math notranslate nohighlight">\(=\)</span> product/sum of model likelihood/surprisal)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a PGM trained as a discriminative model with predictor vector <span class="math notranslate nohighlight">\(\bX\)</span>, response variable <span class="math notranslate nohighlight">\(Y\)</span>, and parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. If</p>
<div class="math notranslate nohighlight">
\[
(\bx_1,y_1),(\bx_2,y_2),\ldots,(\bx_m,y_m)\in \bbr^n \times \bbr
\]</div>
<p>is an observed dataset, then we have</p>
<div class="math notranslate nohighlight">
\[
\calL_\text{data}(\btheta) = \prod_{i=1}^m \calL(\btheta; \ y_i \mid \bx_i) \quad \text{and} \quad \calI_\text{data}(\btheta) = \sum_{i=1}^m \calI(\btheta; \ y_i \mid \bx_i).
\]</div>
</section>
</div><p>Just as for generative models, for discriminative models we have the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \def E_{(\bx, y) \sim \hat{p}(\bx, y)} \left[ \calI(\btheta; \ y \mid \bx) \right] = \frac{1}{m} \sum_{i=1}^m \calI(\btheta; \ y_i \mid \bx_i),
\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>For reasons of space and time, we did not study <em>conditional</em> information-theoretic measures in <a class="reference internal" href="10-info-theory.html#information-theory"><span class="std std-numref">Chapter 10</span></a>. For a discussion of these quantities, see <span id="id2">[<a class="reference internal" href="bib.html#id17" title="T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley &amp; Sons, Inc., second edition, 2006.">CT06</a>]</span>.</p>
</aside>
<p>where <span class="math notranslate nohighlight">\(\hat{p}(\bx,y)\)</span> is the empirical joint mass function of an observed dataset. For generative models, this function was exactly the cross entropy from the empirical distribution to the model distribution—but for discriminative models, it has a different interpretation. In the case that <span class="math notranslate nohighlight">\(Y\)</span> is discrete, it is actually a type of <em>conditional</em> cross entropy, and minimizing this objective is the same as minimizing the <em>conditional</em> KL divergence. In the case that <span class="math notranslate nohighlight">\(Y\)</span> is conditionally normal, the objective <span class="math notranslate nohighlight">\(J(\btheta)\)</span> “is” the <em>mean squared error</em> (or <em>MSE</em>), which we first encountered in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/programming-assignments/assignment_12.ipynb">programming assignment</a> for the previous chapter. We say that it “is” the MSE, with quotation marks, because it’s not <em>quite</em> equal to the MSE on the nose—this is explained precisely in the following result:</p>
<div class="proof theorem admonition" id="MSE-min-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.6 </span> (Mean squared error as a stochastic objective function)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a PGM trained as a discriminative model with predictor vector <span class="math notranslate nohighlight">\(\bX\)</span>, response variable <span class="math notranslate nohighlight">\(Y\)</span>, parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>, and let</p>
<div class="math notranslate nohighlight">
\[
(\bx_1,y_1),(\bx_2,y_2),\ldots,(\bx_m,y_m) \in \bbr^n \times \bbr
\]</div>
<p>be an observed dataset with empirical joint mass function <span class="math notranslate nohighlight">\(\hat{p}(\bx,y)\)</span>. Suppose also that the conditional distribution of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(\bX\)</span> is normal, with <em>fixed</em> variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, and that the mean <span class="math notranslate nohighlight">\(\mu = \mu(\bx,\btheta)\)</span> of the distribution is the link function. Then the minimizers of the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
E_{(\bx, y) \sim \hat{p}(\bx, y)} \left[ \calI(\btheta; \ y \mid \bx) \right] = \frac{1}{m} \sum_{i=1}^m \calI(\btheta; \ y_i \mid \bx_i)
\]</div>
<p>are the same as the minimizers of the mean squared error</p>
<div class="math notranslate nohighlight">
\[
MSE(\btheta) \def \frac{1}{m} \sum_{i=1}^m (y_i - \mu_i)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_i = \mu(\bx_i,\btheta)\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof begins with a simple computation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\calI(\btheta; \ y_i \mid \bx_i) &amp;= - \log \left\{ \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left[- \frac{1}{2\sigma^2} (y_i - \mu_i)^2 \right] \right\} \\
&amp;= \frac{1}{2} \log{\left(2\pi \sigma^2\right)} + \frac{1}{2\sigma^2} (y_i - \mu_i)^2.
\end{align*}\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{m} \sum_{i=1}^m \calI(\btheta; \ y_i \mid \bx_i) = \frac{1}{2}\log\left(2\pi \sigma^2\right) + \frac{1}{2m\sigma^2} \sum_{i=1}^m (y_i - \mu_i)^2.
\]</div>
<p>Assuming that the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is <em>fixed</em>, we immediately see that minimizing the left-hand side with respect to <span class="math notranslate nohighlight">\(\btheta\)</span> is the same as minimizing the MSE. Q.E.D.</p>
</div>
<p>Since multiplying an objective function by a positive constant does not change its extremizers, we may modify the mean squared error function in several ways to best suit the context. For example, sometimes it is convenient to instead consider the <em>residual sum of squares</em> function</p>
<div class="math notranslate nohighlight">
\[
RSS(\btheta) \def  \sum_{i=1}^m (y_i - \mu_i)^2,
\]</div>
<p>while in other situations we might even divide this latter quantity by <span class="math notranslate nohighlight">\(2\)</span> and consider the equivalent objective</p>
<div class="math notranslate nohighlight">
\[
RSS(\btheta) /2 = \frac{1}{2} \sum_{i=1}^m (y_i - \mu_i)^2.
\]</div>
<p>See, for example, <a class="reference internal" href="#mle-lin-reg-thm">Theorem 13.8</a> in the next section.</p>
<p>Our discussion on the identity of the stochastic objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> is summarized in the following chart:</p>
<a class="reference internal image-reference" href="../_images/flow-training.svg"><img alt="../_images/flow-training.svg" class="align-center" src="../_images/flow-training.svg" width="100%" /></a>
<p> </p>
<p>Again, care must be taken in interpreting <span class="math notranslate nohighlight">\(J(\btheta)\)</span> in the case of a discriminative model with (conditionally) normal <span class="math notranslate nohighlight">\(Y\)</span>, since it is technically not equal to the MSE exactly, and we must also assume that the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> is <em>fixed</em>, as discussed in <a class="reference internal" href="#MSE-min-thm">Theorem 13.6</a>. In any case, we may now state the following theorem, which is a version of <a class="reference internal" href="#equiv-obj-gen-thm">Theorem 13.4</a> for discriminative models:</p>
<div class="proof theorem admonition" id="equiv-obj-disc-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.7 </span> (Equivalent learning objectives for discriminative PGMs)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a PGM trained as a discriminative model with predictor vector <span class="math notranslate nohighlight">\(\bX\)</span>, response variable <span class="math notranslate nohighlight">\(Y\)</span>, and parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[
(\bx_1,y_1),(\bx_2,y_2),\ldots,(\bx_m,y_m) \in \bbr^n \times \bbr
\]</div>
<p>be an observed dataset, with empirical joint mass function <span class="math notranslate nohighlight">\(\hat{p}(\bx,y)\)</span>. The following optimization objectives are equivalent:</p>
<ol class="arabic">
<li><p>Minimize the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
    J(\btheta) \def E_{(\bx, y) \sim \hat{p}(\bx, y)} \left[ \calI(\btheta; \ y \mid \bx) \right] = \frac{1}{m} \sum_{i=1}^m \calI(\btheta; \ y_i \mid \bx_i)
    \]</div>
<p>with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</li>
<li><p>Minimize the data surprisal function <span class="math notranslate nohighlight">\(\calI(\btheta; \ y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m)\)</span> with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
<li><p>Maximize the data likelihood function <span class="math notranslate nohighlight">\(\calL(\btheta; \ y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m)\)</span> with respect to <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ol>
</section>
</div><p>Just as for generative models, the optimization process which seeks a solution to these (equivalent) optimization problems is called <em>maximum likelihood estimation</em> (<em>MLE</em>), and any solution is called a <em>maximum likelihood estimate</em> (also <em>MLE</em>) and is denoted <span class="math notranslate nohighlight">\(\btheta_\text{MLE}^\star\)</span>.</p>
</section>
<section id="mle-for-linear-regression">
<h2><span class="section-number">13.3. </span>MLE for linear regression<a class="headerlink" href="#mle-for-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Having studied maximum likelihood estimation in general, we now turn toward specific examples, beginning with linear regression models. These are trained as discriminative models with a response variable <span class="math notranslate nohighlight">\(Y\)</span> which is (conditionally) normal. If we assume that the variance parameter <span class="math notranslate nohighlight">\(\sigma^2\)</span> is <em>fixed</em>, then the underlying graph of the model is of the form</p>
<p> </p>
<a class="reference internal image-reference" href="../_images/log-reg-00.svg"><img alt="../_images/log-reg-00.svg" class="align-center" src="../_images/log-reg-00.svg" width="35%" /></a>
<p> </p>
<p>where <span class="math notranslate nohighlight">\(\beta_0 \in \bbr\)</span> and <span class="math notranslate nohighlight">\(\bbeta \in \mathbb{R}^{n}\)</span> are the only parameters. The link function at <span class="math notranslate nohighlight">\(Y\)</span> is still given by</p>
<div class="math notranslate nohighlight">
\[
Y \mid \bX=\bx ; \ \beta_0,\bbeta \sim \mathcal{N}(\mu, \sigma^2), \quad \text{where} \quad \mu = \beta_0 + \bx^\intercal \bbeta.
\]</div>
<p>For these models, it turns out MLEs are obtainable in closed form:</p>
<div class="proof theorem admonition" id="mle-lin-reg-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.8 </span> (MLEs for linear regression models with known variance)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a linear regression model with <em>fixed</em> variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, and let</p>
<div class="math notranslate nohighlight">
\[
(\bx_1,y_1),(\bx_2,y_2),\ldots,(\bx_m,y_m) \in \bbr^n \times \bbr
\]</div>
<p>be an observed dataset. Supposing <span class="math notranslate nohighlight">\(\bx_i^\intercal= (x_{i1},x_{i2},\ldots,x_{in})\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>, let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbfcal{X} = \begin{bmatrix}
1 &amp; x_{11} &amp; \cdots &amp; x_{1n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{m1} &amp; \cdots &amp; x_{mn}
\end{bmatrix}, \quad \by = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}, \quad \btheta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n \end{bmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bbeta^\intercal = (\beta_1,\ldots,\beta_n)\)</span>. Provided that the <span class="math notranslate nohighlight">\((n+1) \times (n+1)\)</span> square matrix <span class="math notranslate nohighlight">\(\mathbfcal{X}^T \mathbfcal{X}\)</span> is invertible, maximum likelihood estimates for the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\bbeta\)</span> are given by</p>
<div class="math notranslate nohighlight">
\[
\btheta_\text{MLE}^\star = \left(\mathbfcal{X}^T \mathbfcal{X}\right)^{-1}\mathbfcal{X}^T \by.
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. As we noted above in the discussion after the proof of <a class="reference internal" href="#MSE-min-thm">Theorem 13.6</a>, the MLE may be obtained as the minimizer of half the residual sum of squares:</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \def RSS(\btheta)/2 = \frac{1}{2} \sum_{i=1}^m (y_i - \mu_i)^2 = \frac{1}{2} \left( \by - \mathbfcal{X}\btheta\right)^\intercal \left( \by - \mathbfcal{X}\btheta\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_i = \beta_0 + \bx_i^\intercal \bbeta\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>. But as we will show in the worksheet problem directly after this proof, taking the gradient gives</p>
<div class="math notranslate nohighlight">
\[
\nabla J(\btheta) = \left( \by - \mathbfcal{X}\btheta\right)^\intercal \nabla_\btheta \left(\by - \mathbfcal{X}\btheta \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla_\btheta \left(\by - \mathbfcal{X}\btheta \right)\)</span> is the Jacobian matrix of the vector-valued function <span class="math notranslate nohighlight">\(\btheta \mapsto \by - \mathbfcal{X} \btheta\)</span>. But it is easy to show that <span class="math notranslate nohighlight">\(\nabla_\btheta \left(\by - \mathbfcal{X}\btheta \right) = - \mathbfcal{X}\)</span>, and so</p>
<div class="math notranslate nohighlight" id="equation-grad-rss-eq">
<span class="eqno">(13.8)<a class="headerlink" href="#equation-grad-rss-eq" title="Permalink to this equation">#</a></span>\[
\nabla J(\btheta) =  -\left( \by - \mathbfcal{X}\btheta\right)^\intercal \mathbfcal{X}.
\]</div>
<p>Setting the gradient to zero and rearranging gives</p>
<div class="math notranslate nohighlight">
\[
\mathbfcal{X}^\intercal \mathbfcal{X} \btheta = \mathbfcal{X}^\intercal \by,
\]</div>
<p>from which the desired equation follows.</p>
<p>The only thing that is left to prove is that we have actually obtained a global minimizer. But this follows from convexity of the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span>, which we may demonstrate by showing the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta)\)</span> is positive semidefinite (see <a class="reference internal" href="11-optim.html#main-convex-multi-thm">Theorem 11.7</a>). To do this, note that <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta) = \mathbfcal{X}^\intercal \mathbfcal{X}\)</span> from <a class="reference internal" href="#equation-grad-rss-eq">(13.8)</a>. But then, given any vector <span class="math notranslate nohighlight">\(\bz \in \bbr^{n+1}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\bz^\intercal \nabla^2 J(\btheta) \bz = \bz^\intercal \mathbfcal{X}^\intercal \mathbfcal{X} \bz = (\mathbfcal{X}\bx)^\intercal \mathbfcal{X} \bz = |\mathbfcal{X}\bz|^2 \geq 0.
\]</div>
<p>Thus the Hessian matrix is indeed positive semidefinite. Q.E.D.</p>
</div>
<p>As we saw in the proof, the maximum likelihood parameter estimates are those that minimize the residual sum of squares <span class="math notranslate nohighlight">\(RSS(\btheta)\)</span>, which explains why the MLEs are also often called the <em>ordinary least squares</em> (<em>OLS</em>) estimates.</p>
<p>It is worth writing out the MLEs in the case of simple linear regression:</p>
<div class="proof corollary admonition" id="mle-simple-lin-reg-cor">
<p class="admonition-title"><span class="caption-number">Corollary 13.1 </span> (MLEs for simple linear regression models with known variance)</p>
<section class="corollary-content" id="proof-content">
<p>Let the notation be as in <a class="reference internal" href="#mle-lin-reg-thm">Theorem 13.8</a>, but assume that <span class="math notranslate nohighlight">\(\bX\)</span> is <span class="math notranslate nohighlight">\(1\)</span>-dimensional, equal to a random variable <span class="math notranslate nohighlight">\(X\)</span>. Then MLEs for the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(\beta_1)_\text{MLE}^\star &amp;= \frac{\sum_{i=1}^m \left(x_i - \bar{x} \right)\left( y_i - \bar{y} \right)}{\sum_{i=1}^m \left(x_i - \bar{x} \right)^2}, \\
(\beta_0)_\text{MLE}^\star &amp;= \bar{y} - \beta_1 \bar{x},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{m} \sum_{i=1}^m x_i\)</span> and <span class="math notranslate nohighlight">\(\bar{y} = \frac{1}{m} \sum_{i=1}^m y_i\)</span> are the empirical means.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. First note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbfcal{X}^\intercal \mathbfcal{X} = \begin{bmatrix} m &amp; m \bar{x} \\ m \bar{x} &amp; \sum_{i=1}^m x_i^2 \end{bmatrix}.
\end{split}\]</div>
<p>Assuming this matrix has nonzero determinant, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\mathbfcal{X}^\intercal \mathbfcal{X} \right)^{-1} = \frac{1}{m \sum_{i=1}^m x_i^2 - m^2 \bar{x}^2} \begin{bmatrix} \sum_{i=1}^m x_i^2 &amp; -m \bar{x} \\ -m \bar{x} &amp; m \end{bmatrix}.
\end{split}\]</div>
<p>But</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbfcal{X}^\intercal \by = \begin{bmatrix} m \bar{y} \\ \sum_{i=1}^m x_i y_i \end{bmatrix},
\end{split}\]</div>
<p>and so from</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = \btheta =  \left(\mathbfcal{X}^\intercal \mathbfcal{X}\right)^{-1}\mathbfcal{X}^\intercal \by
\end{split}\]</div>
<p>we conclude</p>
<div class="math notranslate nohighlight">
\[
\beta_1 = \frac{\sum_{i=1}^m x_i y_i -m \bar{x}\bar{y} }{ \sum_{i=1}^m x_i^2 - m \bar{x}^2}.
\]</div>
<p>But as you may easily check, we have</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m x_i y_i -m \bar{x}\bar{y}  = \sum_{i=1}^m \left(x_i - \bar{x} \right)\left( y_i - \bar{y} \right)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m x_i^2 - m \bar{x}^2 = \sum_{i=1}^m \left(x_i - \bar{x} \right)^2,
\]</div>
<p>from which the desired equation for <span class="math notranslate nohighlight">\(\beta_1\)</span> follows. To obtain the equation for <span class="math notranslate nohighlight">\(\beta_0\)</span>, note that the equation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbfcal{X}^\intercal \mathbfcal{X} \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = \mathbfcal{X}^\intercal \by 
\end{split}\]</div>
<p>implies <span class="math notranslate nohighlight">\(m \beta_0  + m \beta_1 \bar{x} = m \bar{y}\)</span>, and so <span class="math notranslate nohighlight">\(\beta_0 = \bar{y} - \beta_1 \bar{x}\)</span>. Q.E.D.</p>
</div>
<p>To illustrate the concepts, let’s return yet again to the Ames housing dataset (see the description at the beginning of <a class="reference internal" href="12-models.html#lin-reg-sec"><span class="std std-numref">Section 12.3</span></a>). While in principle we may compute the <em>exact</em> MLEs for a linear regression model on this data, it is amusing to approximate them using stochastic gradient descent. To do this, however, we must “standardize” the area and price features for numerical stability, which means that we subtract the empirical means and divide by the standard deviations. When we do so, we get a scatter plot that looks like:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># import data</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/data-3-1.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;area&#39;</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">])</span>

<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">data_std</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data_std</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">data_std</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;standardized area&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;standardized price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/240f48dce3bed5771f9763850273e07b6dce492957c5a1e6024905e68eb75d1c.svg" src="../_images/240f48dce3bed5771f9763850273e07b6dce492957c5a1e6024905e68eb75d1c.svg" /></figure>
</div>
</div>
<p>Notice that both features are on similar scales. Then, we run the algorithm using the mean squared error function</p>
<div class="math notranslate nohighlight">
\[
MSE(\btheta) = \frac{1}{m} \sum_{i=1}^m (y_i - \beta_0 - \beta x_i)^2, \quad \btheta = (\beta_0, \beta),
\]</div>
<p>as the objective function:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data_std</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data_std</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">beta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span> <span class="n">beta0</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">beta0</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta0&#39;</span><span class="p">]</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">beta0</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">512</span>

<span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">plot_sgd</span><span class="p">(</span><span class="n">sgd_output</span><span class="p">,</span>
         <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;mean squared error (MSE)&#39;</span><span class="p">,</span>
         <span class="n">per_step_label</span><span class="o">=</span><span class="s1">&#39;MSE per step&#39;</span><span class="p">,</span>
         <span class="n">per_epoch_label</span><span class="o">=</span><span class="s1">&#39;mean MSE per epoch&#39;</span><span class="p">,</span>
         <span class="n">per_epoch_color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span>
         <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
         <span class="n">per_step_alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/0b2fab97b76020aba44711db28ef537d1dac0deb90b08e8ca5d8ebdaa8de2391.svg" src="../_images/0b2fab97b76020aba44711db28ef537d1dac0deb90b08e8ca5d8ebdaa8de2391.svg" /></figure>
</div>
</div>
</section>
<section id="mle-for-logistic-regression">
<h2><span class="section-number">13.4. </span>MLE for logistic regression<a class="headerlink" href="#mle-for-logistic-regression" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/ch12-book-data-01.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># convert the data to numpy arrays</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="s1">&#39;x_2&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># convert the data to torch tensors</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># plot the data</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="c1"># change the default seaborn legend</span>
<span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">new_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">new_labels</span><span class="p">):</span>
    <span class="n">t</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/6734d6bbfbe5eba426bc3f5eefbff28c37edadcbd64554800fa67c0981a26b83.svg" src="../_images/6734d6bbfbe5eba426bc3f5eefbff28c37edadcbd64554800fa67c0981a26b83.svg" /></figure>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the link function at Y</span>
<span class="k">def</span> <span class="nf">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">beta0</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta0&#39;</span><span class="p">]</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">beta0</span> <span class="o">+</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span><span class="p">)</span>

<span class="c1"># define the data surprisal function</span>
<span class="k">def</span> <span class="nf">I</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probs</span><span class="p">))</span>

<span class="c1"># define the predictor</span>
<span class="k">def</span> <span class="nf">predictor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">probs</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

<span class="c1"># initialize the weights and biases</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">beta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span> <span class="n">beta0</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">}</span>

<span class="c1"># run gradient descent</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">I</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="c1"># define grid for contour plot</span>
<span class="n">resolution</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x1_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="n">x2_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">x1_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">x2_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))))</span>

<span class="c1"># define colormap for the contour plots</span>
<span class="n">desat_blue</span> <span class="o">=</span> <span class="s1">&#39;#7F93FF&#39;</span>
<span class="n">desat_magenta</span> <span class="o">=</span> <span class="s1">&#39;#FF7CFE&#39;</span>
<span class="n">binary_cmap</span> <span class="o">=</span> <span class="n">clr</span><span class="o">.</span><span class="n">LinearSegmentedColormap</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="n">desat_blue</span><span class="p">,</span> <span class="n">desat_magenta</span><span class="p">],</span> <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">epoch_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="p">]</span>
<span class="n">running_parameters</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">running_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="c1"># plot the objective function</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">grad_steps</span><span class="p">,</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;surprisal per step&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;surprisal&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="n">epoch</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># apply the fitted model to the grid</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

    <span class="c1"># plot the decision boundary and colors</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">resolution</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">binary_cmap</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>

    <span class="c1"># plot the data</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># change the default seaborn legend</span>
    <span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">new_labels</span><span class="p">):</span>
        <span class="n">t</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;gradient descent</span><span class="se">\n</span><span class="s1">$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/53391a74e60b4ea6463d856a2a46c5932cdc3fbc7c0fc5b7cb77de04e56b1e41.svg" src="../_images/53391a74e60b4ea6463d856a2a46c5932cdc3fbc7c0fc5b7cb77de04e56b1e41.svg" /></figure>
</div>
</div>
</section>
<section id="mle-for-neural-networks">
<h2><span class="section-number">13.5. </span>MLE for neural networks<a class="headerlink" href="#mle-for-neural-networks" title="Permalink to this heading">#</a></h2>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/ch12-book-data-02.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># convert the data to numpy arrays</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="s1">&#39;x_2&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># convert the data to torch tensors</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># plot the data</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="c1"># change the default seaborn legend</span>
<span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">new_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">new_labels</span><span class="p">):</span>
    <span class="n">t</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/5298c71748ff02b5f43ece36798072d072dfc56d0f37a8251477635e226d9a51.svg" src="../_images/5298c71748ff02b5f43ece36798072d072dfc56d0f37a8251477635e226d9a51.svg" /></figure>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the link function at Y</span>
<span class="k">def</span> <span class="nf">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">Z_0</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">Z_1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z_0</span> <span class="o">@</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;weight_1&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;bias_1&#39;</span><span class="p">])</span>
    <span class="n">Z_2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z_1</span> <span class="o">@</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;weight_2&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;bias_2&#39;</span><span class="p">])</span>
    <span class="n">Z_3</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">Z_2</span> <span class="o">@</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;weight_3&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;bias_3&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">Z_3</span> <span class="o">@</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;weight_4&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;bias_4&#39;</span><span class="p">])</span>

<span class="c1"># define the model surprisal function</span>
<span class="k">def</span> <span class="nf">I</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probs</span><span class="p">)</span>

<span class="c1"># define the predictor</span>
<span class="k">def</span> <span class="nf">predictor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">probs</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

<span class="c1"># define the network architecture</span>
<span class="n">k1</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># width of first hidden layer</span>
<span class="n">k2</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># width of second hidden layer</span>
<span class="n">k3</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># width of third hidden layer</span>
<span class="n">widths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">k3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># initialize the weights and biases</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="mi">1</span><span class="o">/</span><span class="n">sqrt</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">sqrt</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">a</span><span class="o">=-</span><span class="mi">1</span><span class="o">/</span><span class="n">sqrt</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">sqrt</span><span class="p">(</span><span class="n">widths</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">theta0</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">|</span> <span class="p">{</span><span class="s1">&#39;weight_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">weight</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()}</span>
    <span class="n">theta0</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">|</span> <span class="p">{</span><span class="s1">&#39;bias_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">):</span> <span class="n">bias</span><span class="p">}</span>

<span class="c1"># run SGD</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">80</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="n">I</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># get the grid for the contour plot</span>
<span class="n">resolution</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x1_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.75</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="n">x2_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">x1_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">x2_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))))</span>

<span class="n">epoch_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">750</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">running_parameters</span> <span class="o">=</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">parameters</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">running_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="c1"># plot the objective function</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">grad_steps</span><span class="p">,</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cross entropy per step&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">epoch_step_nums</span><span class="p">,</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_epoch_objectives</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mean cross entropy per epoch&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;cross entropy&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="n">epoch</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># apply the fitted model to the grid</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

    <span class="c1"># plot the decision boundary and colors</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">resolution</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">binary_cmap</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>

    <span class="c1"># plot the data</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># change the default seaborn legend</span>
    <span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">new_labels</span><span class="p">):</span>
        <span class="n">t</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mini-batch gradient descent</span><span class="se">\n</span><span class="s1">$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, $k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/9e094021e21a96599fd4fe5250d1a7f676fdcc5e226a909f1095005b48d4de45.svg" src="../_images/9e094021e21a96599fd4fe5250d1a7f676fdcc5e226a909f1095005b48d4de45.svg" /></figure>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="12-models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Probabilistic graphical models</p>
      </div>
    </a>
    <a class="right-next"
       href="14-stats-estimators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Statistics and general parameter estimation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-first-look-at-likelihood-based-learning-objectives">13.1. A first look at likelihood-based learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-mle">13.2. General MLE</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-linear-regression">13.3. MLE for linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-logistic-regression">13.4. MLE for logistic regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-neural-networks">13.5. MLE for neural networks</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>