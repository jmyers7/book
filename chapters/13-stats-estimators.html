

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>13. Statistics and general parameter estimation &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bA": "\\mathbf{A}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bS": "\\mathbf{S}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/13-stats-estimators';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14. Large sample theory and more sampling distributions" href="14-asymptotic.html" />
    <link rel="prev" title="12. Learning" href="12-learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-info-theory.html">9. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-optim.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-models.html">11. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-learning.html">12. Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">13. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-asymptotic.html">14. Large sample theory and more sampling distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/13-stats-estimators.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Statistics and general parameter estimation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics">13.1. Statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-parametric-models">13.2. General parametric models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimators">13.3. Parameter estimators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blah">13.4. Blah</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-models">13.5. Bayesian models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-estimators">13.6. Bayes estimators</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="statistics-and-general-parameter-estimation">
<span id="stats-estimators"></span><h1><span class="section-number">13. </span>Statistics and general parameter estimation<a class="headerlink" href="#statistics-and-general-parameter-estimation" title="Permalink to this heading">#</a></h1>
<section id="statistics">
<h2><span class="section-number">13.1. </span>Statistics<a class="headerlink" href="#statistics" title="Permalink to this heading">#</a></h2>
<div class="proof definition admonition" id="statistic-def">
<p class="admonition-title"><span class="caption-number">Definition 13.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX\)</span> be a <span class="math notranslate nohighlight">\(k\)</span>-dimensional random vector. A <em>(<span class="math notranslate nohighlight">\(d\)</span>-dimensional) statistic</em> is a random vector of the form</p>
<div class="math notranslate nohighlight">
\[
T = r(\bX),
\]</div>
<p>where <span class="math notranslate nohighlight">\(r:\bbr^k \to \bbr^d\)</span> is a vector-valued function. An observed value <span class="math notranslate nohighlight">\(t\)</span> of <span class="math notranslate nohighlight">\(T\)</span> is called an <em>observed statistic</em> or <em>empirical statistic</em>.</p>
</section>
</div><p>If we conceptualize the random vector <span class="math notranslate nohighlight">\(\bX\)</span> as (theoretical) data, then a statistic is simply a function of the data. Crucially important examples of statistics include those defined as follows:</p>
<div class="proof definition admonition" id="sample-mean-var-def">
<p class="admonition-title"><span class="caption-number">Definition 13.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_m)\)</span> be an <span class="math notranslate nohighlight">\(m\)</span>-dimensional random vector. The <em>sample mean</em> is defined to be the statistic</p>
<div class="math notranslate nohighlight">
\[
\overline{X} \def \frac{1}{m}(X_1+\cdots+X_m),
\]</div>
<p>while the <em>sample variance</em> is defined to be the statistic</p>
<div class="math notranslate nohighlight">
\[
S^2 \def \frac{1}{m-1} \sum_{i=1}^m(X_i - \overline{X})^2.
\]</div>
<p>The corresponding empirical statistics are the <em>empirical mean</em> and <em>empirical variance</em> defined as</p>
<div class="math notranslate nohighlight">
\[
\overline{x} \def \frac{1}{m}(x_1+\cdots+x_m) \quad \text{and} \quad s^2 = \frac{1}{m-1} \sum_{i=1}^m(x_i - \overline{x})^2.
\]</div>
</section>
</div><p>Very often, the component random variables <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> of the random vector <span class="math notranslate nohighlight">\(\bX\)</span> in the definition are assumed to form a random sample, i.e., an IID sequence of random variables. The dimension <span class="math notranslate nohighlight">\(m\)</span> is then referred to as the <em>sample size</em>. In principle, then, the sample size <span class="math notranslate nohighlight">\(m\)</span> can be <em>any</em> positive integer, and so it is often convenient to write <span class="math notranslate nohighlight">\(\overline{X}_m\)</span> for the sample mean, explicitly displaying the sample size. This gives us an entire <em>infinite sequence</em> of sample means:</p>
<div class="math notranslate nohighlight" id="equation-seq-means-eqn">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-seq-means-eqn" title="Permalink to this equation">#</a></span>\[
\overline{X}_1,\overline{X}_2,\ldots,\overline{X}_m, \ldots.
\]</div>
<p>Since statistics are random vectors, they have their own probability distributions. These are given special names:</p>
<div class="proof definition admonition" id="samp-dist-def">
<p class="admonition-title"><span class="caption-number">Definition 13.3 </span></p>
<section class="definition-content" id="proof-content">
<p>The probability distribution of a statistic <span class="math notranslate nohighlight">\(T\)</span> is called the <em>sampling distribution</em> of <span class="math notranslate nohighlight">\(T\)</span>.</p>
</section>
</div><p>The sampling distributions for sample means <span class="math notranslate nohighlight">\(\overline{X}_m\)</span> are particularly important, and one of the main goals of <a class="reference internal" href="14-asymptotic.html#asymptotic"><span class="std std-numref">Chapter 14</span></a> is to study the limiting behavior (or <em>asymptotic behavior</em>) of the sampling distributions in the sequence <a class="reference internal" href="#equation-seq-means-eqn">(13.1)</a> as <span class="math notranslate nohighlight">\(m\to \infty\)</span>.</p>
<p>In general, however, computing the sampling distributions is difficult. But if we actually have <em>observed</em> data <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span>, then (as you will explore in the programming assignment) there is a resampling method known as <em>bootstrapping</em> that yields  approximations to sampling distributions. An example is given by the histogram (with KDE) on the right-hand side of the following figure, where a histogram (with KDE) of the empirical distribution of an observed dataset is given on the left-hand side:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="k">as</span> <span class="nn">clr</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">sample_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">resample_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">random_sample</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
<span class="n">replicate_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">num_resamples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_resamples</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">random_sample</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">resample_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">replicate_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">random_sample</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">replicate_means</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;observed data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;bootstrap sampling distribution of sample mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/06aab3a4acfd9cb950933a8cefb4bfada3207d345ec2fd6138d92b02e6260622.svg" src="../_images/06aab3a4acfd9cb950933a8cefb4bfada3207d345ec2fd6138d92b02e6260622.svg" /></figure>
</div>
</div>
<p>Observe that the sampling distribution on the right-hand side appears to be well approximated by a normal distribution. This is actually a manifestation of the asymptotic behavior of sample means that we alluded to above; indeed, as we will see in <a class="reference internal" href="14-asymptotic.html#asymptotic"><span class="std std-numref">Chapter 14</span></a>, the Central Limit Theorem tells us that the sequence <a class="reference internal" href="#equation-seq-means-eqn">(13.1)</a> of sample means converges (in distribution) to a normal distribution as <span class="math notranslate nohighlight">\(m\to \infty\)</span>, provided that the random variables are IID. This is true even though the observed data are definitely <em>not</em> normally distributed. Moreover, the mean of the sampling distribution is approximately <span class="math notranslate nohighlight">\(4.924\)</span>, while the mean of the observed data is approximately <span class="math notranslate nohighlight">\(4.928\)</span>. The fact that these means are nearly equal is a consequence of another theorem in <a class="reference internal" href="14-asymptotic.html#asymptotic"><span class="std std-numref">Chapter 14</span></a> called the Law of Large Numbers. These asymptotic results provide the foundation for the large-sample <em>confidence intervals</em> that we will construct in <a class="reference internal" href="15-CIs.html#cis"><span class="std std-numref">Chapter 15</span></a>.</p>
<p>Let’s consider the sample mean a little closer:</p>
<div class="proof theorem admonition" id="prop-sample-mean-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.1 </span> (Properties of the sample mean)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> be an IID random sample from a distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<ol class="arabic simple">
<li><p>The expectation of the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>The variance of the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span> is <span class="math notranslate nohighlight">\(\sigma^2/m\)</span>, and hence its standard deviation is <span class="math notranslate nohighlight">\(\sigma/\sqrt{m}\)</span>.</p></li>
<li><p>If the <span class="math notranslate nohighlight">\(X_i\)</span>’s are normally distributed, then so too is the sample mean <span class="math notranslate nohighlight">\(\overline{X}\)</span>.</p></li>
</ol>
</section>
</div><p>You will prove this theorem as</p>
</section>
<section id="general-parametric-models">
<h2><span class="section-number">13.2. </span>General parametric models<a class="headerlink" href="#general-parametric-models" title="Permalink to this heading">#</a></h2>
<p>Abstracting away all the intricate particularities of the fully-observed probabilistic graphical models in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a> reveals that they all are examples of the following type of general structure:</p>
<div class="proof definition admonition" id="gen-para-model-def">
<p class="admonition-title"><span class="caption-number">Definition 13.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX\)</span> be a <span class="math notranslate nohighlight">\(k\)</span>-dimensional random vector and let <span class="math notranslate nohighlight">\(\Omega\)</span> be a (nonempty) subset of a Euclidean space <span class="math notranslate nohighlight">\(\bbr^d\)</span>. A <em>parametric probabilistic model</em> (or simply a <em>parametric model</em>) for <span class="math notranslate nohighlight">\(\bX\)</span> is a specification of a dependence of the probability distribution of <span class="math notranslate nohighlight">\(\bX\)</span> on values <span class="math notranslate nohighlight">\(\btheta \in \Omega\)</span>. In other words, a <em>parametric model</em> is simply a family</p>
<div class="math notranslate nohighlight" id="equation-para-model-eqn">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-para-model-eqn" title="Permalink to this equation">#</a></span>\[
\calP_0 = \{P_\btheta : \btheta \in \Omega\}
\]</div>
<p>of probability distributions on <span class="math notranslate nohighlight">\(\bbr^k\)</span> such that <span class="math notranslate nohighlight">\(\bX \sim P_\btheta\)</span>. In this context, the set <span class="math notranslate nohighlight">\(\Omega\)</span> is called the <em>parameter space</em>, each <span class="math notranslate nohighlight">\(\btheta \in \Omega\)</span> is called a (<span class="math notranslate nohighlight">\(d\)</span>-dimensional) <em>parameter</em>, and the vector <span class="math notranslate nohighlight">\(\bX\)</span> is called the <em>data</em>.</p>
</section>
</div><p>Very often, we shall specify a parametric model <a class="reference internal" href="#equation-para-model-eqn">(13.2)</a> by listing the density functions of the probability measures <span class="math notranslate nohighlight">\(P_\btheta\)</span>, provided that these exist. In other words, we will write</p>
<div class="math notranslate nohighlight">
\[
\calP_0 = \{ p(\bx; \btheta): \btheta \in \Omega\}.
\]</div>
<p>The simplest examples of parametric models are the univariate models introduced and studied in <a class="reference internal" href="05-examples-of-rvs.html#examples"><span class="std std-numref">Chapter 5</span></a>. Indeed, if we have <span class="math notranslate nohighlight">\(X \sim \Ber(\theta)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\mathcal{P}_0 = \{ p(x;\theta) : \theta \in [0,1] \}, \quad p(x;\theta) = \theta^x (1-\theta)^{1-x}, \ x\in \{0,1\},
\]</div>
<p>is a parametric model with <span class="math notranslate nohighlight">\(1\)</span>-dimensional parameter space <span class="math notranslate nohighlight">\(\Omega = [0,1]\)</span>. Similarly, if we have <span class="math notranslate nohighlight">\(X\sim \calN(\mu,\sigma^2)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\mathcal{P}_0 = \{ p(x;\btheta) : \btheta = (\mu,\sigma^2) \in \bbr \times (0,\infty) \}, \quad p(x;\btheta) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp \left[ -\frac{1}{2\sigma^2} (x-\mu)^2\right],
\]</div>
<p>is a parametric model with <span class="math notranslate nohighlight">\(2\)</span>-dimensional parameter space <span class="math notranslate nohighlight">\(\Omega = \bbr \times (0,\infty)\)</span>.</p>
<p>As we mentioned above, all the fully-observed probabilistic graphical models studied in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a> are examples of this general type of parametric model. For example, a linear regression model with <span class="math notranslate nohighlight">\(n\)</span>-dimensional predictor vector <span class="math notranslate nohighlight">\(\bX\)</span> and response variable <span class="math notranslate nohighlight">\(Y\)</span> defines a parametric model</p>
<div class="math notranslate nohighlight">
\[
\calP_0 = \{ p(\bx, y; \btheta) : \btheta = (\beta_0,\bbeta,\sigma^2) \in \bbr \times \bbr^n \times (0,\infty)\}
\]</div>
<p>with <span class="math notranslate nohighlight">\((n+2)\)</span>-dimensional parameter space <span class="math notranslate nohighlight">\(\Omega = \bbr \times \bbr^n \times (0,\infty)\)</span> and where</p>
<div class="math notranslate nohighlight">
\[
p(\bx, y; \btheta) = p(y \mid \bx; \btheta) p(\bx) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp \left[ -\frac{1}{2\sigma^2} (y-\beta_0 - \bx^\intercal \bbeta)^2\right] p(\bx).
\]</div>
<p>In this latter example, notice that the <span class="math notranslate nohighlight">\((n+1)\)</span>-dimensional random vector <span class="math notranslate nohighlight">\((\bX,Y)\)</span> plays the roll of the vector <span class="math notranslate nohighlight">\(\bX\)</span> in <a class="reference internal" href="#para-model-def">Definition 13.5</a> (so that <span class="math notranslate nohighlight">\(k=n+1\)</span>).</p>
<p>The “plated” versions of the fully-observed PGMs in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a> also define parametric models in the sense of <a class="reference internal" href="#para-model-def">Definition 13.5</a>. For example, a “plated” version of our linear regression model from above would define the parametric model</p>
<div class="math notranslate nohighlight">
\[
\calP_0 = \{ p(\bx_1,\ldots,\bx_m,y_1,\ldots,y_m; \btheta) : \btheta = (\beta_0,\bbeta,\sigma^2) \in \Omega\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Omega\)</span> is the same parameter space as above and where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p(\bx_1,\ldots,\bx_m,y_1,\ldots,y_m; \btheta) &amp;= p(y_1,\ldots,y_m \mid \bx_1,\ldots,\bx_m) p(\bx_1,\ldots,\bx_m) \\
&amp;= \left\{\prod_{i=1}^m \frac{1}{\sqrt{2\pi \sigma^2}}\exp \left[ -\frac{1}{2\sigma^2} (y_i-\beta_0 - \bx_i^\intercal \bbeta)^2\right]\right\} p(\bx_1,\ldots,\bx_m).
\end{align*}\]</div>
<p>In this case, the <span class="math notranslate nohighlight">\((mn + m)\)</span>-dimensional random vector <span class="math notranslate nohighlight">\((\bX_1,\ldots,\bX_m,y_1,\ldots,y_m)\)</span> plays the roll of the vector <span class="math notranslate nohighlight">\(\bX\)</span> in <a class="reference internal" href="#para-model-def">Definition 13.5</a> (so that <span class="math notranslate nohighlight">\(k=mn+m\)</span>).</p>
</section>
<section id="parameter-estimators">
<h2><span class="section-number">13.3. </span>Parameter estimators<a class="headerlink" href="#parameter-estimators" title="Permalink to this heading">#</a></h2>
<div class="proof definition admonition" id="para-model-def">
<p class="admonition-title"><span class="caption-number">Definition 13.5 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\calP_0\)</span> be a parametric model for a <span class="math notranslate nohighlight">\(k\)</span>-dimensional random vector <span class="math notranslate nohighlight">\(\bX\)</span> with <span class="math notranslate nohighlight">\(d\)</span>-dimensional parameter space <span class="math notranslate nohighlight">\(\Omega\)</span>. A <em>parameter estimator</em> (or simply an <em>estimator</em>) is a statistic</p>
<div class="math notranslate nohighlight">
\[
\hatbtheta = \delta(\bX),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta: \bbr^k \to \bbr^d\)</span> is a vector-valued function. An observed value of <span class="math notranslate nohighlight">\(\hatbtheta\)</span> is called a <em>point estimate</em>.</p>
</section>
</div><p>There will be much abuse of terminology and notation regarding parameter estimators; it seems wise, then, to formally issue the following:</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ol class="arabic simple">
<li><p>Following our previously established convention of representing random objects with capital letters, we <em>should</em> write a parameter estimator as <span class="math notranslate nohighlight">\(\widehat{\boldsymbol \Theta}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol\Theta\)</span> is a capital theta. However, this is notationally awkward, so we will not do this.</p></li>
<li><p>Though technically the parameter estimator is the random vector <span class="math notranslate nohighlight">\(\hatbtheta\)</span>, we will use the word <em>estimator</em> to also refer to the function <span class="math notranslate nohighlight">\(\delta\)</span>. Moreover, we will often use the notations <span class="math notranslate nohighlight">\(\hatbtheta\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span> interchangeably.</p></li>
<li><p>To complicate things even more, we will sometimes write <span class="math notranslate nohighlight">\(\hatbtheta\)</span> to refer to a point estimate.</p></li>
</ol>
</div>
<p>Thus, the single piece of notation <span class="math notranslate nohighlight">\(\hatbtheta\)</span> might stand for one of three things: Either the random vector <span class="math notranslate nohighlight">\(\delta(\bX)\)</span>, the function <span class="math notranslate nohighlight">\(\delta\)</span>, or an observed value of the random vector <span class="math notranslate nohighlight">\(\delta(\bX)\)</span>. You will need to rely on context to determine which of these three objects is meant when you encounter the symbol <span class="math notranslate nohighlight">\(\hatbtheta\)</span>.</p>
</section>
<section id="blah">
<h2><span class="section-number">13.4. </span>Blah<a class="headerlink" href="#blah" title="Permalink to this heading">#</a></h2>
<p>So far, the only method that we have for estimating (or learning) the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span> of a probabilistic model is the method of <em>maximum likelihood estimation</em> that we studied in detail in <a class="reference internal" href="12-learning.html#learning"><span class="std std-numref">Chapter 12</span></a>. However, as we will see in the present chapter, there are <em>other</em> methods for parameter estimation. The general paradigm in which we shall study these methods begins with the following fundamental definition:</p>
<div class="proof definition admonition" id="param-est-def">
<p class="admonition-title"><span class="caption-number">Definition 13.6 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX\)</span> be a <span class="math notranslate nohighlight">\(k\)</span>-dimensional random vector whose probability distribution is parametrized by a <span class="math notranslate nohighlight">\(d\)</span>-dimensional parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. Then a <em>parameter estimator</em> (or simply an <em>estimator</em>) is a function</p>
<div class="math notranslate nohighlight">
\[
\delta: \bbr^k \to \bbr^d, \quad \bx \mapsto \delta(\bx).
\]</div>
<p>If we plug in the random vector <span class="math notranslate nohighlight">\(\bX\)</span>, we obtain a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector</p>
<div class="math notranslate nohighlight" id="equation-yup-eqn">
<span class="eqno">(13.3)<a class="headerlink" href="#equation-yup-eqn" title="Permalink to this equation">#</a></span>\[
\hatbtheta \ \def \ \delta( \bX ).
\]</div>
<p>An observed value of the random vector <span class="math notranslate nohighlight">\(\hatbtheta\)</span> is called a <em>parameter estimate</em>.</p>
</section>
</div><p>It will very often be the case that <span class="math notranslate nohighlight">\(\bX\)</span> is a random vector of the form</p>
<div class="math notranslate nohighlight">
\[
\bX = ( \bY^{(1)},\ldots,\bY^{(m)}),
\]</div>
<p>where each <span class="math notranslate nohighlight">\(\bY^{(i)}\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional random vector (so <span class="math notranslate nohighlight">\(k=mn\)</span>) that make up an IID random sample whose common distribution is parametrized by a parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. By independence, the probability function of <span class="math notranslate nohighlight">\(\bX\)</span> is then given by the product</p>
<div class="math notranslate nohighlight">
\[
p(\bx;\btheta) = \prod_{i=1}^m p(\by^{(i)}; \btheta), \quad \bx = (\by^{(1)},\ldots,\by^{(m)}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\by^{(i)};\btheta)\)</span> is the probability function of <span class="math notranslate nohighlight">\(\bY^{(i)}\)</span>.</p>
<p>Before continuing, I need to warn you of (many) upcoming abuses of notation and terminology:</p>
<p>With the warning properly issued, let’s move on.</p>
<p>First, we note that maximum likelihood estimation defines a parameter estimator denoted <span class="math notranslate nohighlight">\(\bthetaMLE\)</span>. Indeed, supposing that <span class="math notranslate nohighlight">\(\bx^{(1)},\ldots,\bx^{(m)}\)</span> is an observation of an IID random sample with data likelihood function</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\btheta;\bx^{(1)},\ldots,\bx^{(m)}) \def p(\bx^{(1)},\ldots,\bx^{(m)}; \btheta) = \prod_{i=1}^m p(\bx^{(i)};\btheta),
\]</div>
<p>we define</p>
<div class="math notranslate nohighlight">
\[
\bthetaMLE = \argmax_{\btheta\in \bbr^d} \mathcal{L}(\btheta;\bx^{(1)},\ldots,\bx^{(m)}),
\]</div>
<p>provided that a <em>unique</em> maximizing argument exists. The single acroynm <em>MLE</em> will stand either for the particular estimate <span class="math notranslate nohighlight">\(\bthetaMLE\)</span>, in which case it stands for <em>maximum likelihood estimate</em>, or it will stand for the obtained estimator (function), in which case it stands for <em>maximum likelihood estimator</em>. You will need to rely on context to clarify the intended meaning when this acroynm is encountered.</p>
<p>Notice that <a class="reference internal" href="#param-est-def">Definition 13.6</a> is <em>extremely</em> general. Even though we write <span class="math notranslate nohighlight">\(\hatbtheta = \delta(\bX)\)</span>, there really does not have to be any <em>a priori</em> connection between <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\btheta\)</span> for the former to be called an estimator of the latter. In fact, <span class="math notranslate nohighlight">\(\delta\)</span> might be a constant function which outputs a single vector for <em>all</em> choices of input; in all but very trivial cases, such an “estimator” does a terrible job of estimating the true value of the parameter.</p>
<p>An example will help clarify matters. Suppose that we have a simple univariate binomial model <span class="math notranslate nohighlight">\(X \sim \Bin(100,\theta)\)</span> for some unknown parameter <span class="math notranslate nohighlight">\(\theta \in (0,1)\)</span>. It is not difficult to show that the maximum likelihood estimate is given by <span class="math notranslate nohighlight">\(\thetaMLE = x/100\)</span>. However, the two formulas</p>
<div class="math notranslate nohighlight" id="equation-bin-map-eqn">
<span class="eqno">(13.4)<a class="headerlink" href="#equation-bin-map-eqn" title="Permalink to this equation">#</a></span>\[
\hattheta_1 = \frac{x + 5}{106} \quad \text{and} \quad \hattheta_2 = \frac{x + 1}{106}
\]</div>
<p>also define legitimate estimators for <span class="math notranslate nohighlight">\(\theta\)</span>, at least according to <a class="reference internal" href="#param-est-def">Definition 13.6</a>. They might seem to have been pulled out of the blue, but as we will see below, they are both examples of <em>maximum a posteriori</em> estimators and (as the name suggests) they were obtained as solutions to an optimization problem similar to the MLE.</p>
<p>An entire infinite universe full of potential parameter estimators calls for a method for <em>comparing</em> estimators. One such method goes through the following objects:</p>
<div class="proof definition admonition" id="loss-func-def">
<p class="admonition-title"><span class="caption-number">Definition 13.7 </span></p>
<section class="definition-content" id="proof-content">
<p>A <em>loss function</em> is a real-valued function</p>
<div class="math notranslate nohighlight">
\[
L: \bbr^d \times \bbr^d \to \bbr, \quad (\btheta,\ba) \mapsto L(\btheta, \ba).
\]</div>
</section>
</div><p>In our present context, loss functions serve as metrics for the quality of a parameter estimate. The vector <span class="math notranslate nohighlight">\(\ba\)</span> is conceptualized as an estimate of a parameter <span class="math notranslate nohighlight">\(\btheta\)</span>, and the value <span class="math notranslate nohighlight">\(L(\btheta,\ba)\)</span> is the “loss incurred” in the estimation. Very often we have <span class="math notranslate nohighlight">\(L(\btheta,\ba)\geq 0\)</span> for all inputs, and the smaller the loss the better the estimate.</p>
<p>In particular, we may plug an estimator <span class="math notranslate nohighlight">\(\delta\)</span> directly into a loss function to obtain values <span class="math notranslate nohighlight">\(L\big(\btheta, \delta(\bx)\big)\)</span>. This number is the loss incurred when we estimate the particular parameter value <span class="math notranslate nohighlight">\(\btheta\)</span> using <span class="math notranslate nohighlight">\(\delta\)</span> and the observed value <span class="math notranslate nohighlight">\(\bx\)</span>. However, this is only a “local” measure of the performance of the estimator <span class="math notranslate nohighlight">\(\delta\)</span> evaluated on just a single observation <span class="math notranslate nohighlight">\(\bx\)</span> and parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. In order to obtain a “global” measure of the performance of <span class="math notranslate nohighlight">\(\delta\)</span>, we may average over all observations of the dataset. This leads us to:</p>
<div class="proof definition admonition" id="freq-risk-def">
<p class="admonition-title"><span class="caption-number">Definition 13.8 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\btheta\)</span> be a parameter vector, <span class="math notranslate nohighlight">\(L\)</span> a loss function, and <span class="math notranslate nohighlight">\(\delta\)</span> a parameter estimator. Then the <em>risk</em> associated with <span class="math notranslate nohighlight">\(\delta\)</span> (relative to <span class="math notranslate nohighlight">\(L\)</span>) is given by</p>
<div class="math notranslate nohighlight">
\[
R(\btheta,\delta) \def E\big[ L\big(\btheta, \delta(\bX) \big)\big], 
\]</div>
<p>where the expectation is taken with respect to the probability function <span class="math notranslate nohighlight">\(p(\bx;\btheta)\)</span>.</p>
</section>
</div><p>So, we might hope to compare and rank parameter estimators by comparing their risks (with respect to some fixed loss function). But there’s a problem, because risks are technically <em>functions</em> of the parameter <span class="math notranslate nohighlight">\(\btheta\)</span>, and functions may not always be linearly ranked since their graphs might cross. For example, let’s return to our binomial model <span class="math notranslate nohighlight">\(X\sim \Bin(100,\theta)\)</span> and the three parameter estimators</p>
<div class="math notranslate nohighlight">
\[
\thetaMLE = \frac{x}{100}, \quad \hattheta_1 = \frac{x + 5}{106} \quad \text{and} \quad \hattheta_2 = \frac{x + 1}{106}.
\]</div>
<p>One very common loss function is the <em>squared error loss</em> given by</p>
<div class="math notranslate nohighlight">
\[
L:\bbr \times \bbr \to \bbr, \quad L(\theta,a) = (\theta-a)^2.
\]</div>
<p>With respect to this loss, the risks of the three estimators are given by</p>
<div class="math notranslate nohighlight">
\[
R(\theta,\thetaMLE) = \frac{\theta (1 - \theta)}{100},
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
R(\theta,\hattheta_1) = \frac{25 + 8 \theta (5 - 8 \theta)}{11236} \quad \text{and} \quad R(\theta,\hattheta_2) = \frac{1 + 8  \theta (11 - 8 \theta)}{11236}.
\]</div>
<p>If we plot these risks, we get:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">risk_mle</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span>

<span class="k">def</span> <span class="nf">risk_1</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">25</span> <span class="o">+</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">theta</span><span class="p">))</span> <span class="o">/</span> <span class="mi">11236</span>

<span class="k">def</span> <span class="nf">risk_2</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">11</span> <span class="o">-</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">theta</span><span class="p">))</span> <span class="o">/</span> <span class="mi">11236</span>

<span class="n">grid</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">risk_mle</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$R(</span><span class="se">\\</span><span class="s1">theta,\widehat{</span><span class="se">\\</span><span class="s1">theta}_{</span><span class="se">\\</span><span class="s1">text</span><span class="si">{mle}</span><span class="s1">})$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">risk_1</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$R(</span><span class="se">\\</span><span class="s1">theta,\widehat{</span><span class="se">\\</span><span class="s1">theta}_1)$&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashdot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">risk_2</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$R(</span><span class="se">\\</span><span class="s1">theta,\widehat{</span><span class="se">\\</span><span class="s1">theta}_2)$&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;risk&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/61ff53c50269c234e8f4d38bcf523e9882f12ba449ae4ef13de316e6ebff84b5.svg" src="../_images/61ff53c50269c234e8f4d38bcf523e9882f12ba449ae4ef13de316e6ebff84b5.svg" /></figure>
</div>
</div>
<p>Notice that there is no way to linearly rank these risks. For any one of them, there are certain parameter values where that risk is smaller than the others, while for other parameter values, it is larger. In particular:</p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(\theta=0.5\)</span>, the MLE is less risky (on average) compared to the other two.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(\theta=0.75\)</span> the estimator <span class="math notranslate nohighlight">\(\hattheta_1\)</span> is less risky (on average) than the other two.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(\theta=0.25\)</span> the estimator <span class="math notranslate nohighlight">\(\hattheta_2\)</span> is less risky (on average) than the other two.</p></li>
</ul>
<p>The squared error loss is one of the most common loss functions; two other common losses are called the <em>absolute error loss</em> and <em>0/1 loss</em>. These are all defined officially in:</p>
<div class="proof definition admonition" id="loss-example-def">
<p class="admonition-title"><span class="caption-number">Definition 13.9 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\delta\)</span> be a parameter estimator.</p>
<ol class="arabic">
<li><p>The loss function</p>
<div class="math notranslate nohighlight">
\[
    L:\bbr \times \bbr \to \bbr, \quad L(\theta,a) = (\theta-a)^2,
    \]</div>
<p>is called the <em>squared error loss</em>. The risk of <span class="math notranslate nohighlight">\(\delta\)</span> associated with this loss is called the <em>mean squared error</em> and is denoted <span class="math notranslate nohighlight">\(\MSE(\delta)\)</span>.</p>
</li>
<li><p>The loss function</p>
<div class="math notranslate nohighlight">
\[
    L:\bbr \times \bbr \to \bbr, \quad L(\theta,a) = |\theta-a|,
    \]</div>
<p>is called the <em>absolute error loss</em>. The risk of <span class="math notranslate nohighlight">\(\delta\)</span> associated with this loss is called the <em>mean absolute error</em> and is denoted <span class="math notranslate nohighlight">\(\MAE(\delta)\)</span>.</p>
</li>
<li><p>The loss function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    L:\bbr^d \times \bbr^d \to \bbr, \quad L(\btheta,\ba) = \begin{cases} 0 &amp; : \btheta = \ba, \\ 1  &amp; : \btheta \neq \ba, \end{cases}
    \end{split}\]</div>
<p>is called the <em>0/1 loss</em>.</p>
</li>
</ol>
</section>
</div><p>Observe that the notations <span class="math notranslate nohighlight">\(\MSE(\delta)\)</span> and <span class="math notranslate nohighlight">\(\MAE(\delta)\)</span>, while descriptive, hide the fact that both of these risks are functions of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Another example of “crossing risks” is given in:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 1 on the worksheet.</p>
</div>
</section>
<section id="bayesian-models">
<h2><span class="section-number">13.5. </span>Bayesian models<a class="headerlink" href="#bayesian-models" title="Permalink to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>If the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span> is random, then following our previously established convention, we should write it as a capital Greek theta <span class="math notranslate nohighlight">\(\boldsymbol\Theta\)</span>. However, we will not do this.</p>
</aside>
<p>In some sense, the parameter estimation and comparison scheme described in the previous section does not align well with real world practice. For example, in computing the risk <span class="math notranslate nohighlight">\(R(\btheta,\delta)\)</span> of an estimator <span class="math notranslate nohighlight">\(\delta\)</span>, we average over all possible observations of the dataset. But in real life, the dataset is often <em>fixed</em>, and we may not be particularly concerned with the performance of our estimator on datasets other than the one in our hands. An alternate way to approach parameter estimation and comparison conceptualizes the dataset as fixed, while taking the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span> as a random vector itself. This latter conceptual scheme is called <em>Bayesian modeling</em>.</p>
<p>The Bayesian scheme also allows parameter estimation methods that produce more than just simple numerical estimates, the latter often called <em>point estimates</em> in this context. Indeed, Bayesian parameter estimation produces an entire probability distribution for each parameter, called the <em>posterior distribution</em> of the parameter, which are conditioned on the observed dataset. From these latter distributions, we may extract point estimates such as the (posterior) means, medians, and modes. However, it is really the full posterior distributions that are of most interest.</p>
<p>In order to obtain these posterior distributions, which are obtained as the <em>end result</em> of Bayesian parameter estimation, we need to <em>begin</em> with initial distributions on the parameters called <em>prior distributions</em>. The work that goes into cooking up the prior distributions is the price we pay to use Bayesian modeling. The prior distributions reflect our prior knowledge (hence the name) of the parameters heading into the problem, and entire chapters in textbooks devoted to Bayesian statistics concern best practices for selecting good prior distributions. We will only barely mention some of these ideas. In any case, no matter how the prior distributions are chosen, the mathematical mechanism that carries us from the prior to the posterior distributions via the dataset is exactly Bayes’ Theorem.</p>
<p>Mathematically, the transition from the probabilistic graphical models described in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a> to Bayesian ones is incredibly easy: All you do is treat each parameter as an unobserved random variable or vector. For example, remember that the underlying graph of a linear regression model is given by</p>
<a class="reference internal image-reference" href="../_images/lin-reg-00.svg"><img alt="../_images/lin-reg-00.svg" class="align-center" src="../_images/lin-reg-00.svg" width="50%" /></a>
<p> </p>
<p>To turn this into a Bayesian linear regression model, we simply do this:</p>
<a class="reference internal image-reference" href="../_images/lin-reg-bayes.svg"><img alt="../_images/lin-reg-bayes.svg" class="align-center" src="../_images/lin-reg-bayes.svg" width="60%" /></a>
<p> </p>
<p>For our very simple univariate binomial model <span class="math notranslate nohighlight">\(X\sim \Bin(100, \theta)\)</span> in the previous section, we would consider the following graph:</p>
<a class="reference internal image-reference" href="../_images/bern-bayes.svg"><img alt="../_images/bern-bayes.svg" class="align-center" src="../_images/bern-bayes.svg" width="35%" /></a>
<p> </p>
<p>In this latter example, we might suppose that <span class="math notranslate nohighlight">\(X\)</span> counts the number of heads in <span class="math notranslate nohighlight">\(100\)</span> flips of a coin that shows heads with probability <span class="math notranslate nohighlight">\(\theta\)</span>. We might even imagine that we are in the scenario described in <a class="reference internal" href="07-random-vectors.html#untrustworthy"><span class="std std-numref">Section 7.10</span></a>, where we are playing a coin flipping game with an untrustworthy friend: If the coin shows heads, our friend wins; otherwise, we win. Our suspicion that our friend is untrustworthy and that the coin prefers heads with probability <span class="math notranslate nohighlight">\(\theta&gt;0.5\)</span> could be encoded by using the prior distribution</p>
<div class="math notranslate nohighlight">
\[
\theta \sim \Beta(6,2),
\]</div>
<p>so that the prior mean of <span class="math notranslate nohighlight">\(\theta\)</span> is <span class="math notranslate nohighlight">\(0.75\)</span>. Carrying through the same computations in <a class="reference internal" href="07-random-vectors.html#untrustworthy"><span class="std std-numref">Section 7.10</span></a> using Bayes’ Theorem, we see that the <em>posterior distribution</em> of the parameter has probability function <span class="math notranslate nohighlight">\(p(\theta| x)\)</span> given (on its support) by</p>
<div class="math notranslate nohighlight">
\[
p(\theta | x) = \frac{p(x|\theta) p(\theta)}{p(x)} \propto \theta^{x + 5} (1-\theta)^{101 - x}.
\]</div>
<p>In particular, the posterior distribution is <span class="math notranslate nohighlight">\(\Beta(x + 6, 102 - x)\)</span>. If we want to obtain a point estimate for the parameter from the posterior distribution, we may take the posterior mode (global maximizer) which is easily seen to be</p>
<div class="math notranslate nohighlight">
\[
\thetaMAP \def \argmax_{\theta \in \bbr} p(\theta | x) =  \frac{x + 5}{106}.
\]</div>
<p>Note that this is exactly the point estimate <span class="math notranslate nohighlight">\(\hattheta_1\)</span> described above in <a class="reference internal" href="#equation-bin-map-eqn">(13.4)</a>. As we mentioned there, this point estimate is called a <em>maximum a posteriori</em> (<em>MAP</em>) estimate.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 2 on the worksheet.</p>
</div>
</section>
<section id="bayes-estimators">
<h2><span class="section-number">13.6. </span>Bayes estimators<a class="headerlink" href="#bayes-estimators" title="Permalink to this heading">#</a></h2>
<p>In the Bayesian framework, a new type of risk becomes the central object of interest:</p>
<div class="proof definition admonition" id="bayes-risk-def">
<p class="admonition-title"><span class="caption-number">Definition 13.10 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\btheta\)</span> be a random parameter vector, <span class="math notranslate nohighlight">\(L\)</span> be a loss function, and <span class="math notranslate nohighlight">\(\delta\)</span> a parameter estimator. Then the <em>posterior risk</em> associated with <span class="math notranslate nohighlight">\(\delta\)</span> (relative to <span class="math notranslate nohighlight">\(L\)</span>) is given by</p>
<div class="math notranslate nohighlight">
\[
R(\btheta,\delta \mid \bX = \bx ) \def E\big[ L\big(\btheta, \delta(\bx) \big) \mid \bX = \bx\big], 
\]</div>
<p>where the expectation is taken with respect to the posterior probability function <span class="math notranslate nohighlight">\(p(\btheta | \bx)\)</span>.</p>
</section>
</div><p>In contrast to the risk <span class="math notranslate nohighlight">\(R(\btheta,\delta)\)</span>, which is a function of <span class="math notranslate nohighlight">\(\btheta\)</span>, notice that the posterior risk is a function of <span class="math notranslate nohighlight">\(\bx\)</span> where the dependence on <span class="math notranslate nohighlight">\(\btheta\)</span> has been averaged out. This means that for a fixed dataset, the posterior risks associated with different estimators are real numbers instead of functions, and thus may be linearly ranked. In principle, then, we can always choose the optimal estimator in the Bayesian framework simply by choosing the one with the smallest posterior risk. These optimal estimators have names:</p>
<div class="proof definition admonition" id="bayes-estimator-def">
<p class="admonition-title"><span class="caption-number">Definition 13.11 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\btheta\)</span> be a random parameter vector and <span class="math notranslate nohighlight">\(L\)</span> a loss function. The function</p>
<div class="math notranslate nohighlight">
\[
\delta^\ast(\bx) \def \argmin_{\ba \in \bbr^d}  E\big[ L(\btheta, \ba ) \mid \bX = \bx \big]
\]</div>
<p>is called a <em>Bayes estimator</em> of <span class="math notranslate nohighlight">\(\btheta\)</span>, where the expectation is taken with respect to the posterior probability function <span class="math notranslate nohighlight">\(p(\btheta | \bx)\)</span>.</p>
</section>
</div><p>For certain loss functions, the Bayes estimators have particularly nice descriptions in terms of summary statistics of the posterior distributions.</p>
<div class="proof theorem admonition" id="bayes-estimator-example-thm">
<p class="admonition-title"><span class="caption-number">Theorem 13.2 </span> (Common Bayes estimators)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\btheta\)</span> be a random parameter.</p>
<ol class="arabic">
<li><p>With respect to the squared error loss, the Bayes estimator is the posterior mean, given explicitly as the conditional expectation</p>
<div class="math notranslate nohighlight">
\[
    \delta^\ast(\bx) = E(\theta \mid \bX = \bx).
    \]</div>
</li>
<li><p>With respect to the absolute error loss, the Bayes estimator is a posterior median.</p></li>
<li><p>With respect to the 0/1 loss, the Bayes estimator is a posterior mode, i.e., a <em>maximum a posterior</em> estimator <span class="math notranslate nohighlight">\(\thetaMAP\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. For simplicity, let’s suppose for all three statements that the posterior distribution is discrete, so that we must minimize the objective function</p>
<div class="math notranslate nohighlight">
\[
J(a) \def E\big[ L(\theta, a ) \mid \bX = \bx \big] = \sum_{\theta \in \bbr} L(\theta,a) p(\theta | \bx).
\]</div>
<p>Wherever the loss function is differentiable with respect to <span class="math notranslate nohighlight">\(a\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
J'(a) = \sum_{\theta \in \bbr} \frac{\partial}{\partial a}\big(L(\theta,a)\big) p(\theta | \bx).
\]</div>
<p>In particular, if <span class="math notranslate nohighlight">\(L(\theta,a) = (\theta-a)^2\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
J'(a) = -2 E(\theta \mid \bX = \bx) +2a.
\]</div>
<p>Setting this to <span class="math notranslate nohighlight">\(0\)</span> and solving for <span class="math notranslate nohighlight">\(a\)</span> gives the global minimizer <span class="math notranslate nohighlight">\(a = E(\theta \mid \bX = \bx)\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(L\)</span> is the 0/1 loss, then we have</p>
<div class="math notranslate nohighlight">
\[
J(a) = \sum_{\theta\neq a} p(\theta | \bx).
\]</div>
<p>If <span class="math notranslate nohighlight">\(a^\star\)</span> is a posterior mode so that</p>
<div class="math notranslate nohighlight">
\[
a^\star = \argmax_{a\in \bbr} p(a | \bx),
\]</div>
<p>then we will have <span class="math notranslate nohighlight">\(J(a^\star) \leq J(a)\)</span> for all <span class="math notranslate nohighlight">\(a\in \bbr\)</span> if and only if <span class="math notranslate nohighlight">\(p(a|\bx) \leq p(a^\star|\bx)\)</span>. But this last inequality holds by our choice of <span class="math notranslate nohighlight">\(a^\star\)</span>.</p>
<p>Finally, if <span class="math notranslate nohighlight">\(L\)</span> is the absolute error loss, then you may refer to Theorem 4.5.3 in <span id="id1">[<a class="reference internal" href="bib.html#id7" title="M. H. DeGroot and M. J. Schervish. Probability and statistics. Volume 563. Pearson Education London, UK, 2014.">DS14</a>]</span> for a proof. Q.E.D.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="12-learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="14-asymptotic.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Large sample theory and more sampling distributions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics">13.1. Statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-parametric-models">13.2. General parametric models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimators">13.3. Parameter estimators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#blah">13.4. Blah</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-models">13.5. Bayesian models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-estimators">13.6. Bayes estimators</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>