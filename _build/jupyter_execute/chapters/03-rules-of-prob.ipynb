{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6872aafd",
   "metadata": {},
   "source": [
    "(rules-prob)=\n",
    "# Rules of probability\n",
    "\n",
    "Having learned the basic theory of probability in the previous chapter, in this chapter we begin developing various tools and techniques that will allow us to actually _compute_ probabilities.\n",
    "\n",
    "The first two sections in this chapter deal with techniques for _counting_, and are more widely applicable than just computing probabilities. Indeed, you might have seen some of this material in a course in discrete mathematics or combinatorics. We will treat this material with a much lighter touch than most elementary probability textbooks, believing that these techniques are not _as important_ as many those texts might lead you to believe.\n",
    "\n",
    "The rest of the chapter is devoted to probability theory in which, among other things, we introduce the fundamental topics of _conditioning_ and _independence_, as well as the _Law of Total Probability_ and _Bayes' Theorem_. All four of these topics are absolutely crucial to the rest of the book, and we will return to them again in the context of random variables in a [later](random-vectors) chapter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Product and Sum Rules for Counting\n",
    "\n",
    "The main rules that we will use for counting are:\n",
    "\n",
    "```{prf:theorem} The Product Rule for Counting\n",
    ":label: product-rule-counting-thm\n",
    "\n",
    "Suppose that a procedure can be broken down into a sequence of two tasks. If there are $m$ ways to do the first task, and for each of these ways of doing the first task there are $n$ ways to do the second task, then there are $mn$ total ways to do the procedure.\n",
    "```\n",
    "\n",
    "```{prf:theorem} The Sum Rule for Counting\n",
    ":label: sum-rule-counting-thm\n",
    "\n",
    "If a procedure can be done either in one of $m$ ways or in one of $n$ ways, where none of the set of $m$ ways is the same as any of the set of $n$ ways, then there are $m+n$ ways to do the procedure.\n",
    "```\n",
    "\n",
    "Let's get some practice:\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Do problem 1 on the worksheet.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Permutations and combinations\n",
    "\n",
    "The Product Rule for Counting tells us how to count the number of ways to accomplish a task that can be broken down into a sequence of smaller, sub-tasks. One very simple (yet very important) example of such a task is choosing, in order, a collection of objects from a larger collection of objects. Such collections have names:\n",
    "\n",
    "```{prf:definition}\n",
    ":label: permutation-def\n",
    "\n",
    "An ordered collection of $k$ distinct objects is called a _permutation_ of those objects. The number of permutations of $k$ objects selected from a collection of $n$ objects will be designated by the symbol $P^n_k$.\n",
    "```\n",
    "\n",
    "Mathematically, a permutation consisting of $k$ objects is often represented as $k$-tuple. This is because, for a permutation, the ordering of the objects **matters**. But order matters in $k$-tuples as well; for example, the $2$-tuple $(1,2)$ is _not_ the same as the $2$-tuple $(2,1)$.\n",
    "\n",
    "\n",
    "```{image} ../img/permutation.svg\n",
    ":width: 75%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "We will often need to count permutations:\n",
    "\n",
    "```{prf:theorem} Formula for Counting Permutations\n",
    ":label: counting-perm-thm\n",
    "\n",
    "For $0 \\leq k \\leq n$, we have\n",
    "\n",
    "$$P^n_k = \\frac{n!}{(n-k)!}.$$\n",
    "\n",
    "_Note_: Remember the convention that $0! = 1$.\n",
    "```\n",
    "\n",
    "Why is this true? Remember that a permutation of $k$ objects may be represented as a $k$-tuple:\n",
    "\n",
    "$$(\\text{object $1$},\\text{object $2$},\\ldots, \\text{object $k$}).$$\n",
    "\n",
    "But then, written like this, we see that counting the number of permutations is a simple application of the Product Rule for Counting. Indeed, the number of permutations is the product\n",
    "\n",
    "````{div} full-width\n",
    "$$(\\text{# ways to choose object $1$}) \\times (\\text{# ways to choose object $2$}) \\times \\cdots \\times (\\text{# ways to choose object $k$}),$$\n",
    "````\n",
    "\n",
    "which is equal to\n",
    "\n",
    "$$n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-k+1) = \\frac{n!}{(n-k)!}.$$\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Do problem 2 on the worksheet.\n",
    "```\n",
    "\n",
    "The partners to permutations are defined in:\n",
    "\n",
    "```{prf:definition}\n",
    ":label: combination-def\n",
    "\n",
    "An unordered collection of $k$ distinct objects is called a _combination_ of those objects. The number of combinations of $k$ objects selected from a collection of $n$ objects will be designated by the symbol $C^n_k$ or $\\binom{n}{k}$.\n",
    "```\n",
    "\n",
    "Permutations are represented as $k$-tuples because order matters. But for combinations, order does **not** matter, and therefore they are often represented as sets:\n",
    "\n",
    "```{image} ../img/combination.svg\n",
    ":width: 75%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "```{prf:theorem} Formula for Counting Combinations\n",
    ":label: counting-combo-thm\n",
    "\n",
    "For $0 \\leq k \\leq n$, we have\n",
    "\n",
    "$$\\binom{n}{k} = C^n_k = \\frac{n!}{(n-k)!k!}.$$\n",
    "\n",
    "_Note_: Remember the convention that $0! = 1$.\n",
    "```\n",
    "\n",
    "To see why this formula works, think of the process of forming a *permutation* as a two-step process:\n",
    "\n",
    "1. Choose a combination.\n",
    "2. Rearrange the objects in your chosen combination to fit the order of your desired permutation.\n",
    "\n",
    "Then, it follows from the Product Rule for Counting that the total number of permutations, $P^n_k$, is the product of the number of ways to accomplish the first task with the number of ways to accomplish the second task. In an equation, this means:\n",
    "\n",
    "$$P^n_k = C^n_k \\cdot P^k_k.$$\n",
    "\n",
    "Indeed, on the right-hand side of this equation you see $C^n_k$, which is the number of ways to accomplish the first task, while $P^k_k$ represents the number of permutations of $k$ objects selected from a total number of $k$ objects (which are just rearrangements of those objects). But we have\n",
    "\n",
    "$$P^n_k = \\frac{n!}{(n-k)!} \\quad \\text{and} \\quad P^k_k = k!,$$\n",
    "\n",
    "which means that we must have\n",
    "\n",
    "$$C^n_k = \\frac{ n!}{(n-k)!k!}.$$\n",
    "\n",
    "But this is exactly the desired formula for $C^n_k$!\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Do problem 3 on the worksheet.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Sum Rule for Probability\n",
    "\n",
    "We now transition back to talking about probability theory.\n",
    "\n",
    "\n",
    "```{prf:theorem} The Sum Rule for Probability\n",
    ":label: sum-rule-prob-thm\n",
    "\n",
    "The probability of the union of two events $A$ and $B$ is\n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B) - P(A\\cap B).$$\n",
    "\n",
    "In particular, if $A$ and $B$ are disjoint, then\n",
    "\n",
    "$$P(A\\cup B) = P(A) + P(B).$$\n",
    "```\n",
    "\n",
    "So, the Sum Rule for Probability tells you how to compute the probability of a *union* of two events. We will see [later](prod-rule-prob) a different rule that tells you how to compute the probability of an *intersection*.\n",
    "\n",
    "In the very special case that the sample space $S$ is finite and the probability measure $P$ is uniform, I claim that the Sum Rule for Probability is \"obvious.\"\n",
    "\n",
    "Let me explain.\n",
    "\n",
    "Notice that we have\n",
    "\n",
    "$$|A\\cup B| = |A| + |B| - |A\\cap B|,$$(inc-ex-eqn)\n",
    "\n",
    "where the vertical bars denote cardinalities. Can you see why this is true? If you want to count the number of points in the union $A\\cup B$, then you count all the points in $A$, then all the points in $B$, but you have to subtract out the points in $A\\cap B$ because they would be double counted (this is called [inclusion-exclusion](https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle)). \n",
    "\n",
    "Now, if we divide both sides of {eq}`inc-ex-eqn` by the cardinality $|S|$, we get\n",
    "\n",
    "$$\\frac{|A\\cup B|}{|S|} = \\frac{|A|}{|S|} + \\frac{|B|}{|S|} - \\frac{|A\\cap B|}{|S|}.$$\n",
    "\n",
    "But because we are working with a uniform probability measure $P$, each of these four ratios _are_ the probabilities:\n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B) - P(A\\cap B).$$\n",
    "\n",
    "Simple!\n",
    "\n",
    "But what if $P$ is _not_ uniform? Why is the Sum Rule still true?\n",
    "\n",
    "To explain, imagine the usual Venn diagram with the events $A$ and $B$ inside the sample space S:\n",
    "\n",
    "```{image} ../img/eventssum.svg\n",
    ":width: 25%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "The argument that I am about to give depends crucially on the two equations\n",
    "\n",
    "$$A\\cup B = A \\cup ( A^c \\cap B) \\quad \\text{and} \\quad B = (A^c \\cap B) \\cup ( A\\cap B),$$(fund-eqn)\n",
    "\n",
    "where for brevity I've written $A^c$ for the complement $S\\smallsetminus A$. If you're not quite sure why these equations are true, here's a visual explanation using Venn diagrams:\n",
    "\n",
    "```{image} ../img/sumrule.svg\n",
    ":width: 70%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "The first row of this figure simply shows you how to visualize the intersection $A^c \\cap B$, and the second two rows explain the fundamental equations {eq}`fund-eqn` above. The crucial point to notice is that the two pairs of sets on the right-hand sides of the equations {eq}`fund-eqn` are _disjoint_, which you can see easily from the Venn diagrams. Thus, if we apply $P$ to both sides of these equations, we get:\n",
    "\n",
    "$$P(A\\cup B) = P(A) + P(A^c\\cap B) \\quad \\text{and} \\quad P(B) = P(A^c \\cap B) +P(A\\cap B).$$\n",
    "\n",
    "Now subtract these equations to get\n",
    "\n",
    "$$P(A\\cup B) - P(B) = P(A) + P(A^c\\cap B) - (P(A^c \\cap B) + P(A\\cap B)),$$\n",
    "\n",
    "which we may rearrange to get the Sum Rule:\n",
    "\n",
    "$$P(A\\cup B) = P(A) + P(B) - P(A\\cap B).$$\n",
    "\n",
    "And there we have it! This explains why the Sum Rule works.\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Have a go at problems 4 and 5 in the worksheet.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(cond-prob)=\n",
    "## Conditional probability\n",
    "\n",
    "So far, we have studied just plain probabilities $P(A)$. Intuitively, this value represents the probability that you expect the event $A$ to occur, _in the absence of any additional given information_. But what if you actually _had_ additional information that might affect the probability of the event occurring? How would you update the probability $P(A)$?\n",
    "\n",
    "Let's do an example. Suppose that you roll two fair, six-sided dice. The sample space $S$ in this scenario is the collection of all ordered pairs $(\\text{roll 1}, \\text{roll 2})$. Here's a picture:\n",
    "\n",
    "```{image} ../img/conditionalSS.svg\n",
    ":width: 45%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "Since there are six possibilities for each roll, according to the Product Rule for Counting, the cardinality of the sample space is $6^2 = 36$. And, since each die is fair, any number is equally likely to be rolled as any other, and hence the probability measure is uniform: Each possible pair of numbers has probability $1/36$ of being rolled.\n",
    "\n",
    "Now, suppose I ask you to compute the probability of the event $A$ that your two rolls together sum to 6. This computation is pretty easy, since you can just count the possibilities by hand:\n",
    "\n",
    "$$(5,1), (4,2), (3,3), (2,4), (1,5). $$(who-eqn)\n",
    "\n",
    "We may visualize the situation like this:\n",
    "\n",
    "```{image} ../img/conditionalrolls.svg\n",
    ":width: 85%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "The figure on the left shows all sums for all possible pair of rolls in the sample space $S$, while the figure on the right shows the event $A$. Since the probability is uniform, we have\n",
    "\n",
    "$$P(A) = \\frac{5}{36} \\approx 0.139.$$\n",
    "\n",
    "However, what if I _now_ tell you that your first roll is an odd number---does this affect your expected probability for $A$?\n",
    "\n",
    "_It does_!\n",
    "\n",
    "To see why, let's first give the event that the first roll is odd a name, say $B$. Then we're not interested in the _full_ event $A$ anymore, since it includes the rolls $(4,2)$ and $(2,4)$. Rather, we are _now_ interested in the intersection $A\\cap B$:\n",
    "\n",
    "```{image} ../img/conditionalrollsInt.svg\n",
    ":width: 85%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "The intersection $A\\cap B$ is where the blue horizontal highlights (representing $B$) intersect the magenta diagonal highlight (representating $A$). But there are three pairs of rolls in this intersection, and the cardinality of $B$ is $18$, so our probability of $A$ occuring changes from $P(A) = 5/36$, to $3/18$. But notice that $3/18$ is the _same_ as the ratio\n",
    "\n",
    "$$\\frac{P(A\\cap B)}{P(B)}.$$\n",
    "\n",
    "These considerations suggest the following fundamental definition:\n",
    "\n",
    "```{prf:definition}\n",
    ":label: condition-event-def\n",
    "\n",
    "Let $A$ and $B$ be two events in a probability space $S$. Then the _conditional probability of $A$ given $B$_ is the ratio\n",
    "\n",
    "$$P(A|B) \\stackrel{\\text{def}}{=} \\frac{P(A\\cap B)}{P(B)},$$\n",
    "\n",
    "provided that $P(B) >0$. The event $B$ is called the _conditioning event_, and the conditional probability $P(A|B)$ is often called a probability _conditioned on $B$_.\n",
    "```\n",
    "\n",
    "In plain language, the conditional probability $P(A|B)$ represents the probability that $A$ will occur, _provided that you already know that event $B$ has occurred_. So, in our die-rolling scenario above where $A$ is the event that the sum of the two rolls is $6$, and $B$ is the event that the first roll is odd, we would write\n",
    "\n",
    "$$ P(A|B) = \\frac{3}{18}.$$\n",
    "\n",
    "Conceptually, a conditional probability $P(A|B)$ may be thought of as collapsing the full probability space $S$ down to the event $B$. In our dice-roll scenario above, this collapse looks like:\n",
    "\n",
    "```{image} ../img/collapse.svg\n",
    ":width: 95%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "\n",
    "Do problems 6 and 7 on the worksheet.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(independence-first)=\n",
    "## Independence\n",
    "\n",
    "Sometimes the outcomes of two experiments or processes do not affect each other. For example, if I flip a single coin twice, then we would expect that the outcome of the first flip should have no effect on the outcome of the second flip. Such events are called _independent_, and they are defined formally in:\n",
    "\n",
    "```{prf:definition}\n",
    ":label: independent-event-def\n",
    "\n",
    "Two events $A$ and $B$ in a sample space are _independent_ if\n",
    "\n",
    "$$P(A\\cap B) = P(A)P(B).$$ (ind2-eqn)\n",
    "```\n",
    "\n",
    "The definition of independence technically only applies to two events, but there's an obvious generalization of the definition to an arbitrary number of events. Indeed, we say that events $A_1,A_2,\\ldots,A_n$ are _independent_ if\n",
    "\n",
    "$$P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1)P(A_2) \\cdots P(A_n).$$\n",
    "\n",
    "Here's a test for independence that involves conditional probabilities:\n",
    "\n",
    "```{prf:theorem} Conditional Criterion for Independence\n",
    ":label: conditional-ind-thm\n",
    "\n",
    "Two events $A$ and $B$ with nonzero probability are independent if and only if\n",
    "\n",
    "$$P(A|B) = P(A) \\quad \\text{or} \\quad P(B|A) = P(B).$$(ind-eqn)\n",
    "```\n",
    "\n",
    "Do you see why this criterion follows immediately from the definition {eq}`ind2-eqn` of independence and the definition of conditional probability? See if you can work it out for yourself.\n",
    "\n",
    "Think about what the first equation in {eq}`ind-eqn` is attempting to tell you: It says that the probability that $A$ occurs, _given that $B$ has occurred_, is just the original probability $P(A)$. This means that whether $B$ has occurred or not has _no_ effect on whether $A$ occurs. This captures precisely the intuitive idea that the formal definition of independence {eq}`ind2-eqn` is attempting to get at.\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Do problems 8 and 9 on the worksheet.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(prod-rule-prob)=\n",
    "## The Product Rule for Probability\n",
    "\n",
    "Just as the Sum Rule for Probability expresses the probability of a union $A\\cup B$ of two events as a sum of probabilities, the next rule expresses the probability of an intersection $A\\cap B$ as a product of probabilities:\n",
    "\n",
    "```{prf:theorem} The Product Rule for Probability\n",
    ":label: product-rule-prob-thm\n",
    "\n",
    "The probability of the intersection of two events $A$ and $B$ is\n",
    "\n",
    "$$P(A \\cap B) = P(A|B) P(B) = P(B|A)P(A).$$ (product-eqn)\n",
    "```\n",
    "\n",
    "Notice that the equations in {eq}`product-eqn` are immediate consequences of the definition of conditional probability, so there's not really any mystery for why they hold.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(total-prob-bayes)=\n",
    "## The Law of Total Probability and Bayes' Theorem\n",
    "\n",
    "A *partition* of a sample space $S$ is exactly what it sounds like: It's a division of the sample space into a collection of disjoint subsets. For example, the six subsets $B_1,B_2,\\ldots,B_6$ of the sample space $S$ in the following Venn diagram form a partition:\n",
    "\n",
    "```{image} ../img/partition.svg\n",
    ":width: 35%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "There are two things required of a partition: All the sets in the partition must be pairwise disjoint, and their union must be the entire sample space $S$.\n",
    "\n",
    "Using the concept of a partition, we may state the following law of probability:\n",
    "\n",
    "```{prf:theorem} The Law of Total Probability\n",
    ":label: law-of-total-prob-event-thm\n",
    "\n",
    "Suppose that $\\{B_1,B_2,\\ldots,B_n\\}$ is a partition of a sample space $S$, where each $B_k$ is an event. Then for any event $A$, we have\n",
    "\n",
    "$$P(A) = \\sum_{k=1}^n P(A|B_k) P(B_k).$$\n",
    "```\n",
    "\n",
    "Why is this law true? Notice first that\n",
    "\n",
    "$$A = \\bigcup_{k=1}^n ( A\\cap B_k),$$\n",
    "\n",
    "and that the sets in the union on the right-hand side are pairwise disjoint. (Why?) Therefore, if we apply the probability measure $P$ to both sides of this last equation, we get\n",
    "\n",
    "$$P(A) = \\sum_{k=1}^n P(A\\cap B_k).$$\n",
    "\n",
    "But $P(A\\cap B_k) = P(A|B_k) P(B_k)$ by the Product Rule for Probability, and hence we get\n",
    "\n",
    "$$P(A) = \\sum_{k=1}^n P(A|B_k) P(B_k),$$\n",
    "\n",
    "which is what we wanted to prove.\n",
    "\n",
    "Often times, the Law of Total Probability is applied when only *two* events are involved. In this case, the formula simplifies considerably:\n",
    "\n",
    "```{prf:theorem} The Law of Total Probability (Two-Event Version)\n",
    ":label: law-of-total-prob-two-event-thm\n",
    "\n",
    "Let $A$ and $B$ be two events in a sample space $S$. Then\n",
    "\n",
    "$$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c),$$\n",
    "\n",
    "where I've written $B^c$ for $S \\smallsetminus B $.\n",
    "```\n",
    "\n",
    "Indeed, this version of the Law of Total Probability is the special case of the first where the partition is $\\{B,B^c\\}$. Check this!\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Do problem 10 on the worksheet.\n",
    "```\n",
    "\n",
    "Finally, to motivate the last result in this chapter, let's consider the following real-world scenario: \n",
    "\n",
    "```{admonition} The Canonical Example of Bayes' Theorem\n",
    "Suppose that a test has been devised to detect a certain disease. Moreover, suppose that:\n",
    "\n",
    "* The disease affects $0.1\\%$ of the population.\n",
    "* The test does not produce any false negatives.\n",
    "* The test produces false positives at a rate of $5\\%$.\n",
    "\n",
    "Given that a randomly selected individual tests positive for the disease, what is the probability that they have it?\n",
    "```\n",
    "\n",
    "In order to answer this question, we first need to set up a probability space that models the scenario. As a sample space $S$, we would take the population of people under consideration. We assume that the probability measure $P$ is uniform over the population. Then, the two events in $S$ that we are concerned with are the events $D$ and $T$, where $D$ consists of those people that have the disease and $T$ consists of those people who tested positive.\n",
    "\n",
    "Our first goal is to take the three statements in bullet points and turn them into probability statements involving the events $D$ and $T$. Since the probability measure $P$ is uniform, the percentages given in the problem statements _are_ probabilities. So, to say that the disease affects only $0.1\\%$ of the population means that\n",
    "\n",
    "$$\n",
    "P(D) = 0.001.\n",
    "$$\n",
    "\n",
    "The test produces a false negative when it says that a person does _not_ have the disease when they actually _do_ have the disease. The problem statement tells us that this _never_ happens, so we have $P(T^c | D) = 0$, which is equivalent to\n",
    "\n",
    "```{margin}\n",
    "The probability $P(T|D)$ is sometimes called the _true positive rate_ or the _recall_ of the test, while the probability $P(D|T)$ is sometimes called the _precision_ of the test.\n",
    "```\n",
    "\n",
    "$$\n",
    "P(T | D) = 1.\n",
    "$$\n",
    "\n",
    "The test produces a false positive when it says that a person _has_ the disease when they actually do _not_ have the disease. This happens at a rate of $5\\%$, so we have\n",
    "\n",
    "$$\n",
    "P(T|D^c) = 0.05.\n",
    "$$\n",
    "\n",
    "Finally, the goal of the problem is to compute the conditional probability $P(D|T)$.\n",
    "\n",
    "It is helpful to conceptualize a _causal_ relationship between the two events $D$ and $T$, or a \"direction of influence.\" It is certainly the case that the presence of the disease influences the outcome of the test, while it is difficult to imagine that the outcome of the test could influence whether or not a person has the disease. Thus, the direction of influence flows from $D$ to $T$, which we might depict graphically as:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "```{image} ../img/bayes.svg\n",
    ":width: 35%\n",
    ":align: center\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "This problem involves reasoning about the events $D$ and $T$. Reasoning that _begins_ with $D$ and flows downward toward $T$ in the direction of influence is sometimes called _causal reasoning_; in the other direction, reasoning that _begins_ with $T$ and flows upward toward $D$ in the opposite direction of influence is sometimes called _evidential reasoning_. In this language:\n",
    "\n",
    "* The event $T$ is called the _evidence_ and is said to be _observed_. The event $D$ is called _hidden_ or _latent_.\n",
    "* Determining the conditional probability $P(T|D)$ is part of causal reasoning, since it begins with $D$ as a _given_.\n",
    "* Determining the conditional probability $P(D|T)$ is part of evidential reasoning, since it begins with $T$ (the evidence!) as a _given_.\n",
    "\n",
    "Now, the problem statement gives us $P(T|D)=1$ and asks us to compute $P(D|T)$. Thus, we need a tool that allows us to reverse causal reasoning and turn it into evidential reasoning. This tool is precisely Bayes' Theorem!\n",
    "\n",
    "```{prf:theorem} Bayes' Theorem\n",
    ":label: bayes-event-thm\n",
    "\n",
    "Let $A$ and $B$ be two events in a sample space $S$ with $P(B)>0$. Then\n",
    "\n",
    "$$ P(A| B) = \\frac{P(B|A)P(A)}{P(B)}.$$\n",
    "```\n",
    "\n",
    "The proof of Bayes' Theorem is a complete triviality and follows immediately from the definitions. The real value of the theorem derives not from it being a \"difficult\" theorem, but rather from the fact that it allows us to reverse the direction of conditioning and to thereby reverse causal reasoning to evidential reasoning (and vice versa). Applying Bayes' Theorem to our situation, we would write:\n",
    "\n",
    "$$\n",
    "P(D| T) = \\frac{P(T|D)P(D)}{P(T)}.\n",
    "$$\n",
    "\n",
    "Thus, to compute $P(D|T)$, we just need to compute the three probabilities on the right-hand side. We will do precisely this in the next problem. You will probably find the answer surprising.\n",
    "\n",
    "```{admonition} Problem Prompt\n",
    "Do problem 11 on the worksheet.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}