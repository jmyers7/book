
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. More probability theory &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Cat": "\\mathcal{C}at", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bpi": "\\boldsymbol\\pi", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bf": "\\mathbf{f}", "bg": "\\mathbf{g}", "bp": "\\mathbf{p}", "br": "\\mathbf{r}", "bs": "\\mathbf{s}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bD": "\\mathbf{D}", "bH": "\\mathbf{H}", "bI": "\\mathbf{I}", "bK": "\\mathbf{K}", "bS": "\\mathbf{S}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bW": "\\mathbf{W}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calJ": "\\mathcal{J}", "calH": "\\mathcal{H}", "calI": "\\mathcal{I}", "calL": "\\mathcal{L}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "calS": "\\mathcal{S}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/08-more-prob';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. The halfway point: pivoting toward models and data analysis" href="09-halfway.html" />
    <link rel="prev" title="7. Random vectors" href="07-random-vectors.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-halfway.html">9. The halfway point: pivoting toward models and data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-info-theory.html">10. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-optim.html">11. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-models.html">12. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-learning.html">13. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-stat-infer.html">14. Statistical inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">15. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/08-more-prob.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>More probability theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectations-and-joint-distributions">8.1. Expectations and joint distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectations-and-conditional-distributions">8.2. Expectations and conditional distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-transformations">8.3. Density transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#moment-generating-functions">8.4. Moment generating functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependent-random-variables-covariance-and-correlation">8.5. Dependent random variables, covariance, and correlation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-normal-distributions">8.6. Multivariate normal distributions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="more-probability-theory">
<h1><span class="section-number">8. </span>More probability theory<a class="headerlink" href="#more-probability-theory" title="Link to this heading">#</a></h1>
<p>Introduction to be written later… Beware that this is a first draft of the chapter. To do:</p>
<ol class="arabic simple">
<li><p>Rewrite section on moment generating functions.</p></li>
</ol>
<section id="expectations-and-joint-distributions">
<h2><span class="section-number">8.1. </span>Expectations and joint distributions<a class="headerlink" href="#expectations-and-joint-distributions" title="Link to this heading">#</a></h2>
<p>To motivate the considerations in this section, suppose that we have two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> defined on a probability space <span class="math notranslate nohighlight">\(S\)</span> and a real-valued function <span class="math notranslate nohighlight">\(g:\bbr^2 \to \bbr\)</span>. We may then “transform” the random variables to obtain the random variable</p>
<div class="math notranslate nohighlight">
\[
g(X,Y): S \xrightarrow{(X,Y)} \bbr^2 \xrightarrow{g} \bbr,
\]</div>
<p>where <span class="math notranslate nohighlight">\((X,Y)\)</span> denotes the <span class="math notranslate nohighlight">\(2\)</span>-dimensional random vector with <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as its components. Notice that the notation <span class="math notranslate nohighlight">\(g(X,Y)\)</span> strictly interpreted is an abuse of notation; as the arrows above indicate, this notation actually stands for the composite <span class="math notranslate nohighlight">\(g\circ (X,Y)\)</span>, where we view <span class="math notranslate nohighlight">\((X,Y)\)</span> as a function in accordance with <a class="reference internal" href="07-random-vectors.html#two-dim-vector-def">Definition 7.1</a>.</p>
<p>In any case, provided that this “transformed” random variable <span class="math notranslate nohighlight">\(Z = g(X,Y)\)</span> is continuous (say) with density <span class="math notranslate nohighlight">\(f_{g(X,Y)}(z)\)</span>, we may compute its expectation (according to the definition!) as</p>
<div class="math notranslate nohighlight">
\[
E(g(X,Y)) = \int_{\bbr} z f_{g(X,Y)}(z) \ \text{d}z.
\]</div>
<p>However, this formula is quite inconvenient to use in practice, due to the necessity of the density <span class="math notranslate nohighlight">\(f_{g(X,Y)}(z)\)</span>. We wonder if there is an alternate method to compute this expectation, one that uses the joint density <span class="math notranslate nohighlight">\(f(x,y)\)</span>. Indeed, there is! It is outlined in the following bivariate generalization of the LotUS from <a class="reference internal" href="04-random-variables.html#lotus-thm">Theorem 4.1</a>.</p>
<div class="proof theorem admonition" id="bivariate-lotus-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.1 </span> (Bivariate Law of the Unconscious Statistician (LotUS))</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables and <span class="math notranslate nohighlight">\(g:\mathbb{R}^2 \to \mathbb{R}\)</span> a function.</p>
<ol class="arabic">
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are jointly discrete with mass function <span class="math notranslate nohighlight">\(p(x,y)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
    E\left(g(X,Y)\right) = \sum_{(x,y)\in\mathbb{R}^2} g(x,y) p(x,y).
    \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are jointly continuous with density function <span class="math notranslate nohighlight">\(f(x,y)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
    E\left(g(X,Y)\right) = \iint_{\mathbb{R}^2} g(x,y) f(x,y) \ \text{d}x \text{d}y.
    \]</div>
</li>
</ol>
</section>
</div><p>Though the argument in the discrete case is very similar to the one given for the univariate version of <a class="reference internal" href="04-random-variables.html#lotus-thm">Theorem 4.1</a>, we will not give it here. See if you can work it out on your own. You can imagine that the univariate and bivariate LotUS’s are special cases of a general multivariate LotUS that computes expecations of random variables of the form <span class="math notranslate nohighlight">\(g(X_1,\ldots,X_n)\)</span>, where each <span class="math notranslate nohighlight">\(X_i\)</span> is a random variable and <span class="math notranslate nohighlight">\(n\geq 1\)</span>. I will leave you to imagine what the statement of this multivariate LotUS looks like. For those who might be interested, in the most general case, all of these LotUS’s are consequences of the general <a class="reference external" href="https://en.wikipedia.org/wiki/Pushforward_measure#Main_property:_change-of-variables_formula">change-of-variables formula</a> for Lebesgue integrals.</p>
<p>Our first application of the bivariate LotUS is to show that the expectation operator is multiplicative on independent random variables:</p>
<div class="proof theorem admonition" id="ind-expect-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.2 </span> (Independence and expectations)</p>
<section class="theorem-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables, then <span class="math notranslate nohighlight">\(E(XY) = E(X) E(Y)\)</span>.</p>
</section>
</div><p>The proof is a simple computation using the LotUS; it appears in your <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/08-homework.md#problem-2-independence-and-expectations">homework</a> for this chapter.</p>
<p>Our second application of the bivariate LotUS is to tie up a loose end from <a class="reference internal" href="04-random-variables.html#linear-of-exp"><span class="std std-numref">Section 4.9</span></a> and prove the full-strength version of “linearity of expectation.”</p>
<div class="proof theorem admonition" id="linear-exp-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.3 </span> (Linearity of Expectations)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables and let <span class="math notranslate nohighlight">\(c\in \mathbb{R}\)</span> be a constant. Then:</p>
<div class="math notranslate nohighlight" id="equation-target-eqn">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-target-eqn" title="Link to this equation">#</a></span>\[
E(X+Y) = E(X) + E(Y),
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-scalar-eqn">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-scalar-eqn" title="Link to this equation">#</a></span>\[
E(cX) = c E(X).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof of the second equation <a class="reference internal" href="#equation-scalar-eqn">(8.2)</a> was already handled back in the proof of <a class="reference internal" href="04-random-variables.html#weak-linear-thm">Theorem 4.3</a>. For the proof of the first equation <a class="reference internal" href="#equation-target-eqn">(8.1)</a> (in the continuous case), we apply the bivariate LotUS:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
E(X+Y) &amp;= \iint_{\mathbb{R}^2} (x+y) f(x,y) \ \text{d}x \text{d}y \\
&amp;= \int_{\mathbb{R}} x\left[ \int_{\mathbb{R}} f(x,y) \ \text{d}y\right] \ \text{d}x + \int_{\mathbb{R}} y\left[ \int_{\mathbb{R}} f(x,y) \ \text{d}x\right] \ \text{d}y \\
&amp;= \int_{\mathbb{R}} xf(x) \ \text{d}x + \int_{\mathbb{R}} y f(y) \ \text{d}y \\
&amp;= E(X) + E(Y).
\end{align*}\]</div>
</div>
<p>Let’s finish off the section by working through an example problem:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 1 on the worksheet.</p>
</div>
</section>
<section id="expectations-and-conditional-distributions">
<span id="cond-expect"></span><h2><span class="section-number">8.2. </span>Expectations and conditional distributions<a class="headerlink" href="#expectations-and-conditional-distributions" title="Link to this heading">#</a></h2>
<p>In the previous section, we learned how to use joint distributions in computations of expectations. In this section, we define <em>new</em> types of expectations using conditional distributions.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>There is also a conditional version of variance. Can you guess how it might be defined? Take a look at Section 4.7 in <span id="id1">[<a class="reference internal" href="bib.html#id7" title="M. H. DeGroot and M. J. Schervish. Probability and statistics. Volume 563. Pearson Education London, UK, 2014.">DS14</a>]</span> to learn more.</p>
</aside>
<div class="proof definition admonition" id="conditional-exp-def">
<p class="admonition-title"><span class="caption-number">Definition 8.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables.</p>
<ol class="arabic">
<li><p>If <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are jointly discrete with conditional mass function <span class="math notranslate nohighlight">\(p(y|x)\)</span>, then the <em>conditional expected value</em> of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span> is the sum</p>
<div class="math notranslate nohighlight">
\[
    E(Y\mid X=x) \def \sum_{y\in \mathbb{R}} y p(y|x).
    \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> are jointly continuous with conditional density function <span class="math notranslate nohighlight">\(f(y|x)\)</span>, then the <em>conditional expected value</em> of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span> is the integral</p>
<div class="math notranslate nohighlight">
\[
    E(Y\mid X=x) \def \int_{\mathbb{R}} y f(y|x) \ \text{d}y.
    \]</div>
</li>
</ol>
</section>
</div><p>In both cases, notice that conditional expected values depend on the particular choice of observed value <span class="math notranslate nohighlight">\(x\)</span>. This means that these conditional expectations yield <em>functions</em>:</p>
<div class="math notranslate nohighlight">
\[
h:A \to \bbr, \quad x\mapsto h(x) = E(Y\mid X=x),
\]</div>
<p>where <span class="math notranslate nohighlight">\(A\)</span> is the subset of <span class="math notranslate nohighlight">\(\bbr\)</span> consisting of all <span class="math notranslate nohighlight">\(x\)</span> for which either the conditional mass function <span class="math notranslate nohighlight">\(p(y|x)\)</span> or density function <span class="math notranslate nohighlight">\(f(y|x)\)</span> exists. There is a bit of notational awkwardness here, since I rather arbitrarily chose the name <span class="math notranslate nohighlight">\(h\)</span> for this function. It would be much preferable to have notation that is somehow built from <span class="math notranslate nohighlight">\(E(Y \mid X=x)\)</span>, but there isn’t a commonly accepted one.</p>
<p>Let’s take a look at a practice problem before proceeding.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 2 on the worksheet.</p>
</div>
<p>The input <span class="math notranslate nohighlight">\(x\)</span> to the conditional expectation function</p>
<div class="math notranslate nohighlight">
\[
h(x) = E(Y\mid X=x)
\]</div>
<p>is an observation of a random variable, and thus this function is also “random.” To make this precise, let’s suppose for simplicity that <span class="math notranslate nohighlight">\(h\)</span> is defined for all <span class="math notranslate nohighlight">\(x\in \bbr\)</span>, so that <span class="math notranslate nohighlight">\(h:\bbr \to \bbr\)</span>. Then, if <span class="math notranslate nohighlight">\(S\)</span> is the probability space on which <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are defined, we may form the function</p>
<div class="math notranslate nohighlight">
\[
E(Y \mid X) : S \xrightarrow{X} \bbr \xrightarrow{h} \bbr
\]</div>
<p>obtained as the composite <span class="math notranslate nohighlight">\(E(Y \mid X) \def h \circ X\)</span>. Note that <span class="math notranslate nohighlight">\(E(Y\mid X)\)</span> is a real-valued function defined on a probability space and thus (by definition!) it is a random variable. This is a bit confusing the first time you see it, so make sure to pause and ponder it for bit! To help, let’s take a look at a simple example problem where we unwind everything:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 3 on the worksheet.</p>
</div>
<p>Now, since <span class="math notranslate nohighlight">\(Z = E(Y\mid X)\)</span> is a random variable, it has an expected value. Provided that it is continuous (for example) with density function <span class="math notranslate nohighlight">\(f_{E(Y\mid X)}(z)\)</span>, we would compute its expectation (by definition!) as the integral</p>
<div class="math notranslate nohighlight" id="equation-ah-eqn">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-ah-eqn" title="Link to this equation">#</a></span>\[
E\big[ E(Y \mid X) \big] = \int_\bbr z f_{E(Y\mid X)}(z) \ \text{d} z.
\]</div>
<p>However, this formula is of little practical value, since we would need to compute the density <span class="math notranslate nohighlight">\(f_{E(Y\mid X)}(z)\)</span> which can be quite difficult. So, we wonder: Is there an alternate way to compute this expectation? In fact there is! The key is to <em>not</em> compute the expectation according to the definition <a class="reference internal" href="#equation-ah-eqn">(8.3)</a>, but rather to use the LotUS:</p>
<div class="math notranslate nohighlight" id="equation-cond-lotus-eq">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-cond-lotus-eq" title="Link to this equation">#</a></span>\[
E\big[ E(Y \mid X) \big] = \int_\bbr E(Y \mid X=x) f(x) \ \text{d}x.
\]</div>
<p>However, if you push through the computations beginning with <a class="reference internal" href="#equation-cond-lotus-eq">(8.4)</a>, you will find that the “iterated” expectation reduces to the expectation of <span class="math notranslate nohighlight">\(Y\)</span>. This is the content of:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This law is also often called The Law of Iterated Expecation, among a bunch of <a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_total_expectation">other names</a>.</p>
</aside>
<div class="proof theorem admonition" id="law-of-total-exp-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.4 </span> (The Law of Total Expectation)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables that are either jointly discrete or jointly continuous. Then</p>
<div class="math notranslate nohighlight">
\[
E\big[ E(Y \mid X) \big] = E(Y).
\]</div>
</section>
</div><p>Before we begin the proof, observe that there is a problem with the interpretation of the integral <a class="reference internal" href="#equation-cond-lotus-eq">(8.4)</a> since it extends over <em>all</em> <span class="math notranslate nohighlight">\(x\)</span>-values, whereas the function <span class="math notranslate nohighlight">\(E(Y\mid X=x)\)</span> is only defined for those <span class="math notranslate nohighlight">\(x\)</span>-values for which the conditional density <span class="math notranslate nohighlight">\(f(y|x)\)</span> exists (at least according to the conventions we use in this class). We ran into a similar problem when we discussed the Law of Total Probability in <a class="reference internal" href="07-random-vectors.html#law-of-total-prob-rvs-thm">Theorem 7.5</a>. However, we will fix this problem by declaring <span class="math notranslate nohighlight">\(E(Y\mid X=x)=0\)</span> for those <span class="math notranslate nohighlight">\(x\)</span>-values outside the domain of the conditional expectation.</p>
<div class="proof admonition" id="proof">
<p>Proof. Let’s consider the case that the variables are jointly continuous. Beginning from <a class="reference internal" href="#equation-cond-lotus-eq">(8.4)</a>, we compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
E\big[ E(Y \mid X) \big] &amp;= \int_{\bbr} E(Y \mid X=x) f(x) \ \text{d}x \\
&amp;= \int_\bbr \int_\bbr y f(y|x) f(x) \ \text{d}y \text{d}x \\
&amp;= \int_\bbr \int_\bbr y f(x,y) \ \text{d}y \text{d}x \\
&amp;= \int_\bbr y f(y) \ \text{d} y \\
&amp;= E(Y).
\end{align*}\]</div>
<p>Besides the LotUS in the first line, notice that in going from the third line to the fourth, we integrated out the dependence on <span class="math notranslate nohighlight">\(x\)</span> of the joint density <span class="math notranslate nohighlight">\(f(x,y)\)</span> to obtain the marginal <span class="math notranslate nohighlight">\(f(y)\)</span>. Q.E.D.</p>
</div>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 4 on the worksheet.</p>
</div>
</section>
<section id="density-transformations">
<span id="fun-rvs"></span><h2><span class="section-number">8.3. </span>Density transformations<a class="headerlink" href="#density-transformations" title="Link to this heading">#</a></h2>
<p>It is very often the case that one needs to compute the density of a transformed continuous random variable. Actually, we saw such a situation already in <a class="reference internal" href="05-examples-of-rvs.html#affine-gaussian-thm">Theorem 5.7</a> where we computed the distribution of an affine transformation of a normal random variable. We proved that theorem by explicitly computing the density of the transformed random variable. The main result in this section gives us a direct formula:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>A function is called <em>continuously differentiable</em> if it has a continuous derivative.</p>
</aside>
<div class="proof theorem admonition" id="univar-density-trans-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.5 </span> (Density Transformation Theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a continuous random variable and let <span class="math notranslate nohighlight">\(T\)</span> be the support of the density <span class="math notranslate nohighlight">\(f_X(x)\)</span>. Let <span class="math notranslate nohighlight">\(r:T\to \bbr\)</span> be a function and set <span class="math notranslate nohighlight">\(Y=r(X)\)</span>. Suppose that the range <span class="math notranslate nohighlight">\(U\)</span> of <span class="math notranslate nohighlight">\(r\)</span> is open in <span class="math notranslate nohighlight">\(\bbr\)</span>, and that there exists a continuously differentiable function <span class="math notranslate nohighlight">\(s:U \to \bbr\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-inv-s-eq">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-inv-s-eq" title="Link to this equation">#</a></span>\[
s(r(x)) = x
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\in T\)</span>. Then the random variable <span class="math notranslate nohighlight">\(Y\)</span> is continuous, and we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f_Y(y) = \begin{cases}
f_X(s(y)) \left| \displaystyle\frac{\text{d}s}{\text{d}y}(y) \right| &amp; : y\in U, \\
0 &amp; : \text{otherwise}.
\end{cases}
\end{split}\]</div>
</section>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>First, a reminder for the well-informed reader: According to our definitions in this book, the <em>support</em> of a function <span class="math notranslate nohighlight">\(f:\mathcal{X} \to \bbr\)</span> defined on a topological space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is simply the set of points <span class="math notranslate nohighlight">\(x\in \mathcal{X}\)</span> such that <span class="math notranslate nohighlight">\(f(x)\neq 0\)</span>. In particular, we are <em>not</em> taking the closure!</p>
<p>Then, we recall that a continuous random variable technically has infinitely many densities, and that any two are equal <a class="reference external" href="https://en.wikipedia.org/wiki/Almost_everywhere">almost everywhere</a>. So, if you give me a density of <span class="math notranslate nohighlight">\(X\)</span> whose support contains the range of <span class="math notranslate nohighlight">\(X\)</span>, I can alter the density by setting it to <span class="math notranslate nohighlight">\(0\)</span> on a finite set of points (say) to obtain a density whose support does <em>not</em> contain the range of <span class="math notranslate nohighlight">\(X\)</span>.</p>
</aside>
<p>We will not prove the theorem, as it will end up being a special case of the generalized transformation theorem given in <a class="reference internal" href="#multivar-density-trans-thm">Theorem 8.6</a> below. But we should also say that there are simpler proofs of the theorem that do not use the heavy machinery that the proof of <a class="reference internal" href="#multivar-density-trans-thm">Theorem 8.6</a> relies upon; see, for example, Section 4.7 in <span id="id2">[<a class="reference internal" href="bib.html#id6" title="J. L. Devore, K. N. Berk, and M. A. Carlton. Modern mathematical statistics with applications. Springer Texts in Statistics. Springer, Cham, third edition, 2021.">DBC21</a>]</span>.</p>
<p>We are assuming that the support <span class="math notranslate nohighlight">\(T\)</span> contains the range of the random variable <span class="math notranslate nohighlight">\(X\)</span> so that the random variable <span class="math notranslate nohighlight">\(Y\)</span> is defined; indeed, remember that <span class="math notranslate nohighlight">\(Y=r(X)\)</span> is an abuse of notation that stands for the composite function <span class="math notranslate nohighlight">\(r\circ X\)</span>. (See the margin note directly to the right.)</p>
<p>Observe that the equality <a class="reference internal" href="#equation-inv-s-eq">(8.5)</a> is “one half” of what it means for <span class="math notranslate nohighlight">\(s\)</span> to be the inverse of <span class="math notranslate nohighlight">\(r\)</span>. But in fact, since the domain of <span class="math notranslate nohighlight">\(s\)</span> is precisely the range of <span class="math notranslate nohighlight">\(r\)</span>, as you may easily check we <em>automatically</em> have the other equality:</p>
<div class="math notranslate nohighlight">
\[
r(s(y)) = y
\]</div>
<p>for all <span class="math notranslate nohighlight">\(y\in U\)</span>. Hence, <span class="math notranslate nohighlight">\(r\)</span> is invertible with inverse function <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>Note that we are <em>explicitly</em> assuming that the inverse <span class="math notranslate nohighlight">\(s\)</span> is continuously differentiable. But in other versions of the Density Transformation Theorem (and its generalization <a class="reference internal" href="#multivar-density-trans-thm">Theorem 8.6</a> below), certain conditions are placed on <span class="math notranslate nohighlight">\(r\)</span> that guarantee differentiability of <span class="math notranslate nohighlight">\(s\)</span> via the <a class="reference external" href="https://en.wikipedia.org/wiki/Inverse_function_theorem">Inverse Function Theorem</a>. We have elected to give the version of the theorem presented here because in practice (or, at least in many textbook problems) it is often quicker to simply check differentiability of <span class="math notranslate nohighlight">\(s\)</span> directly, rather than verify the preconditions of the Inverse Function Theorem hold.</p>
<p>Let’s do some examples:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problems 5 and 6 on the worksheet.</p>
</div>
<p>We now begin moving toward the generalization of The Density Transformation Theorem to higher dimensions. The statement of this generalization uses the following object:</p>
<div class="proof definition admonition" id="gradient-mat-def">
<p class="admonition-title"><span class="caption-number">Definition 8.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Given a vector-valued function</p>
<div class="math notranslate nohighlight">
\[
\bs:\bbr^n\to \bbr^m, \quad \bs(\by) = (s_1(\by),\ldots,s_m(\by)),
\]</div>
<p>its <em>gradient matrix</em> at <span class="math notranslate nohighlight">\(\by\)</span>, denoted <span class="math notranslate nohighlight">\(\nabla \bs(\by)\)</span>, is the <span class="math notranslate nohighlight">\(n\times m\)</span> matrix of first-order partial derivatives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla \bs (\by) \def \left[ \frac{\partial s_j}{\partial y_i}(\by)\right] = \begin{bmatrix}
\displaystyle\frac{\partial s_1}{\partial y_1}(\by) &amp; \cdots &amp; \displaystyle\frac{\partial s_m}{\partial y_1}(\by) \\
\vdots &amp; \ddots &amp; \vdots \\
\displaystyle\frac{\partial s_1}{\partial y_n}(\by) &amp; \cdots &amp; \displaystyle\frac{\partial s_m}{\partial y_n}(\by)
\end{bmatrix},
\end{split}\]</div>
<p>provided that the partial derivatives exist at <span class="math notranslate nohighlight">\(\by \in \bbr^m\)</span>.</p>
</section>
</div><p>Note that the gradient matrix is the transpose of the <em>Jacobian matrix</em> of <span class="math notranslate nohighlight">\(\bs\)</span> at <span class="math notranslate nohighlight">\(\by\)</span>, denoted <span class="math notranslate nohighlight">\((\partial \bs / \partial \by)(\by)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla \bs (\by)^\intercal = \frac{\partial \bs}{\partial \by}(\by) \def \begin{bmatrix}
\displaystyle\frac{\partial s_1}{\partial y_1}(\by) &amp; \cdots &amp; \displaystyle\frac{\partial s_1}{\partial y_n}(\by) \\
\vdots &amp; \ddots &amp; \vdots \\
\displaystyle\frac{\partial s_m}{\partial y_1}(\by) &amp; \cdots &amp; \displaystyle\frac{\partial s_m}{\partial y_n}(\by)
\end{bmatrix}.
\end{split}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>If all the first-order partial derivatives of <span class="math notranslate nohighlight">\(s\)</span> exist and are continuous, then <span class="math notranslate nohighlight">\(s\)</span> is said to be <em>continuously differentiable</em>. If <span class="math notranslate nohighlight">\(s\)</span> is a funtion from <span class="math notranslate nohighlight">\(\bbr\)</span> to <span class="math notranslate nohighlight">\(\bbr\)</span>, this coincides with the definition of <em>continuously differentiable</em> given in the margin note above.</p>
</aside>
<p>The Jacobian matrix is the matrix representation (with respect to the standard bases) of the <em>derivative</em> of <span class="math notranslate nohighlight">\(\bs\)</span>, provided that the latter is <em>differentiable</em>. We will not talk about <em>differentiability</em> and <em>derivatives</em> in higher dimensions; for that, see Chapter 2 in <span id="id3">[<a class="reference internal" href="bib.html#id15" title="M. Spivak. Calculus on manifolds. Perseus Books Publishing, LLC, 1965.">Spi65</a>]</span>. All we will say is that existence of the partial derivatives in the Jacobian matrix is <em>necessary</em> for differentiability, but not <em>sufficient</em>. If, however, the partial derivatives are all continuous, then <span class="math notranslate nohighlight">\(s\)</span> is differentiable. (See Theorem 2-8 in <span id="id4">[<a class="reference internal" href="bib.html#id15" title="M. Spivak. Calculus on manifolds. Perseus Books Publishing, LLC, 1965.">Spi65</a>]</span>.)</p>
<p>The gradient matrix is used to define the affine <em>tangent space</em> approximation of <span class="math notranslate nohighlight">\(\bs\)</span> near a point of differentiability <span class="math notranslate nohighlight">\(\by^\star\)</span> via the equation</p>
<div class="math notranslate nohighlight">
\[
\bs(\by) = \bs(\by^\star) + \nabla \bs (\by^\star)^\intercal (\by - \by^\star), \quad \by \in \bbr^n.
\]</div>
<p>Notice the similarity to the affine <em>tangent line</em> approximation studied in single-variable calculus.</p>
<p>With gradient matrices in hand, we now state the generalization of <a class="reference internal" href="#univar-density-trans-thm">Theorem 8.5</a>:</p>
<div class="proof theorem admonition" id="multivar-density-trans-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.6 </span> (Multivariate Density Transformation Theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX\)</span> be a continuous <span class="math notranslate nohighlight">\(m\)</span>-dimensional random vector and let <span class="math notranslate nohighlight">\(T\)</span> be the support of the density <span class="math notranslate nohighlight">\(f_\bX(\bx)\)</span>. Let <span class="math notranslate nohighlight">\(\br:T \to \bbr^m\)</span> be a function and set <span class="math notranslate nohighlight">\(\bY = \br(\bX)\)</span>. Suppose that the range <span class="math notranslate nohighlight">\(U\)</span> of <span class="math notranslate nohighlight">\(\br\)</span> is open in <span class="math notranslate nohighlight">\(\bbr^m\)</span>, and that there exists a continuously differentiable function <span class="math notranslate nohighlight">\(\bs:U \to \bbr^m\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\bs(\br(\bx)) = \bx
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\bx\in T\)</span>. Then the random vector <span class="math notranslate nohighlight">\(\bY\)</span> is continuous, and we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f_{\bY}(\by) = \begin{cases}
f_{\bX}(\bs(\by)) \left| \det\left(\nabla\bs(\by)\right) \right| &amp; : \by \in U, \\
0 &amp; : \by \notin U.
\end{cases}
\end{split}\]</div>
</section>
</div><p>The following proof uses mathematics that are likely unfamiliar. Look through it if you like, but also feel free to skip it as well. I’ve included it really only out of principle, because I could not find a satisfactory proof in the standard references on my bookshelf.</p>
<div class="proof admonition" id="proof">
<p>Proof. Letting <span class="math notranslate nohighlight">\(V \subset \bbr^m\)</span> be an open set, we compute:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
P(\bY\in V) &amp;= P(\bX\in \bs(V\cap U)) \\
&amp;= \int_{\bs(V\cap U)} f_\bX(\bx) \ \text{d}^m\bx \\
&amp;= \int_{V\cap U} f_\bX(\bs(\by))\left| \det\left(\nabla \bs(\by)\right) \right| \ \text{d}^m \by,
\end{align*}\]</div>
<p>where the final equality follows from the Change-of-Variables Theorem for Multiple Integrals; see Theorem 3-13 in <span id="id5">[<a class="reference internal" href="bib.html#id15" title="M. Spivak. Calculus on manifolds. Perseus Books Publishing, LLC, 1965.">Spi65</a>]</span>. If we then define <span class="math notranslate nohighlight">\(f_\bY(\by)\)</span> via the formula given in the statement of the theorem, this shows</p>
<div class="math notranslate nohighlight">
\[
P(\bY\in V) = \int_{V\cap U} f_\bX(\bs(\by))\left| \det\left( \nabla \bs (\by)\right) \right| \ \text{d}^m \by = \int_V f_\bY(\by) \ \text{d}^m \by.
\]</div>
<p>Since a probability measure defined on the Borel algebra of <span class="math notranslate nohighlight">\(\bbr^m\)</span> is uniquely determined by its values on open sets (via a generating class argument), this is enough to prove that <span class="math notranslate nohighlight">\(f_\bY(\by)\)</span> is indeed a density of <span class="math notranslate nohighlight">\(\bY\)</span>. Q.E.D.</p>
</div>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 7 in the worksheet.</p>
</div>
</section>
<section id="moment-generating-functions">
<span id="mgf"></span><h2><span class="section-number">8.4. </span>Moment generating functions<a class="headerlink" href="#moment-generating-functions" title="Link to this heading">#</a></h2>
<p>We begin with:</p>
<div class="proof definition admonition" id="moments-def">
<p class="admonition-title"><span class="caption-number">Definition 8.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(k\geq 1\)</span> be an integer and <span class="math notranslate nohighlight">\(X\)</span> a random variable.</p>
<ul class="simple">
<li><p>The <em><span class="math notranslate nohighlight">\(k\)</span>-th moment</em> of <span class="math notranslate nohighlight">\(X\)</span> is the expectation <span class="math notranslate nohighlight">\(E(X^k)\)</span>.</p></li>
<li><p>The <em><span class="math notranslate nohighlight">\(k\)</span>-th central moment</em> of <span class="math notranslate nohighlight">\(X\)</span> is the expectation <span class="math notranslate nohighlight">\(E\left( (X-\mu)^k \right)\)</span>, where <span class="math notranslate nohighlight">\(\mu = E(X)\)</span>.</p></li>
</ul>
</section>
</div><p>Take care to notice that I am not claiming that <em>all</em> of these moments exist for <em>all</em> random variables. Notice also that the first moment of <span class="math notranslate nohighlight">\(X\)</span> is precisely its expectation, while its second central moment is its variance. The “higher moments” are more difficult to interpret. The situation with them is analogous to the “higher derivatives” of a function <span class="math notranslate nohighlight">\(y=f(x)\)</span>. I have good intuition for what the first two derivatives <span class="math notranslate nohighlight">\(f'(x)\)</span> and <span class="math notranslate nohighlight">\(f''(x)\)</span> measure, but I have much less intuition for what the thirty-first derivative <span class="math notranslate nohighlight">\(f^{(31)}(x)\)</span> measures!</p>
<p>Actually, this analogy with derivatives can be carried further, so let’s leave the world of probability theory for a moment and return to calculus. Indeed, as you well know if a function <span class="math notranslate nohighlight">\(y=f(x)\)</span> has derivatives of all orders at <span class="math notranslate nohighlight">\(x=0\)</span>, then we can form its Taylor series centered at <span class="math notranslate nohighlight">\(x=0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-taylor-eqn">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-taylor-eqn" title="Link to this equation">#</a></span>\[
f(0) + f'(0) x + \frac{f''(0)}{2!}x^2 + \cdots = \sum_{k=0}^\infty \frac{f^{(k)}(0)}{k!}x^k.
\]</div>
<p>You also learned that this series may, or may <em>not</em>, converge to the original function <span class="math notranslate nohighlight">\(y=f(x)\)</span> on an open interval about <span class="math notranslate nohighlight">\(x=0\)</span>. For example, the Taylor series of <span class="math notranslate nohighlight">\(y=e^x\)</span> actually converges to <span class="math notranslate nohighlight">\(y=e^x\)</span> <em>everywhere</em>:</p>
<div class="math notranslate nohighlight">
\[
e^x = \sum_{k=0}^\infty \frac{x^k}{k!}.
\]</div>
<p>On the other hand, <a class="reference external" href="https://en.wikipedia.org/wiki/Non-analytic_smooth_function">there exist</a> functions for which the Taylor series <a class="reference internal" href="#equation-taylor-eqn">(8.6)</a> exists and converges <em>everywhere</em>, but does <em>not</em> converge to the function on <em>any</em> open interval around <span class="math notranslate nohighlight">\(x=0\)</span>.</p>
<div class="proof theorem admonition" id="taylor-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.7 </span> (Taylor Series Uniqueness Theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose <span class="math notranslate nohighlight">\(y=f(x)\)</span> and <span class="math notranslate nohighlight">\(y=g(x)\)</span> are two functions whose Taylor series (centered at <span class="math notranslate nohighlight">\(x=0\)</span>) converge to <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> on open intervals containing <span class="math notranslate nohighlight">\(x=0\)</span>. Then the following statements are equivalent:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(f(x)=g(x)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> in an open interval containing <span class="math notranslate nohighlight">\(x=0\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(f^{(k)}(0) = g^{(k)}(0)\)</span> for all <span class="math notranslate nohighlight">\(k\geq 0\)</span>.</p></li>
<li><p>The Taylor series for <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> (centered at <span class="math notranslate nohighlight">\(x=0\)</span>) are equal coefficient-wise.</p></li>
</ol>
</section>
</div><p>What this Uniqueness Theorem tells us is that complete knowledge of <em>all</em> the values <span class="math notranslate nohighlight">\(f^{(k)}(0)\)</span> determines the function <span class="math notranslate nohighlight">\(y=f(x)\)</span> uniquely, at least locally near <span class="math notranslate nohighlight">\(x=0\)</span>. Therefore, even though we don’t have good intuition for what all the higher derivatives <span class="math notranslate nohighlight">\(f^{(k)}(0)\)</span> <em>mean</em>, they are still incredibly important and useful objects. Just think about it: If the Taylor series of <span class="math notranslate nohighlight">\(f(x)\)</span> converges to <span class="math notranslate nohighlight">\(f(x)\)</span> on an open interval containing <span class="math notranslate nohighlight">\(x=0\)</span>, then <em>uncountably</em> many functional values <span class="math notranslate nohighlight">\(f(x)\)</span> are determined by a <em>countable</em> list of numbers <span class="math notranslate nohighlight">\(f(0),f'(0),f''(0),\ldots\)</span>. There is an incredible amount of information encoded in these derivatives.</p>
<p>Now, hold this lesson in your mind for just a bit as we return to probability theory. We are about to see something <em>very similar</em> occur in the context of moments of random variables. The gadget that is going to play the role for random variables analogous to Taylor series is defined in:</p>
<div class="proof definition admonition" id="mgf-def">
<p class="admonition-title"><span class="caption-number">Definition 8.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable. The <em>moment generating function</em> (<em>MGF</em>) of <span class="math notranslate nohighlight">\(X\)</span> is defined to be</p>
<div class="math notranslate nohighlight">
\[
\psi(t) = E(e^{tX}).
\]</div>
<p>We shall say the moment generating function <em>exists</em> if <span class="math notranslate nohighlight">\(\psi(t)\)</span> is finite for all <span class="math notranslate nohighlight">\(t\)</span> in an open interval containing <span class="math notranslate nohighlight">\(t=0\)</span>.</p>
</section>
</div><p>The reason that the function <span class="math notranslate nohighlight">\(\psi(t)\)</span> is said to <em>generate</em> the moments is encapsulated in:</p>
<div class="proof theorem admonition" id="derivatives-mgf-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.8 </span> (Derivatives of Moment Generating Functions)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable whose moment generating function <span class="math notranslate nohighlight">\(\psi(t)\)</span> exists. Then the moments <span class="math notranslate nohighlight">\(E(X^k)\)</span> are finite for all <span class="math notranslate nohighlight">\(k\geq 1\)</span>, and <span class="math notranslate nohighlight">\(\psi^{(k)}(0) = E(X^k)\)</span>.</p>
</section>
</div><p>Thus, the moments <span class="math notranslate nohighlight">\(E(X^k)\)</span> may be extracted from the moment generating function <span class="math notranslate nohighlight">\(\psi(t)\)</span> simply by taking derivatives and evaluating at <span class="math notranslate nohighlight">\(t=0\)</span>. In this sense, the function <span class="math notranslate nohighlight">\(\psi (t)\)</span> <em>generates</em> the moments.</p>
<div class="proof admonition" id="proof">
<p>Proof. Let’s prove the theorem in the case that <span class="math notranslate nohighlight">\(X\)</span> is continuous with density <span class="math notranslate nohighlight">\(f(x)\)</span>. Supposing that all the moments are finite (which I will <em>not</em> prove; see instead Theorem 4.4.2 in <span id="id6">[<a class="reference internal" href="bib.html#id7" title="M. H. DeGroot and M. J. Schervish. Probability and statistics. Volume 563. Pearson Education London, UK, 2014.">DS14</a>]</span>) and that we can pull derivatives under integral signs, for all <span class="math notranslate nohighlight">\(k\geq 1\)</span> we have:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\psi^{(k)}(0) &amp;= \frac{\text{d}^k}{\text{d}t^k} \int_\bbr e^{tx} f(x) \ \text{d}x \Bigg|_{t=0} \\
&amp;= \int_\bbr \frac{\partial^k}{\partial t^k} e^{tx} f(x) \ \text{d} x \Bigg|_{t=0} \\
&amp;= \int_\bbr x^k e^{tx} f(x) \ \text{d} x \Bigg|_{t=9} \\
&amp;= \int_\bbr x^k f(x) \ \text{d} x \\
&amp;= E(X^k)
\end{align*}\]</div>
<p>Notice that we used the LotUS in the first line. Q.E.D.</p>
</div>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 8 on the worksheet.</p>
</div>
<p>Now, the true power of moment generating functions comes from the following extremely important and useful theorem, which may be seen as an analog of the Taylor Series Uniqueness Theorem stated above as <a class="reference internal" href="#taylor-thm">Theorem 8.7</a>. It essentially says that: <em>If you know all the moments, then you know the distribution.</em></p>
<div class="proof theorem admonition" id="mgf-uniqueness-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.9 </span> (Moment Generating Function Uniqueness Theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are two random variables whose moment generating functions <span class="math notranslate nohighlight">\(\psi_X(t)\)</span> and <span class="math notranslate nohighlight">\(\psi_Y(t)\)</span> exist. Then the following statements are equivalent:</p>
<ol class="arabic simple">
<li><p>The distributions of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are equal</p></li>
<li><p><span class="math notranslate nohighlight">\(E(X^k) = E(Y^k)\)</span> for all <span class="math notranslate nohighlight">\(k\geq 1\)</span>.</p></li>
<li><p>The moment generating functions <span class="math notranslate nohighlight">\(\psi_X(t)\)</span> and <span class="math notranslate nohighlight">\(\psi_Y(t)\)</span> are equal for all <span class="math notranslate nohighlight">\(t\)</span> in an open interval containing <span class="math notranslate nohighlight">\(t=0\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Here is a very brief sketch of the proof: The implication <span class="math notranslate nohighlight">\((1) \Rightarrow (2)\)</span> follows from the fact that distributions determine moments. The implication <span class="math notranslate nohighlight">\((3) \Rightarrow (2)\)</span> follows from the observation that if <span class="math notranslate nohighlight">\(\psi_X(t) = \psi_Y(t)\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> near <span class="math notranslate nohighlight">\(0\)</span>, then certainly</p>
<div class="math notranslate nohighlight">
\[
E(X^k) = \psi^{(k)}_X(0) = \psi_Y^{(k)}(0) = E(Y^k)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k\geq 1\)</span>. Assuming that <span class="math notranslate nohighlight">\(X\)</span> is continuous with density <span class="math notranslate nohighlight">\(f(x)\)</span>, the implication <span class="math notranslate nohighlight">\((2) \Rightarrow (3)\)</span> follows from the computations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\psi_X(t) &amp;= \int_\bbr e^{xt} f(x) \ \text{d} x \\
&amp;= \int_\bbr \left( \sum_{k=0}^\infty \frac{(tx)^k}{k!} \right) f(x) \ \text{d} x \\
&amp;= \sum_{k=0}^\infty  \left( \int_\bbr x^k f(x) \ \text{d} x \right)\frac{t^k}{k!} \\
&amp;= \sum_{k=0}^\infty \frac{E(X^k)}{k!} t^k,
\end{align*}\]</div>
<p>and similarly <span class="math notranslate nohighlight">\(\psi_Y(t) = \sum_{k=0}^\infty \frac{E(Y^k)}{k!} t^k\)</span>. Thus, if the moments of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are all equal, then so too <span class="math notranslate nohighlight">\(\psi_X(t)\)</span> and <span class="math notranslate nohighlight">\(\psi_Y(t)\)</span> are equal, at least near <span class="math notranslate nohighlight">\(t=0\)</span>.</p>
<p>Then, the <em>hard</em> part of the proof is showing that <span class="math notranslate nohighlight">\((2) \Rightarrow (1)\)</span> (or <span class="math notranslate nohighlight">\((3) \Rightarrow (1)\)</span>). <em>This</em>, unfortunately, we cannot do in this course, since it uses some <a class="reference external" href="https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)">rather sophisticated things</a>. In any case, we declare: Q.E.D.</p>
</div>
<p>To illustrate how this theorem may be used, we will establish the fundamental fact that sums of independent normal random variables are normal; this is stated officially in the second part of the following theorem.</p>
<div class="proof theorem admonition" id="mgf-norm-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.10 </span> (Moment generating functions of normal variables)</p>
<section class="theorem-content" id="proof-content">
<ol class="arabic">
<li><p>If <span class="math notranslate nohighlight">\(X \sim \calN(\mu,\sigma^2)\)</span>, then its moment generating function is given by</p>
<div class="math notranslate nohighlight">
\[
    \psi(t) = \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]
    \]</div>
<p>for all <span class="math notranslate nohighlight">\(t\in \bbr\)</span>.</p>
</li>
<li><p>Let <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> be independent random variables such that <span class="math notranslate nohighlight">\(X_i \sim \calN(\mu_i,\sigma^2_i)\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>, and let <span class="math notranslate nohighlight">\(a_1,\ldots,a_m,b\)</span> be scalars. Then the affine linear combination</p>
<div class="math notranslate nohighlight">
\[
    Y = a_1X_1 + \cdots + a_mX_m + b
    \]</div>
<p>is normal with mean <span class="math notranslate nohighlight">\(\mu = a_1\mu_1+\cdots + a_m\mu_m+b\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 = a_1^2\sigma_1^2 + \cdots + a_m^2\sigma_m^2\)</span>.</p>
</li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We will only prove the second part; for the proof of the first (which isn’t hard), see Theorem 5.6.2 in <span id="id7">[<a class="reference internal" href="bib.html#id7" title="M. H. DeGroot and M. J. Schervish. Probability and statistics. Volume 563. Pearson Education London, UK, 2014.">DS14</a>]</span>. So, we begin our computations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\psi_Y(t) &amp;= E\big( e^{ta_1X_1} \cdots e^{ta_mX_m} e^{tb}\big) \\
&amp;= E(e^{ta_1X_1}) \cdots E(e^{ta_mX_m})e^{tb} \\
&amp;= \left[\prod_{i=1}^m \exp \left( a_i\mu_i t + \frac{1}{2} a_i^2\sigma_i^2 t^2 \right)\right]e^{tb} \\
&amp;= \exp \left[ \left(\sum_{i=1}^m a_i\mu_i + b\right) t + \frac{1}{2} \left( \sum_{i=1}^m a_i^2\sigma_i^2\right) t^2 \right].
\end{align*}\]</div>
<p>The second line follows from the first by independence, <a class="reference internal" href="07-random-vectors.html#invar-independent-thm">Theorem 7.8</a>, and <a class="reference internal" href="#ind-expect-thm">Theorem 8.2</a>, while we obtain the third line from the first part of this theorem and <a class="reference internal" href="05-examples-of-rvs.html#affine-gaussian-thm">Theorem 5.7</a>. But notice that the expression on the last line is exactly the moment generating function of an <span class="math notranslate nohighlight">\(\calN(\mu,\sigma^2)\)</span> random variable (by the first part), where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are as in the statement of the theorem. By <a class="reference internal" href="#mgf-uniqueness-thm">Theorem 8.9</a>, it follows that <span class="math notranslate nohighlight">\(Y \sim \calN(\mu,\sigma^2)\)</span> as well. Q.E.D.</p>
</div>
<p>Before turning to the worksheet to finish this section, it is worth extracting one of the steps used in the previous proof and putting it in its own theorem:</p>
<div class="proof theorem admonition" id="mgf-sum-eqn">
<p class="admonition-title"><span class="caption-number">Theorem 8.11 </span> (Moment generating functions of sums of independent variables)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose that <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> are independent random variables with moment generating functions <span class="math notranslate nohighlight">\(\psi_i(t)\)</span> and let <span class="math notranslate nohighlight">\(Y = X_1 + \cdots + X_m\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\psi_Y(t) = \psi_1(t) \cdots \psi_m(t)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\)</span> such that each <span class="math notranslate nohighlight">\(\psi_i(t)\)</span> is finite.</p>
</section>
</div><p>Now, let’s do an example:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 9 on the worksheet.</p>
</div>
</section>
<section id="dependent-random-variables-covariance-and-correlation">
<span id="covar-correl-sec"></span><h2><span class="section-number">8.5. </span>Dependent random variables, covariance, and correlation<a class="headerlink" href="#dependent-random-variables-covariance-and-correlation" title="Link to this heading">#</a></h2>
<p>In this section, we begin our study of <em>dependent</em> random variables, which are just random variables that are not independent. This study will continue through <a class="reference internal" href="10-info-theory.html#cond-entropy-mutual-info-sec"><span class="std std-numref">Section 10.3</span></a> in the next chapter, and then culminate in <a class="reference internal" href="12-models.html#prob-models"><span class="std std-numref">Chapter 12</span></a> where we learn how to use “networks” of dependent random variables to model real-world data.</p>
<p>While the definition of dependence of random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is simply that they are <em>not independent</em>, it is helpful to conceptualize dependence as a flow of “information” or “influence” between them. If they are independent, then this flow vanishes and there is no transfer of “information”:</p>
<a class="reference internal image-reference" href="../_images/dep-flow.svg"><img alt="../_images/dep-flow.svg" class="align-center" src="../_images/dep-flow.svg" width="75%" /></a>
<p> </p>
<p>The most straightforward method to guarantee a transfer of “information” between the variables is to link them deterministically via a function <span class="math notranslate nohighlight">\(g:\bbr \to \bbr\)</span>, in the sense that <span class="math notranslate nohighlight">\(Y = g(X)\)</span>. This means that an observed value <span class="math notranslate nohighlight">\(x\)</span> (of <span class="math notranslate nohighlight">\(X\)</span>) <em>uniquely</em> determines an observed value <span class="math notranslate nohighlight">\(y=g(x)\)</span> (of <span class="math notranslate nohighlight">\(Y\)</span>). For random variables linked in this way, we prove what should be an intuitively obvious result:</p>
<div class="proof theorem admonition" id="functional-dep-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.12 </span> (Functional dependence <span class="math notranslate nohighlight">\(\Rightarrow\)</span> dependence)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables. If <span class="math notranslate nohighlight">\(Y = g(X)\)</span> for some function <span class="math notranslate nohighlight">\(g:\mathbb{R} \to \mathbb{R}\)</span>, then <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dependent.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. In order to prove this, we need to make the (mild) assumption that there is an event <span class="math notranslate nohighlight">\(B\subset \mathbb{R}\)</span> with</p>
<div class="math notranslate nohighlight" id="equation-middle-eqn">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-middle-eqn" title="Link to this equation">#</a></span>\[
0&lt;P(Y\in B)&lt;1.
\]</div>
<p>(This doesn’t <em>always</em> have to be true. For example, what happens if <span class="math notranslate nohighlight">\(g\)</span> is constant?) In this case, we set <span class="math notranslate nohighlight">\(A = g^{-1}(B)^c\)</span> and observe that</p>
<div class="math notranslate nohighlight">
\[
P(X\in A, \ Y\in B) = P(\emptyset) =0.
\]</div>
<p>On the other hand, we have</p>
<div class="math notranslate nohighlight">
\[
P(X\in A) = 1 - P(Y\in B),
\]</div>
<p>and so</p>
<div class="math notranslate nohighlight">
\[
P(X\in A) P(Y\in B) = (1 - P(Y\in B))P(Y\in B) \neq 0
\]</div>
<p>by <a class="reference internal" href="#equation-middle-eqn">(8.7)</a>. But then</p>
<div class="math notranslate nohighlight">
\[
P(X\in A, \ Y\in B) = 0 \neq P(X\in A) P(Y\in B),
\]</div>
<p>which proves <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dependent. Q.E.D.</p>
</div>
<p>What does a pair of functionally dependent random variables look like? For an example, let’s suppose that</p>
<div class="math notranslate nohighlight">
\[
X \sim \mathcal{N}(1,0.5^2) \quad \text{and} \quad Y = g(X) = X(X-1)(X-2).
\]</div>
<p>Then, let’s simulate a draw of 1000 samples from <span class="math notranslate nohighlight">\(X\)</span>, toss them into</p>
<div class="math notranslate nohighlight">
\[
g(x) = x(x-1)(x-2)
\]</div>
<p>to obtain the associated <span class="math notranslate nohighlight">\(y\)</span>-values, and then produce a scatter plot:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y=g(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/1f8ff93aefdbd1746c6e76f444016be2b29485e9ccf214cfeffe0a4fbc0df446.svg" src="../_images/1f8ff93aefdbd1746c6e76f444016be2b29485e9ccf214cfeffe0a4fbc0df446.svg" /></figure>
</div>
</div>
<p>The plot looks exactly like we would expect: A bunch of points lying on the graph of the function <span class="math notranslate nohighlight">\(y=g(x)\)</span>. However, very often with real-world data, an <em>exact</em> dependence <span class="math notranslate nohighlight">\(Y = g(X)\)</span> does not truly hold. Instead, the functional relationship is “noisy”, resulting in scatter plots that look like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y=g(x) + $noise&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FD46FC&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/002941229fc97acb7ecbdb8820657b7d4e0a2a5e2f32a7995287028e2cb53e67.svg" src="../_images/002941229fc97acb7ecbdb8820657b7d4e0a2a5e2f32a7995287028e2cb53e67.svg" /></figure>
</div>
</div>
<p>The “noisy” functional relationship is drawn in the left-hand plot, while on the right-hand plot I have superimposed the graph of the function <span class="math notranslate nohighlight">\(y=g(x)\)</span> for reference. Instead of lying directly on the graph of <span class="math notranslate nohighlight">\(y=g(x)\)</span>, the data is clustered along the graph.</p>
<p>The goal in this chapter is to study “noisy” <em>linear</em> dependencies between random variables; relationships that look like these:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">)</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">epsilon</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">m</span> <span class="o">*</span> <span class="n">grid</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FD46FC&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/d5d426a2b2ddc24de59100407ceabcd799a98cbbfa703cef9a44c67cec96ebcc.svg" src="../_images/d5d426a2b2ddc24de59100407ceabcd799a98cbbfa703cef9a44c67cec96ebcc.svg" /></figure>
</div>
</div>
<p>Our goal in this section is to uncover ways to <em>quantify</em> or <em>measure</em> the strength of these types of “noisy” linear dependencies between random variables. We will discover that there are two such measures, called <em>covariance</em> and <em>correlation</em>. An alternate measure of more general dependence, called <em>mutual information</em>, will be studied in the next chapter in <a class="reference internal" href="10-info-theory.html#cond-entropy-mutual-info-sec"><span class="std std-numref">Section 10.3</span></a>.</p>
<p>The definition of <em>covariance</em> is based on the following pair of basic observations:</p>
<blockquote>
<div><p>Let <span class="math notranslate nohighlight">\((x,y)\)</span> be an observation of a two-dimensional random vector <span class="math notranslate nohighlight">\((X,Y)\)</span></p>
<ol class="arabic simple">
<li><p>If the observed values of <span class="math notranslate nohighlight">\((X,Y)\)</span> cluster along a line of <em>positive</em> slope, then <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> tend to be large (small) at the same time.</p></li>
<li><p>If the observed values of <span class="math notranslate nohighlight">\((X,Y)\)</span> cluster along a line of <em>negative</em> slope, then a large (small) value <span class="math notranslate nohighlight">\(x\)</span> tends to be paired with a small (large) value <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ol>
</div></blockquote>
<p>The visualizations that go along with these observations are:</p>
<a class="reference internal image-reference" href="../_images/corr-01.svg"><img alt="../_images/corr-01.svg" class="align-center" src="../_images/corr-01.svg" width="75%" /></a>
<p> </p>
<p>In order to extract something useful from these observations, it is convenient to <em>center</em> the dataset by subtracting off the means:</p>
<div class="math notranslate nohighlight">
\[
X \xrightarrow{\text{replace with}} X - \mu_X \quad \text{and} \quad Y \xrightarrow{\text{replace with}} Y - \mu_Y.
\]</div>
<p>Notice that</p>
<div class="math notranslate nohighlight">
\[
E(X - \mu_X) = E(X) - E(\mu_X) = 0,
\]</div>
<p>and similarly <span class="math notranslate nohighlight">\(E(Y-\mu_Y) = 0\)</span>, so that when we carry out these replacements, we get random variables with mean <span class="math notranslate nohighlight">\(0\)</span>. Here’s an example:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;magenta&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;raw data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;centered data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/34d40ea52bc91cfaa65451b15a3d9c1a6bf6bb329d67704578cdf64783f80453.svg" src="../_images/34d40ea52bc91cfaa65451b15a3d9c1a6bf6bb329d67704578cdf64783f80453.svg" /></figure>
</div>
</div>
<p>You can see that the dataset has not changed its shape—it has only shifted so that its mean (represented by the magenta dot) is at the origin <span class="math notranslate nohighlight">\((0,0)\)</span>.</p>
<p>The reason that we center the data is that it allows us to conveniently rephrase our observations above in terms of signs:</p>
<blockquote>
<div><p>Let <span class="math notranslate nohighlight">\((x,y)\)</span> be an observation of a <strong>centered</strong> two-dimensional random vector <span class="math notranslate nohighlight">\((X,Y)\)</span></p>
<ol class="arabic simple">
<li><p>If the observed values of <span class="math notranslate nohighlight">\((X,Y)\)</span> cluster along a line of <em>positive</em> slope, then <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> tend to have the same sign, i.e., <span class="math notranslate nohighlight">\(xy&gt;0\)</span>.</p></li>
<li><p>If the observed values of <span class="math notranslate nohighlight">\((X,Y)\)</span> cluster along a line of <em>negative</em> slope, then <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> tend to have opposite signs, i.e., <span class="math notranslate nohighlight">\(xy&lt;0\)</span>.</p></li>
</ol>
</div></blockquote>
<p>The new visualization is:</p>
<a class="reference internal image-reference" href="../_images/corr-02.svg"><img alt="../_images/corr-02.svg" class="align-center" src="../_images/corr-02.svg" width="75%" /></a>
<p> </p>
<p>Essentially, the next definition takes the average value of the product <span class="math notranslate nohighlight">\(xy\)</span>, as <span class="math notranslate nohighlight">\((x,y)\)</span> ranges over observed pairs of values of a centered random vector <span class="math notranslate nohighlight">\((X,Y)\)</span>. If this average value is positive, it suggests a (noisy) linear dependence with positive slope; if it is negative, it suggests a (noisy) linear dependence with negative slope. If the random variables are not centered, then we subtract off their means before computing the product and taking its average value.</p>
<div class="proof definition admonition" id="covar-def">
<p class="admonition-title"><span class="caption-number">Definition 8.5 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables with expectations <span class="math notranslate nohighlight">\(\mu_X = E(X)\)</span> and <span class="math notranslate nohighlight">\(\mu_Y = E(Y)\)</span>. The <em>covariance</em> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, denoted either by <span class="math notranslate nohighlight">\(\sigma(X,Y)\)</span> or <span class="math notranslate nohighlight">\(\sigma_{XY}\)</span>, is defined via the equation</p>
<div class="math notranslate nohighlight">
\[
\sigma_{XY} = E \left[ (X-\mu_X)(Y-\mu_Y) \right].
\]</div>
</section>
</div><p>Notice that the covariance of a random variable <span class="math notranslate nohighlight">\(X\)</span> with itself is exactly its variance:</p>
<div class="math notranslate nohighlight">
\[
\sigma_{XX} = E \left[ (X-\mu_X)^2\right] = V(X).
\]</div>
<p>Before we look at an example, it will be convenient to state and prove the following generalization of <a class="reference internal" href="04-random-variables.html#shortcut-var-thm">Theorem 4.5</a>:</p>
<div class="proof theorem admonition" id="shortcut-covar-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.13 </span> (Shortcut Formula for Covariance)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables. Then</p>
<div class="math notranslate nohighlight">
\[
\sigma_{XY} = E(XY) - E(X) E(Y).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof is a triviality, given all the properties that we already know about expectations:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sigma_{XY} &amp;= E\left(XY - \mu_Y X - \mu_X Y + \mu_X \mu_Y \right) \\
&amp;= E(XY) - 2\mu_X\mu_Y + \mu_X\mu_Y \\
&amp;= E(XY) - E(X) E(Y).
\end{align*}\]</div>
</div>
<p>Armed with this formula, let’s do an example problem:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 10 on the worksheet.</p>
</div>
<p>A pair of very useful properties of covariance are listed in the following:</p>
<div class="proof theorem admonition" id="bilinear-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.14 </span> (Covariance <span class="math notranslate nohighlight">\(=\)</span> symmetric bilinear form)</p>
<section class="theorem-content" id="proof-content">
<ol class="arabic">
<li><p><em>Symmetry</em>. If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are random variables, then <span class="math notranslate nohighlight">\(\sigma_{XY} = \sigma_{YX}\)</span>.</p></li>
<li><p><em>Bilinearity</em>. Let <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> and <span class="math notranslate nohighlight">\(Y_1,\ldots,Y_n\)</span> be sequences of random variables, and <span class="math notranslate nohighlight">\(a_1,\ldots,a_m\)</span> and <span class="math notranslate nohighlight">\(b_1,\ldots,b_n\)</span> sequences of real numbers. Then:</p>
<div class="math notranslate nohighlight" id="equation-bilinear-eqn">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-bilinear-eqn" title="Link to this equation">#</a></span>\[
    \sigma\Big( \sum_{i=1}^m a_i X_i, \sum_{j=1}^n b_j Y_j\Big) = \sum_{i=1}^m \sum_{j=1}^n a_i b_j \sigma(X_i,Y_j).
    \]</div>
</li>
</ol>
</section>
</div><p>I suggest that you attempt to prove this theorem on your own. A special case appears in your <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/08-homework.md#problem-8-a-special-case-of-bilinearity">homework</a> for this chapter.</p>
<p>Bilinearity of covariance allows us to generalize <a class="reference internal" href="04-random-variables.html#var-affine-thm">Theorem 4.6</a> on the variance of an affine transformation of a random variable:</p>
<div class="proof theorem admonition" id="variance-lin-combo-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.15 </span> (Variance of a linear combination)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,\ldots,X_m\)</span> be a sequence of random variables and <span class="math notranslate nohighlight">\(a_1,\ldots,a_m\)</span> a sequence of real numbers. Then:</p>
<div class="math notranslate nohighlight">
\[
V(a_1X_1 + \cdots + a_m X_m) = \sum_{i=1}^m a_i^2 V(X_i) + 2\sum_{1 \leq i &lt; j \leq m }a_ia_j \sigma(X_i,X_j).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof is an application of bilinearity of covariance:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
V(a_1X_1 + \cdots + a_m X_m) &amp;= \sigma\Big(\sum_{i=1}^m a_i X_i, \sum_{j=1}^m a_j X_j \Big)  \\
&amp;= \sum_{i,j=1}^m a_i a_j \sigma(X_i,X_j) \\
&amp;= \sum_{i=1}^m a_i^2 V(X_i) + 2\sum_{1 \leq i &lt; j \leq m }a_ia_j \sigma(X_i,X_j).
\end{align*}\]</div>
</div>
<p>In particular, we see that if <span class="math notranslate nohighlight">\(\sigma(X_i,X_j)=0\)</span> for all <span class="math notranslate nohighlight">\(i\neq j\)</span> (i.e., if the random variables are pairwise <em>uncorrelated</em>; see below), the formula simplifies to:</p>
<div class="math notranslate nohighlight">
\[
V(a_1X_1 + \cdots + a_m X_m) = \sum_{i=1}^m a_i^2 V(X_i).
\]</div>
<p>We now turn toward the other measure of linear dependence, called <em>correlation</em>. To motivate this latter measure, we note that while the signs of covariances are significant, their precise numerical values may be less so, if we are attempting to use them to measure the strength of a linear dependence. One reason for this is that covariances are sensitive to the scales on which the variables are measured. For an example, consider the following two plots:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">epsilon</span>
<span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;raw data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">10</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">10</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;scaled data&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/ea8d9bb65777bf142a940a05856503f0cb917cd2a4b22775c7b3b15856163242.svg" src="../_images/ea8d9bb65777bf142a940a05856503f0cb917cd2a4b22775c7b3b15856163242.svg" /></figure>
</div>
</div>
<p>The only difference between the two datasets is the axis scales, but the linear relationship between the <span class="math notranslate nohighlight">\(x\)</span>’s and <span class="math notranslate nohighlight">\(y\)</span>’s remains the same. In fact, the scales on the axes differ by a factor of <span class="math notranslate nohighlight">\(10\)</span>, so by <a class="reference internal" href="#bilinear-thm">Theorem 8.14</a>, we have</p>
<div class="math notranslate nohighlight">
\[
\sigma(10X, 10Y) = 100 \sigma(X,Y).
\]</div>
<p>Thus, if we use the numerical value of the covariance to indicate the <em>strength</em> of a linear relationship, then we should conclude that the data on right-hand side is <em>one hundred times</em> more “linearly correlated” than the data on the left. But this is nonsense!</p>
<p>The remedy is to define a “normalized” measure of linear dependence:</p>
<div class="proof definition admonition" id="correlation-def">
<p class="admonition-title"><span class="caption-number">Definition 8.6 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be two random variables. The <em>correlation</em> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, denoted by either <span class="math notranslate nohighlight">\(\rho(X,Y)\)</span> or <span class="math notranslate nohighlight">\(\rho_{XY}\)</span>, is defined via the equation</p>
<div class="math notranslate nohighlight">
\[
\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}.
\]</div>
</section>
</div><p>The correlation <span class="math notranslate nohighlight">\(\rho_{XY}\)</span> is also sometimes called the <a class="reference external" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson (product-moment) correlation coefficient</a>, to distinguish it from other types of correlation measures. Its key properties are given in the following:</p>
<div class="proof theorem admonition" id="prop-correlation-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.16 </span> (Properties of correlation)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables.</p>
<ol class="arabic">
<li><p><em>Symmetry</em>. We have <span class="math notranslate nohighlight">\(\rho_{XY} = \rho_{YX}\)</span>.</p></li>
<li><p><em>Scale invariance</em>. If <span class="math notranslate nohighlight">\(a\)</span> is a nonzero real number, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \rho(aX, Y) = \begin{cases}
    \rho(X, Y) &amp; : a&gt;0, \\
    -\rho(X, Y) &amp; : a &lt;0.
    \end{cases}
    \end{split}\]</div>
</li>
<li><p><em>Normalization</em>. We have <span class="math notranslate nohighlight">\(|\rho(X,Y)| \leq 1\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The symmetry property of correlation follows from the same property of covariance in <a class="reference internal" href="#bilinear-thm">Theorem 8.14</a>. Scale invariance follows from bilinearity of covariance, as well as the equality <span class="math notranslate nohighlight">\(\sigma_{aX} = |a| \sigma_X\)</span> established in <a class="reference internal" href="04-random-variables.html#var-affine-thm">Theorem 4.6</a> (or its generalization <a class="reference internal" href="#variance-lin-combo-thm">Theorem 8.15</a>). The proof of normalization is a bit more involved but still not very difficult. It requires the Cauchy-Schwarz inequality; see the proof in Section 4.6 of <span id="id8">[<a class="reference internal" href="bib.html#id7" title="M. H. DeGroot and M. J. Schervish. Probability and statistics. Volume 563. Pearson Education London, UK, 2014.">DS14</a>]</span>, for example. Q.E.D.</p>
</div>
<p>Remember, covariance and correlation were cooked up to measure linear dependencies between random variables. We wonder, then, what is the correlation between two random variables that are <em>perfectly</em> linearly dependent? Answer:</p>
<div class="proof theorem admonition" id="linearity-correlation-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.17 </span> (Correlation of linearly dependent random variables)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable, let <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> constants with <span class="math notranslate nohighlight">\(a\neq 0\)</span>, and set <span class="math notranslate nohighlight">\(Y = aX+b\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\rho(X,Y) = \begin{cases}
1 &amp; : a&gt;0, \\
-1 &amp; : a &lt; 0.
\end{cases}
\end{split}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof is a simple computation, similar to the proof of scale invariance from above:</p>
<div class="math notranslate nohighlight">
\[
\rho(X,Y) = \frac{a\sigma(X,X)+\sigma(X,b)}{\sigma_X\sigma_{aX+b}} = \frac{a V(X)}{\sqrt{V(X)}\sqrt{a^2V(X)}} = \frac{a}{|a|}.
\]</div>
</div>
<p>We give a name to two random variables whose correlation is zero:</p>
<div class="proof definition admonition" id="uncorrelated-def">
<p class="admonition-title"><span class="caption-number">Definition 8.7 </span></p>
<section class="definition-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are two random variables with <span class="math notranslate nohighlight">\(\rho(X,Y)=0\)</span>, then we say <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <em>uncorrelated</em>. Otherwise, they are said to be <em>(linearly) correlated</em>.</p>
</section>
</div><p>Let’s take a look at an example before continuing:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 11 in the worksheet.</p>
</div>
<p>You should think of independence as a strong form of non-correlation, and hence correlation is also a strong form of dependence. This is the content of the first part of the following result:</p>
<div class="proof theorem admonition" id="ind-vs-correlation-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.18 </span> (Dependence and correlation)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables.</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, then they are uncorrelated.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are correlated, then they are dependent.</p></li>
<li><p>However, there exist dependent <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> that are uncorrelated.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof of the first statement is a simple application of <a class="reference internal" href="#ind-expect-thm">Theorem 8.2</a> and the Shortcut Formula for Covariance in <a class="reference internal" href="#shortcut-covar-thm">Theorem 8.13</a>. Indeed, we have</p>
<div class="math notranslate nohighlight">
\[
\sigma_{XY} = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X) E(Y) =0,
\]</div>
<p>and then <span class="math notranslate nohighlight">\(\rho_{XY} = \sigma_{XY} / (\sigma_X \sigma_Y) = 0\)</span>. The second statement is just the contrapositive of the first.</p>
<p>For the second statement, take two continuous random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with joint density</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x,y) = \begin{cases}
\frac{2}{\pi}(x^2+y^2) &amp; : x^2 + y^2 \leq 1, \\
0 &amp; : \text{otherwise}.
\end{cases}
\end{split}\]</div>
<p>By symmetry, we have <span class="math notranslate nohighlight">\(E(X) = E(Y) = E(XY)=0\)</span>, and hence <span class="math notranslate nohighlight">\(\rho_{XY}=0\)</span> as well. However, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are clearly depenendent since (for example) the support of <span class="math notranslate nohighlight">\(f(y|x)\)</span> depends on <span class="math notranslate nohighlight">\(x\)</span>. Q.E.D.</p>
</div>
<p>We have only considered a pair <span class="math notranslate nohighlight">\((X,Y)\)</span> of random variables; but what if we have a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span>? Then we may compute the pairwise covariances <span class="math notranslate nohighlight">\(\sigma(X_i,X_j)\)</span> and <span class="math notranslate nohighlight">\(\rho(X_i,X_j)\)</span>, and we can’t resist the urge to put them into matrices:</p>
<div class="proof definition admonition" id="covar-correl-matrix-def">
<p class="admonition-title"><span class="caption-number">Definition 8.8 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector.</p>
<ol class="arabic">
<li><p>We define the <em>covariance matrix</em> of <span class="math notranslate nohighlight">\(\bX\)</span> to be the <span class="math notranslate nohighlight">\(d\times d\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \bSigma_\bX \def \left[ \sigma_{ij} \right] = \begin{bmatrix}
    \sigma_{11} &amp; \cdots &amp; \sigma_{1d} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \sigma_{d1} &amp; \cdots &amp; \sigma_{dd}
    \end{bmatrix},
    \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_{ij} = \sigma(X_i,X_j)\)</span>.</p>
</li>
<li><p>We define the <em>correlation matrix</em> of <span class="math notranslate nohighlight">\(\bX\)</span> to be the <span class="math notranslate nohighlight">\(d\times d\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \bP_\bX \def \left[ \rho_{ij} \right] = \begin{bmatrix}
    \rho_{11} &amp; \cdots &amp; \rho_{1d} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \rho_{d1} &amp; \cdots &amp; \rho_{dd}
    \end{bmatrix},
    \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\rho_{ij} = \rho(X_i,X_j)\)</span>.</p>
</li>
</ol>
</section>
</div><p>Covariance matrices will prove especially important in the <a class="reference internal" href="#multivar-norm-sec"><span class="std std-ref">next section</span></a>.</p>
<p>Notice that both the covariance matrix and correlation matrix of a random vector are <em>symmetric</em>, meaning <span class="math notranslate nohighlight">\(\sigma_{ij} = \sigma_{ji}\)</span> and <span class="math notranslate nohighlight">\(\rho_{ij} = \rho_{ji}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. But symmetry is not their <em>only</em> special property; in fact, they are both examples of <em>positive semidefinite</em> matrices. These types of matrices will be important over the next few chapters, so let’s define them:</p>
<div class="proof definition admonition" id="first-semidefinite-def">
<p class="admonition-title"><span class="caption-number">Definition 8.9 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bA\)</span> be a symmetric matrix of size <span class="math notranslate nohighlight">\(d\times d\)</span>.</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(\bv^\intercal \bA \bv \geq 0\)</span> for all <span class="math notranslate nohighlight">\(\bv \in \bbr^d\)</span>, then <span class="math notranslate nohighlight">\(\bA\)</span> is called <em>positive semidefinite</em>. If the only vector for which equality holds is <span class="math notranslate nohighlight">\(\bv=\boldsymbol{0}\)</span>, then <span class="math notranslate nohighlight">\(\bA\)</span> is called <em>positive definite</em>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\bv^\intercal \bA \bv \leq 0\)</span> for all <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span>, then <span class="math notranslate nohighlight">\(\bA\)</span> is called <em>negative semidefinite</em>. If the only vector for which equality holds is <span class="math notranslate nohighlight">\(\bv=\boldsymbol{0}\)</span>, then <span class="math notranslate nohighlight">\(\bA\)</span> is called <em>negative definite</em>.</p></li>
</ol>
</section>
</div><p>It will be convenient to have alternate characterizations of definite and semidefinite matrices:</p>
<div class="proof theorem admonition" id="psd-char-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.19 </span></p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bA\)</span> be a symmetric matrix of size <span class="math notranslate nohighlight">\(d\times d\)</span>. Then:</p>
<ol class="arabic simple">
<li><p>The matrix <span class="math notranslate nohighlight">\(\bA\)</span> is positive semidefinite (definite) if and only if all its eigenvalues are nonnegative (positive).</p></li>
<li><p>The matrix <span class="math notranslate nohighlight">\(\bA\)</span> is positive semidefinite if and only if there is a positive semidefinite matrix <span class="math notranslate nohighlight">\(\bB\)</span> such that <span class="math notranslate nohighlight">\(\bA = \bB^2\)</span>. Moreover, the matrix <span class="math notranslate nohighlight">\(\bB\)</span> is the unique positive semidefinite matrix with this property.</p></li>
</ol>
</section>
</div><p>The matrix <span class="math notranslate nohighlight">\(\bB\)</span> in the second part is, for obvious reasons, called the <em>square root</em> of <span class="math notranslate nohighlight">\(\bA\)</span> and is often denoted <span class="math notranslate nohighlight">\(\bA^{1/2}\)</span>. The proof of this theorem is quite lengthy in comparison to the other proofs in this book. It is not essential reading, so you may safely skip it if you desire to get back quickly to the main thread of the section.</p>
<div class="proof admonition" id="proof">
<p>Proof. Implicit in both characterizations are the claims that the eigenvalues of <span class="math notranslate nohighlight">\(\bA\)</span> are real. To see this, let <span class="math notranslate nohighlight">\(\lambda\)</span> be an eigenvalue with eigenvector <span class="math notranslate nohighlight">\(\bv\)</span>. It is convenient to introduce the <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate transpose</a> <span class="math notranslate nohighlight">\((-)^\ast\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\lambda ||\bv||^2 = \lambda \bv^\ast \bv = \bv^\ast \bA \bv = (\bA \bv)^\ast \bv = (\lambda \bv)^\ast \bv = \bar{\lambda}\bv^\ast\bv = \bar{\lambda} ||\bv||^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\lambda}\)</span> denotes the complex conjugate of <span class="math notranslate nohighlight">\(\lambda\)</span>. Note that we used symmetry of <span class="math notranslate nohighlight">\(\bA\)</span> in the third equality. Since <span class="math notranslate nohighlight">\(\bv\)</span> is not zero, we may divide out the squared norm on the far ends of the sequence of equalities to obtain <span class="math notranslate nohighlight">\(\lambda = \bar{\lambda}\)</span>. But this shows <span class="math notranslate nohighlight">\(\lambda\)</span> is real.</p>
<p>Now, since <span class="math notranslate nohighlight">\(\bA\)</span> is symmetric, by the <a class="reference external" href="https://en.wikipedia.org/wiki/Spectral_theorem">Spectral Theorem</a> (see also Theorem 5.8 in Chapter 7 of <span id="id9">[<a class="reference internal" href="bib.html#id12" title="M. Artin. Algebra. Prentice Hall, 1991.">Art91</a>]</span>) there exists an orthogonal matrix <span class="math notranslate nohighlight">\(\bQ\)</span> and a diagonal matrix <span class="math notranslate nohighlight">\(\bD\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-diag-eq">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-diag-eq" title="Link to this equation">#</a></span>\[
\bA = \bQ \bD \bQ^\intercal.
\]</div>
<p>The columns of <span class="math notranslate nohighlight">\(\bQ\)</span> are necessarily eigenvectors <span class="math notranslate nohighlight">\(\bv_1,\ldots,\bv_d\)</span> of <span class="math notranslate nohighlight">\(\bA\)</span> while the main diagonal entries of <span class="math notranslate nohighlight">\(\bD\)</span> are the eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_d\)</span>, with <span class="math notranslate nohighlight">\(\bA \bv_i = \lambda_i \bv_i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>. If for each <span class="math notranslate nohighlight">\(i=1,\ldots,d\)</span> we write <span class="math notranslate nohighlight">\(\bE_i\)</span> for the diagonal matrix with a <span class="math notranslate nohighlight">\(1\)</span> in the <span class="math notranslate nohighlight">\((i,i)\)</span>-th position and zeros elsewhere, then <a class="reference internal" href="#equation-diag-eq">(8.9)</a> may be rewritten as</p>
<div class="math notranslate nohighlight" id="equation-spectral-decomp-eq">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-spectral-decomp-eq" title="Link to this equation">#</a></span>\[
\bA = \lambda_1 \bP_1 + \cdots + \lambda_d \bP_d
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bP_i = \bQ \bE_i \bQ^\intercal\)</span>. This is the so-called <em>spectral decomposition</em> of <span class="math notranslate nohighlight">\(\bA\)</span> into a linear combination of rank-<span class="math notranslate nohighlight">\(1\)</span> projection matrices. Note that <span class="math notranslate nohighlight">\(\bP_i\bP_j = \boldsymbol{0}\)</span> if <span class="math notranslate nohighlight">\(i\neq j\)</span>, <span class="math notranslate nohighlight">\(\bP_i^2 = \bP_i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>, and <span class="math notranslate nohighlight">\(\bP_iv_i = v_i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>The eigenvectors of <span class="math notranslate nohighlight">\(\bA\)</span> form an orthonormal basis of <span class="math notranslate nohighlight">\(\bbr^d\)</span> since <span class="math notranslate nohighlight">\(\bQ\)</span> is orthongal. So, given <span class="math notranslate nohighlight">\(\bv\in \bbr^d\)</span>, we may write <span class="math notranslate nohighlight">\(\bv = b_1 \bv_1 + \cdots + b_d \bv_d\)</span> for some scalars <span class="math notranslate nohighlight">\(b_1,\ldots,b_d\)</span>. Then, from <a class="reference internal" href="#equation-spectral-decomp-eq">(8.10)</a>, we get</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal\bA\bv = b_1^2 \lambda_1 + \cdots + b_d^2 \lambda_d.
\]</div>
<p>The characterizations in the first statement of the theorem follow immediately.</p>
<p>Turning toward the second statement, suppose that <span class="math notranslate nohighlight">\(\bA\)</span> is positive semidefinite. We then define</p>
<div class="math notranslate nohighlight">
\[
\bB = \sqrt{\lambda_1} P_1 + \cdots + \sqrt{\lambda_n}P_n.
\]</div>
<p>Notice that the square roots are real, since the eigenvalues are (real) nonnegative. You may easily check that <span class="math notranslate nohighlight">\(\bA = \bB^2\)</span>, and that <span class="math notranslate nohighlight">\(\bB\)</span> is positive semidefinite. I will leave you to prove the converse, that if such a <span class="math notranslate nohighlight">\(\bB\)</span> exists, then <span class="math notranslate nohighlight">\(\bA\)</span> is necessarily positive semidefinite. We will not address uniqueness of <span class="math notranslate nohighlight">\(\bB\)</span> here; for that, see, for example, Theorem 7.2.6 in <span id="id10">[<a class="reference internal" href="bib.html#id22" title="R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge University Press, 1985.">HJ85</a>]</span>. Q.E.D.</p>
</div>
<p>We are now ready to prove the main result regarding covariance and correlation matrices:</p>
<div class="proof theorem admonition" id="covar-matrix-prop-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.20 </span> (Covariance and correlation matrices are positive semidefinite)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector. Both the covariance matrix <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span> and the correlation matrix <span class="math notranslate nohighlight">\(\bP_\bX\)</span> are positive semidefinite. They are positive definite if and only if their determinants are nonzero.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We will only prove that the correlation matrix <span class="math notranslate nohighlight">\(\bP = \bP_\bX\)</span> is positive semidefinite, leaving you to make the easy changes to cover the covariance matrix. First, we have already observed that <span class="math notranslate nohighlight">\(\bP\)</span> is symmetric. Then, given <span class="math notranslate nohighlight">\(\bv\in \bbr^d\)</span>, we use <a class="reference internal" href="#bilinear-thm">Theorem 8.14</a> to compute:</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal \bP \bv = \sum_{i,j=1}^d \frac{v_i}{\sigma_i} \frac{v_j}{\sigma_j} \sigma(X_i,X_j) = \sigma \left(\sum_{i=1}^d \frac{v_i}{\sigma_i}X_i, \sum_{j=1}^d \frac{v_j}{\sigma_j} X_j \right) = V\left( \sum_{j=1}^d \frac{v_j}{\sigma_j} X_j \right) \geq 0.
\]</div>
<p>This establishes that <span class="math notranslate nohighlight">\(\bP\)</span> is positive semidefinite. To see that a nonzero determinant is equivalent to positive definiteness, recall that the determinant is the product of the eigenvalues. Then use the characterization of positive definite matrices given in <a class="reference internal" href="#psd-char-thm">Theorem 8.19</a>. Q.E.D.</p>
</div>
</section>
<section id="multivariate-normal-distributions">
<span id="multivar-norm-sec"></span><h2><span class="section-number">8.6. </span>Multivariate normal distributions<a class="headerlink" href="#multivariate-normal-distributions" title="Link to this heading">#</a></h2>
<p>The goal in this section is simple: Generalize the univariate normal distributions from <a class="reference internal" href="05-examples-of-rvs.html#norm-univariate-sec"><span class="std std-numref">Section 5.6</span></a> to higher dimensions. We needed to wait until the current chapter to do this because our generalization will require the machinery of covariance matrices that we developed at the end of the previous section. The ultimate effect will be that the familiar “bell curves” of univariate normal densities will turn into “bell (hyper)surfaces.” For example, in two dimensions, the density surface of a bivariate normal random vector might look something like this:</p>
<a class="reference internal image-reference" href="../_images/multi-var-norm-01.png"><img alt="../_images/multi-var-norm-01.png" class="align-center" src="../_images/multi-var-norm-01.png" style="width: 80%;" /></a>
<p> </p>
<p>The <em>isoprobability contours</em> of this density surface (i.e., curves of constant probability) look like:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rho</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">sigma1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7c9d8618e67b337ce60e6572dbcfa3dab8389d3871f40035f7ec3c6ba398e4ac.svg" src="../_images/7c9d8618e67b337ce60e6572dbcfa3dab8389d3871f40035f7ec3c6ba398e4ac.svg" /></figure>
</div>
</div>
<p>Notice that these contours are concentric ellipses centered at the point <span class="math notranslate nohighlight">\((1,1)\in \bbr^2\)</span> with principal axes parallel to the coordinate <span class="math notranslate nohighlight">\(x\)</span>- and <span class="math notranslate nohighlight">\(y\)</span>-axes. In higher dimensions, the isoprobability surfaces are so-called <em>ellipsoids</em>.</p>
<p>Actually, if we consider only this special case, i.e., when the principal axes of the isoprobability surfaces are parallel with the coordinate axes—we do not need the full strength of the machinery of covariance matrices. Studying this case will also help us gain insight into the general formula for the density of a multivariate normal distribution. So, let’s begin with a sequence <span class="math notranslate nohighlight">\(X_1,\ldots,X_d\)</span> of <em>independent</em>(!) random variables such that <span class="math notranslate nohighlight">\(X_i \sim \calN(\mu_i,\sigma_i^2)\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>. Then independence gives us</p>
<div class="math notranslate nohighlight">
\[
f(x_1,\ldots,x_d) = \prod_{i=1}^d \frac{1}{\sqrt{2\pi \sigma_i^2}} \exp \left[ -\frac{1}{2\sigma_i^2} ( x_i-\mu_i)^2\right],
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-init-multi-var-norm-eqn">
<span class="eqno">(8.11)<a class="headerlink" href="#equation-init-multi-var-norm-eqn" title="Link to this equation">#</a></span>\[
f(x_1,\ldots,x_d) = \frac{1}{(2\pi)^{d/2} (\sigma_1^2\cdots \sigma_d^2)^{1/2}} \exp \left[- \frac{1}{2} \sum_{i=1}^d \frac{(x_i-\mu_i)^2}{\sigma_i^2} \right].
\]</div>
<p>As you may easily check by creating your own plots, this formula (in dimension <span class="math notranslate nohighlight">\(d=2\)</span>) produces “bell surfaces” like the one shown above whose contours are concentric ellipses centered at <span class="math notranslate nohighlight">\((\mu_1,\mu_2)\in \bbr^2\)</span> with their principal axes parallel with the coordinate axes. However, we shall also be interested in creating density surfaces whose isoprobability contours are rotated ellipses, like the following:</p>
<a class="reference internal image-reference" href="../_images/multi-var-norm-02.png"><img alt="../_images/multi-var-norm-02.png" class="align-center" src="../_images/multi-var-norm-02.png" style="width: 80%;" /></a>
<p> </p>
<p>Indeed, the contours of this surface are given by:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">sigma1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/ed8211aee83bb8ca19436306ec44fc7046238a6918c88a08d85d5508e4418b17.svg" src="../_images/ed8211aee83bb8ca19436306ec44fc7046238a6918c88a08d85d5508e4418b17.svg" /></figure>
</div>
</div>
<p>The key to uncovering the formula for the density in the general case begins by returning to the formula <a class="reference internal" href="#equation-init-multi-var-norm-eqn">(8.11)</a> and rewriting it using vector and matrix notation. Indeed, notice that if we set</p>
<div class="math notranslate nohighlight">
\[
\bx^\intercal = (x_1,\ldots,x_d) \quad \text{and} \quad \bmu^\intercal = (\mu_1,\ldots,\mu_d)
\]</div>
<p>and let <span class="math notranslate nohighlight">\(\bSigma\)</span> be the covariance matrix of the <span class="math notranslate nohighlight">\(X_i\)</span>’s, then we have</p>
<div class="math notranslate nohighlight">
\[
f(\bx) = \frac{1}{\det(2\pi\bSigma)^{1/2}} \exp \left[ -\frac{1}{2} (\bx - \bmu)^\intercal \bSigma^{-1}(\bx - \bmu)\right],
\]</div>
<p>where <span class="math notranslate nohighlight">\(\det{(2\pi\bSigma)} = (2\pi)^d \det{\bSigma}\)</span>. Indeed, by independence, the covariance matrix <span class="math notranslate nohighlight">\(\bSigma\)</span> is diagonal, with the variances <span class="math notranslate nohighlight">\(\sigma_1^2,\ldots,\sigma_d^2\)</span> along its main diagonal. We get the general formula for the density of a multivariate normal distribution by simply letting the covariance matrix <span class="math notranslate nohighlight">\(\bSigma\)</span> be a general positive definite matrix:</p>
<div class="proof definition admonition" id="multivar-norm-def">
<p class="admonition-title"><span class="caption-number">Definition 8.10 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bmu \in \bbr^d\)</span> and let <span class="math notranslate nohighlight">\(\bSigma\in \bbr^{d\times d}\)</span> be a positive definite matrix. A continuous <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector <span class="math notranslate nohighlight">\(\bX\)</span> is said to have a <em>multivariate normal distribution</em> with parameters <span class="math notranslate nohighlight">\(\bmu\)</span> and <span class="math notranslate nohighlight">\(\bSigma\)</span>, denoted</p>
<div class="math notranslate nohighlight">
\[
\bX \sim \calN_d(\bmu,\bSigma),
\]</div>
<p>if its probability density function is given by</p>
<div class="math notranslate nohighlight" id="equation-multivar-norm-eqn">
<span class="eqno">(8.12)<a class="headerlink" href="#equation-multivar-norm-eqn" title="Link to this equation">#</a></span>\[
f(\bx;\bmu,\bSigma) = \frac{1}{\det(2\pi\bSigma)^{1/2}} \exp \left[ -\frac{1}{2} (\bx - \bmu)^\intercal \bSigma^{-1}(\bx - \bmu)\right]
\]</div>
<p>with support <span class="math notranslate nohighlight">\(\bbr^d\)</span>. The <em>standard (<span class="math notranslate nohighlight">\(d\)</span>-dimensional) multivariate normal distribution</em> corresponds to the case that <span class="math notranslate nohighlight">\(\bmu^\intercal = \boldsymbol{0}\)</span> and <span class="math notranslate nohighlight">\(\bSigma = \mathbf{I}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{0}\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-dimensional zero vector and <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the <span class="math notranslate nohighlight">\(d\times d\)</span> identity matrix.</p>
</section>
</div><p>Since <span class="math notranslate nohighlight">\(\bSigma\)</span> is positive definite and its determinant is the product of its eigenvalues, we must have <span class="math notranslate nohighlight">\(\det(2\pi\bSigma) = (2\pi)^d \det{\bSigma} &gt;0\)</span> by <a class="reference internal" href="#psd-char-thm">Theorem 8.19</a>. Thus, the square root <span class="math notranslate nohighlight">\(\det(2\pi\bSigma)^{1/2}\)</span> is real and positive, and the inverse <span class="math notranslate nohighlight">\(\bSigma^{-1}\)</span> exists.</p>
<p>To help understand the shape of the density (hyper)surfaces created by <a class="reference internal" href="#equation-multivar-norm-eqn">(8.12)</a>, it helps to ignore the normalizing constant and write</p>
<div class="math notranslate nohighlight" id="equation-prop-exp-density-eq">
<span class="eqno">(8.13)<a class="headerlink" href="#equation-prop-exp-density-eq" title="Link to this equation">#</a></span>\[
f(\bx;\bmu,\bSigma) \propto \exp \left[ -\frac{1}{2} Q(\bx)\right],
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q(\bx)\)</span> is the (inhomogeneous) degree-<span class="math notranslate nohighlight">\(2\)</span> polynomial</p>
<div class="math notranslate nohighlight">
\[
Q(\bx) = (\bx - \bmu)^\intercal \bSigma^{-1}(\bx - \bmu).
\]</div>
<p>Note that since <span class="math notranslate nohighlight">\(\bSigma\)</span> is positive definite, so too is its inverse <span class="math notranslate nohighlight">\(\bSigma^{-1}\)</span> since it is symmetric and has real, positive eigenvalues (can you prove this?). Thus, in the argument to the exponential function in <a class="reference internal" href="#equation-prop-exp-density-eq">(8.13)</a>, we have <span class="math notranslate nohighlight">\(-\frac{1}{2}Q(\bmu)=0\)</span>, while <span class="math notranslate nohighlight">\(-\frac{1}{2}Q(\bx) &lt; 0\)</span> for all other vectors <span class="math notranslate nohighlight">\(\bx \in \bbr^d\)</span> different from <span class="math notranslate nohighlight">\(\bmu\)</span>. But since the exponential function <span class="math notranslate nohighlight">\(z\mapsto e^z\)</span> is strictly increasing over the interval <span class="math notranslate nohighlight">\((-\infty,0]\)</span> with a global maximum at <span class="math notranslate nohighlight">\(z=0\)</span>, we see that the density <span class="math notranslate nohighlight">\(f(\bx;\bmu,\bSigma)\)</span> has a global maximum at <span class="math notranslate nohighlight">\(\bx = \bmu\)</span>.</p>
<p>The isoprobability contours of <span class="math notranslate nohighlight">\(f(\bx;\bmu,\bSigma)\)</span> are the sets of solutions <span class="math notranslate nohighlight">\(\bx\)</span> to equations of the form</p>
<div class="math notranslate nohighlight">
\[
\exp \left[ -\frac{1}{2} Q(\bx)\right] = c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> is a constant with <span class="math notranslate nohighlight">\(0 &lt; c \leq 1\)</span>. Or, equivalently, they are the sets of solutions to equations of the form</p>
<div class="math notranslate nohighlight" id="equation-mahalanobis-sphere-eq">
<span class="eqno">(8.14)<a class="headerlink" href="#equation-mahalanobis-sphere-eq" title="Link to this equation">#</a></span>\[
Q(\bx) = c^2,
\]</div>
<p>where now <span class="math notranslate nohighlight">\(c\)</span> is any real constant. Suppose that we then define the scalar product</p>
<div class="math notranslate nohighlight" id="equation-inner-prod-eqn">
<span class="eqno">(8.15)<a class="headerlink" href="#equation-inner-prod-eqn" title="Link to this equation">#</a></span>\[
\left\langle \bx, \by \right\rangle_\bSigma = \bx^\intercal \bSigma^{-1}\by, \quad \bx,\by\in \bbr^d.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\bSigma^{-1}\)</span> is positive definite, this is in fact an <a class="reference external" href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a> on <span class="math notranslate nohighlight">\(\bbr^d\)</span>. The associated norm is given by</p>
<div class="math notranslate nohighlight" id="equation-normed-eqn">
<span class="eqno">(8.16)<a class="headerlink" href="#equation-normed-eqn" title="Link to this equation">#</a></span>\[
||\bx||^2_\bSigma = \bx^\intercal \bSigma^{-1} \bx,
\]</div>
<p>and hence the square root <span class="math notranslate nohighlight">\(\sqrt{Q(\bx)} = ||\bx - \bmu||_\bSigma\)</span> is the distance from <span class="math notranslate nohighlight">\(\bx\)</span> to <span class="math notranslate nohighlight">\(\bmu\)</span> in this normed space. This distance is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Mahalanobis_distance"><em>Mahalanobis distance</em></a>, and it then follows that the isoprobability contours defined by <a class="reference internal" href="#equation-mahalanobis-sphere-eq">(8.14)</a> are exactly the “Mahalanobis spheres” of radius <span class="math notranslate nohighlight">\(c\)</span>. In the case that <span class="math notranslate nohighlight">\(d=2\)</span> and</p>
<div class="math notranslate nohighlight" id="equation-multivar-norm-ex-eq">
<span class="eqno">(8.17)<a class="headerlink" href="#equation-multivar-norm-ex-eq" title="Link to this equation">#</a></span>\[\begin{split}
\bu = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad \bSigma = \begin{bmatrix}
1 &amp; 1 \\ 1 &amp; 4
\end{bmatrix},
\end{split}\]</div>
<p>these “spheres” are ellipses centered at <span class="math notranslate nohighlight">\(\bmu\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mahalanobis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">mahalanobis</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span>

<span class="n">contour</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/9e83f536d9d90a9b5641558c4fd24fad22bea4e00fb326adc8ed2a3680502265.svg" src="../_images/9e83f536d9d90a9b5641558c4fd24fad22bea4e00fb326adc8ed2a3680502265.svg" /></figure>
</div>
</div>
<p>Let’s collect our observations in the following theorem, along with a few additional facts:</p>
<div class="proof theorem admonition" id="prop-multivar-norm-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.21 </span> (Isoprobability contours of normal vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose that <span class="math notranslate nohighlight">\(\bX \sim \calN_d(\bmu,\bSigma)\)</span>, where <span class="math notranslate nohighlight">\(\bSigma\)</span> is a positive definite matrix with real, positive eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_d\)</span>. Define <span class="math notranslate nohighlight">\(Q(\bx) = (\bx - \bmu)^\intercal \bSigma^{-1}(\bx - \bmu)\)</span> for all <span class="math notranslate nohighlight">\(\bx \in \bbr^d\)</span>.</p>
<ol class="arabic">
<li><p>The isoprobability contours of the density function <span class="math notranslate nohighlight">\(f(\bx;\bmu,\bSigma)\)</span> are concentric ellipsoids centered at <span class="math notranslate nohighlight">\(\bmu\)</span> defined by equations</p>
<div class="math notranslate nohighlight" id="equation-mahalanobis-sphere-02-eq">
<span class="eqno">(8.18)<a class="headerlink" href="#equation-mahalanobis-sphere-02-eq" title="Link to this equation">#</a></span>\[
    Q(\bx) = c^2
    \]</div>
<p>for fixed <span class="math notranslate nohighlight">\(c\in \bbr\)</span>.</p>
</li>
<li><p>For <span class="math notranslate nohighlight">\(c\neq 0\)</span>, the principal axes of the ellipsoid defined by <a class="reference internal" href="#equation-mahalanobis-sphere-02-eq">(8.18)</a> point along the eigenvectors of the matrix <span class="math notranslate nohighlight">\(\bSigma\)</span>. The half-lengths of the principal axes are given by <span class="math notranslate nohighlight">\(|c|\sqrt{\lambda_i}\)</span>.</p></li>
<li><p>In particular, if <span class="math notranslate nohighlight">\(\bSigma\)</span> is a (positive) multiple of the identity matrix, then the isoprobability contours are concentric spheres centered at <span class="math notranslate nohighlight">\(\bmu\)</span>.</p></li>
</ol>
</section>
</div><p>We have already proved the first statement; for the second, see Section 4.4 in <span id="id11">[<a class="reference internal" href="bib.html#id20" title="W. K. Härdle and L. Simar. Applied multivariate statistical analysis. Springer Nature Switzerland, fifth edition, 2019.">HardleS19</a>]</span>, for example.</p>
<p>It will be useful to generalize a pair of important univariate results from <a class="reference internal" href="05-examples-of-rvs.html#norm-univariate-sec"><span class="std std-numref">Section 5.6</span></a> to the case of multivariate normal distributions. The first result is a generalization of <a class="reference internal" href="05-examples-of-rvs.html#affine-gaussian-thm">Theorem 5.7</a> that states (invertible) affine transformations of normal random variables are still normal. The same is true for normal random vectors:</p>
<div class="proof theorem admonition" id="affine-trans-mutivar-norm-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.22 </span> (Affine transformations of normal vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX \sim \calN_d(\bmu,\bSigma)\)</span>, let <span class="math notranslate nohighlight">\(\bA\in \bbr^{d\times d}\)</span> be nonsingular matrix, and let <span class="math notranslate nohighlight">\(\bb \in \bbr^d\)</span>. Then <span class="math notranslate nohighlight">\(\bY = \bA \bX + \bb\)</span> is a normal random vector with <span class="math notranslate nohighlight">\(\bmu_\bY = \bA\bmu + \bb\)</span> and <span class="math notranslate nohighlight">\(\bSigma_\bY = \bA\bSigma\bA^\intercal\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The proof goes through <a class="reference internal" href="#multivar-density-trans-thm">Theorem 8.6</a>. In the language and notation of that theorem, we have <span class="math notranslate nohighlight">\(r(\bx) = \bA \bx + \bb\)</span>, and so <span class="math notranslate nohighlight">\(s(\by) = \bA^{-1}(\by - \bb)\)</span>. It is easy to show that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial(s_1,\ldots,s_d)}{\partial(y_1,\ldots,y_d)}(\by) = \bA^{-1},
\]</div>
<p>and so</p>
<div class="math notranslate nohighlight">
\[
f_\bY(\by) = \frac{1}{|\det{\bA}|} f_\bX(\bA^{-1}(\by - \bb)) 
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\by \in \bbr^d\)</span>. But then we compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f_\bY(\by) &amp;= \frac{1}{\det(2\pi \bA\bSigma\bA^\intercal)^{1/2}} \exp \left[ -\frac{1}{2}\left(\bA^{-1}\by - \bA^{-1}\bb - \bmu \right)^\intercal \bSigma^{-1}\left(\bA^{-1}\by - \bA^{-1}\bb - \bmu \right) \right] \\
&amp;= \frac{1}{\det(2\pi \bA\bSigma\bA^\intercal)^{1/2}} \exp \left[ -\frac{1}{2}\left(\by - \bb - \bA\bmu \right)^\intercal (\bA \bSigma\bA^\intercal)^{-1} \left(\by - \bb - \bA\bmu \right) \right],
\end{align*}\]</div>
<p>where we recognize the expression on the second line as the density of an <span class="math notranslate nohighlight">\(\calN_d(\bA\bmu+\bb, \bA\bSigma\bA^\intercal)\)</span> random vector. Q.E.D.</p>
</div>
<p>The second result is a generalization of <a class="reference internal" href="05-examples-of-rvs.html#standardization-cor">Corollary 5.1</a>, which states that we may perform an invertible affine transformation of a normal random variable to obtain a standard normal one. Again, the same is true for normal random vectors. To state this generalized result formally, we require the concept of the <em>square root</em> of a positive definite matrix that we introduced in <a class="reference internal" href="#psd-char-thm">Theorem 8.19</a>.</p>
<div class="proof corollary admonition" id="standardize-multivar-cor">
<p class="admonition-title"><span class="caption-number">Corollary 8.1 </span> (Standardization of normal vectors)</p>
<section class="corollary-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(\bX \sim \calN_d(\bmu,\bSigma)\)</span>, then <span class="math notranslate nohighlight">\(\bZ = \bSigma^{-1/2}(\bX - \bmu)\)</span> has a (<span class="math notranslate nohighlight">\(d\)</span>-dimensional) standard normal distribution.</p>
</section>
</div><p>The affine transformation <span class="math notranslate nohighlight">\(\bZ = \bSigma^{-1/2}(\bx - \bmu)\)</span> in the corollary is sometimes called a <em>Mahalanobis transformation</em>. We may use these transformations to help us determine the distributions of the components of a normal random vector:</p>
<div class="proof theorem admonition" id="components-multivar-norm-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.23 </span> (Components of normal vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX \sim \calN_d(\bmu,\bSigma)\)</span> and <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
\bmu^\intercal = (\mu_1,\ldots,\mu_d) \quad \text{and} \quad \bSigma = \left[ \sigma_{ij} \right].
\]</div>
<p>Then each component random variable <span class="math notranslate nohighlight">\(X_i\)</span> is normal with mean <span class="math notranslate nohighlight">\(\mu_i\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_{ii}\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. By <a class="reference internal" href="#standardize-multivar-cor">Corollary 8.1</a>, the random vector <span class="math notranslate nohighlight">\(\bZ = \bSigma^{-1/2}(\bX - \bmu)\)</span> is a <span class="math notranslate nohighlight">\(d\)</span>-dimensional standard normal vector. It is not difficult to show that the components of <span class="math notranslate nohighlight">\(\bZ\)</span> are all standard normal variables:</p>
<div class="math notranslate nohighlight" id="equation-z-comp-norm-eq">
<span class="eqno">(8.19)<a class="headerlink" href="#equation-z-comp-norm-eq" title="Link to this equation">#</a></span>\[
Z_1,\ldots,Z_d\sim \calN(0,1).
\]</div>
<p>But <span class="math notranslate nohighlight">\(\bX = \bSigma^{1/2} \bZ + \bmu\)</span>, and so</p>
<div class="math notranslate nohighlight">
\[
X_i = \sum_{j=1}^d \omega_{ij} Z_j + \mu_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bSigma^{1/2} = [\omega_{ij}]\)</span>. Now apply <a class="reference internal" href="#mgf-norm-thm">Theorem 8.10</a> and use <a class="reference internal" href="#equation-z-comp-norm-eq">(8.19)</a> to conclude that</p>
<div class="math notranslate nohighlight">
\[
X_i \sim \calN\left(\mu_i, \sum_{j=1}^d \omega_{ij}^2 \right).
\]</div>
<p>However, we have</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^d \omega_{ij}^2 = \sum_{j=1}^d \omega_{ij}\omega_{ji} = \sigma_{ii}
\]</div>
<p>since <span class="math notranslate nohighlight">\(\bSigma^{1/2} \bSigma^{1/2} = \bSigma\)</span> and <span class="math notranslate nohighlight">\(\bSigma^{1/2}\)</span> is symmetric. Q.E.D.</p>
</div>
<p>In fact, an even stronger result than <a class="reference internal" href="#components-multivar-norm-thm">Theorem 8.23</a> is true: Not only are the individual univariate components of a normal random vector themselves normal, but <em>any</em> linear combination of these components is also normal. Even more surprising, this fact turns out to give a complete characterization of normal random vectors! This is the content of:</p>
<div class="proof theorem admonition" id="char-normal-thm">
<p class="admonition-title"><span class="caption-number">Theorem 8.24 </span> (The Linear-Combination Criterion for Normal Random Vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bX = (X_1,\ldots,X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional random vector. Then <span class="math notranslate nohighlight">\(\bX\)</span> is a normal random vector with parameters <span class="math notranslate nohighlight">\(\bmu\)</span> and <span class="math notranslate nohighlight">\(\bSigma\)</span> if and only if every linear combination</p>
<div class="math notranslate nohighlight">
\[
a_1 X_1 + \cdots + a_d X_d, \quad a_1,\ldots,a_d \in \bbr
\]</div>
<p>is a normal random variable. In this case, the parameter <span class="math notranslate nohighlight">\(\bSigma\)</span> is the covariance matrix of <span class="math notranslate nohighlight">\(\bX\)</span>, while <span class="math notranslate nohighlight">\(\bmu^\intercal = (\mu_1,\ldots,\mu_d)\)</span> with <span class="math notranslate nohighlight">\(E(X_i) = \mu_i\)</span> for each <span class="math notranslate nohighlight">\(i\)</span>.</p>
</section>
</div><p>We will not prove this, since the clearest proof (that I know of) makes use of <em>characteristic functions</em>. (See, for example, Chapter 5 in <span id="id12">[<a class="reference internal" href="bib.html#id21" title="A. Gut. An intermediate course in probability. Springer Science+Business Media, LLC, second edition, 2009.">Gut09</a>]</span>.)</p>
<p>At least in two dimensions, using the fact that the parameter <span class="math notranslate nohighlight">\(\bSigma\)</span> in <span class="math notranslate nohighlight">\((X,Y) \sim \calN(\bmu,\bSigma)\)</span> is the covariance matrix of <span class="math notranslate nohighlight">\((X,Y)\)</span> gives us an easy method to cook up examples of bivariate normal distributions of a specified shape. Indeed, by the theorem, we must have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bSigma = \begin{bmatrix}
\sigma_X^2 &amp; \sigma_{XY} \\ \sigma_{XY} &amp; \sigma_Y^2
\end{bmatrix} = \begin{bmatrix}
\sigma_X^2 &amp; \rho \sigma_X \sigma_Y \\ \rho\sigma_X \sigma_Y &amp; \sigma_Y^2
\end{bmatrix}
\end{split}\]</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>What goes wrong in the case that <span class="math notranslate nohighlight">\(\rho=\pm 1\)</span>?</p>
</aside>
<p>where <span class="math notranslate nohighlight">\(\rho = \rho_{XY}\)</span>. One then first selects the correlation <span class="math notranslate nohighlight">\(\rho\)</span> of the random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> such that <span class="math notranslate nohighlight">\(|\rho|&lt;1\)</span> (note the <em>strict</em> inequality!), which rotates the principal axes of the elliptical isoprobability contours; a value of <span class="math notranslate nohighlight">\(\rho=0\)</span> will create principal axes parallel to the <span class="math notranslate nohighlight">\(x\)</span>- and <span class="math notranslate nohighlight">\(y\)</span>-axes. Then, by choosing the standard deviations <span class="math notranslate nohighlight">\(\sigma_X\)</span> and <span class="math notranslate nohighlight">\(\sigma_Y\)</span>, we can alter the widths of the projections of these ellipses onto the <span class="math notranslate nohighlight">\(x\)</span>- and <span class="math notranslate nohighlight">\(y\)</span>-axes. For example, the following shows the isoprobability contours for four different selections of the parameters <span class="math notranslate nohighlight">\((\rho,\sigma_X,\sigma_Y)\)</span> where <span class="math notranslate nohighlight">\(\bmu^\intercal = (1,1)\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">covar_matrix</span><span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">sigma1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">]])</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>   <span class="c1"># rho, sigma1, sigma2</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">Sigma</span> <span class="o">=</span> <span class="n">covar_matrix</span><span class="p">(</span><span class="o">*</span><span class="n">parameter</span><span class="p">)</span>
    <span class="n">rho</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="n">parameter</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$(</span><span class="se">\\</span><span class="s1">rho,</span><span class="se">\\</span><span class="s1">sigma_X,</span><span class="se">\\</span><span class="s1">sigma_Y)=(</span><span class="si">{</span><span class="n">rho</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">sigma1</span><span class="si">}</span><span class="s1">,</span><span class="si">{</span><span class="n">sigma2</span><span class="si">}</span><span class="s1">)$&#39;</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/3eda68c29a1017ef51f20213fedbac0d76a256954c199888df74f8062bcfd87d.svg" src="../_images/3eda68c29a1017ef51f20213fedbac0d76a256954c199888df74f8062bcfd87d.svg" /></figure>
</div>
</div>
<p>In the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">programming assignment</a> for this chapter, you will actually implement this procedure. For example, I used these methods to simulate <span class="math notranslate nohighlight">\(1{,}000\)</span> random draws from a bivariate normal distribution with positively correlated components and which is taller than it is wide. If I plot these simulated data over top of isoprobability contours, I get this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rho</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">sigma1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">],</span> <span class="p">[</span><span class="n">rho</span> <span class="o">*</span> <span class="n">sigma1</span> <span class="o">*</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">Sigma</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="mi">7</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">sample</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/ed2850f5d760fc1850dc7c520f455236f4664edf764aa55c1c3cac02e815156f.svg" src="../_images/ed2850f5d760fc1850dc7c520f455236f4664edf764aa55c1c3cac02e815156f.svg" /></figure>
</div>
</div>
<p>Let’s finish off this long chapter with an example problem:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 12 on the worksheet.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="07-random-vectors.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Random vectors</p>
      </div>
    </a>
    <a class="right-next"
       href="09-halfway.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>The halfway point: pivoting toward models and data analysis</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectations-and-joint-distributions">8.1. Expectations and joint distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectations-and-conditional-distributions">8.2. Expectations and conditional distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#density-transformations">8.3. Density transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#moment-generating-functions">8.4. Moment generating functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependent-random-variables-covariance-and-correlation">8.5. Dependent random variables, covariance, and correlation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-normal-distributions">8.6. Multivariate normal distributions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>