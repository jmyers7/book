

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>6. Connecting theory to practice: a first look at model building &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bA": "\\mathbf{A}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bS": "\\mathbf{S}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/06-theory-to-practice';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Random vectors" href="07-random-vectors.html" />
    <link rel="prev" title="5. Examples of random variables" href="05-examples-of-rvs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-info-theory.html">9. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-optim.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-models.html">11. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-learning.html">12. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-stats-estimators.html">13. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-asymptotic.html">14. Large sample theory and more sampling distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/06-theory-to-practice.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Connecting theory to practice: a first look at model building</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-and-random-samples">6.1. Data and random samples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-models-and-empirical-distributions">6.2. Probabilistic models and empirical distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">6.3. Histograms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-density-estimation">6.4. Kernel density estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-statistics">6.5. Empirical statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qq-plots">6.6. QQ-plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#box-plots-and-violin-plots">6.7. Box plots and violin plots</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="connecting-theory-to-practice-a-first-look-at-model-building">
<span id="theory-to-practice"></span><h1><span class="section-number">6. </span>Connecting theory to practice: a first look at model building<a class="headerlink" href="#connecting-theory-to-practice-a-first-look-at-model-building" title="Permalink to this heading">#</a></h1>
<p>Though you investigated some real-world datasets and scenarios in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">programming assignments</a>, essentially everything that we have studied so far belongs to abstract probability theory. In this chapter, we take a first look at <em>modeling</em> while also continuing the discussion begun in the third programming assignment on <em>empirical distributions</em> of datasets. The ultimate goal is to begin connecting our abstract theory to real-world practice.</p>
<p>The modeling concept is bigger than just probability theory, statistics, and machine learning; indeed, the process of model building is central to essentially all quantitative sciences:</p>
<a class="reference internal image-reference" href="../_images/model-build.svg"><img alt="../_images/model-build.svg" class="align-center" src="../_images/model-build.svg" width="80%" /></a>
<p> </p>
<p>Popular stories in the history of western science tell of the search for better and more accurate physical models—the transition from the geocentric Ptolemaic model of the solar system to the heliocentric Copernican one immediately springs to mind as an example.</p>
<p>Mathematical models serve multiple functions in science: an accurate model may be used to test hypotheses and suggest new theories, and it may be used to predict future events. The same is true for the models that we study in this chapter. These new models are <em>probabilistic</em>, however, which means that an analyst needs a firm foundation in probability theory to understand the language used to describe the models.</p>
<p>While there are similarities between the models that you may have studied before and the new ones in this chapter, there are some differences as well along with novel use cases. For example, once these probabilistic models have been fully trained and all their parameters have been learned, they may be sampled from, <em>generating</em> new data in the process. This is the realm of <em>generative models</em> and <em>generative machine learning</em>. The probabilistic nature of the models means that the new data is random. Those who have no issues using anthropomorphized language to describe the models might refer to this randomness as <em>creativity</em>.</p>
<p>But our aims in this chapter are much more limited. We will study univariate models made out of only a <em>single</em> random variable, though it may be “duplicated” many times to produce so-called <em>IID sequences</em> of random variables that serve as theoretical avatars of observed real-world datasets. We will briefly study a few tools and techniques that help us visualize the <em>shape</em> of a dataset; some of these visualization tools will then be used for <em>model checking</em> and to help judge <em>goodness-of-fit</em> for proposed probabilistic models. Later, in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>, we will embed the univariate models in this chapter into interconnected “webs” of random variables, creating ever more expressive and powerful probabilistic models. But we must learn to walk before we can run!</p>
<section id="data-and-random-samples">
<h2><span class="section-number">6.1. </span>Data and random samples<a class="headerlink" href="#data-and-random-samples" title="Permalink to this heading">#</a></h2>
<p>The story begins with a careful study of the stuff that our models are supposed to model: data! Throughout this chapter, we will explore a real-world dataset consisting of listing prices (in USD) for Airbnbs in Austin, Texas, over an observation period of 12 months:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.gofplots</span> <span class="kn">import</span> <span class="n">qqplot</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="n">srs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../aux-files/airbnb-data.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="n">srs</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0        111.97
1         97.62
2         48.48
3        106.66
4        156.66
          ...  
13248    515.50
13249     70.78
13250    297.56
13251    212.29
13252     74.66
Name: price, Length: 13253, dtype: float64
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This dataset was obtained <a class="reference external" href="http://insideairbnb.com">here</a>. I removed outliers and also smoothed the data to remove spikes at prices that are multiples of <span class="math notranslate nohighlight">\(\$50\)</span>.</p>
</aside>
<p>Notice that there are <span class="math notranslate nohighlight">\(m=13{,}239\)</span> prices in the dataset.</p>
<p>To fit this dataset into the theoretical framework of the first few chapters, we conceptualize the (hypothetical) collection of <em>all</em> possible listings as a probability space <span class="math notranslate nohighlight">\(S\)</span>. Then, we conceptualize the price of a listing as a random variable</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X: S \to \mathbb{R}
\end{equation*}\]</div>
<p>that takes a listing from the population <span class="math notranslate nohighlight">\(S\)</span> as input and spits out its price:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X(\text{listing}) = \text{price}.
\end{equation*}\]</div>
<p>The actual prices in our dataset are called <em>observed values</em> of the random variable <span class="math notranslate nohighlight">\(X\)</span> and, as in previous chapters, they are represented with a lowercase <span class="math notranslate nohighlight">\(x\)</span>. We would list the elements in our dataset as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_1,x_2,\ldots,x_m \in \mathbb{R}
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(m=13{,}239\)</span>.</p>
<p>So far, this discussion is not all that different from ones that we have had before; in fact, I can even use the same cartoon from previous chapters to visualize the action of the random variable <span class="math notranslate nohighlight">\(X\)</span>:</p>
<a class="reference internal image-reference" href="../_images/airbnb0.svg"><img alt="../_images/airbnb0.svg" class="align-center" src="../_images/airbnb0.svg" width="90%" /></a>
<p> </p>
<p>However, what makes our current scenario different from those considered in past chapters is that we have an entire dataset of prices at hand, not just a <em>single</em> price. How are we to fit <em>datasets</em> into our theoretical framework?</p>
<p>The answer is pretty obvious, actually. After all, a collection of <span class="math notranslate nohighlight">\(m\)</span> prices like those in our dataset must come from a collection of <span class="math notranslate nohighlight">\(m\)</span> listings, right? This suggests that we should simply “duplicate” the random variable <span class="math notranslate nohighlight">\(X\)</span> to obtain a <em>sequence</em> of random variables</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m,
\end{equation*}\]</div>
<p>where the random variable <span class="math notranslate nohighlight">\(X_i\)</span> (for <span class="math notranslate nohighlight">\(1\leq i \leq m\)</span>) spits out the price of the <span class="math notranslate nohighlight">\(i\)</span>-th listing. Something like this:</p>
<a class="reference internal image-reference" href="../_images/cartesian.svg"><img alt="../_images/cartesian.svg" class="align-center" src="../_images/cartesian.svg" width="90%" /></a>
<p> </p>
<p>To make this work, technically we need to replace the sample space <span class="math notranslate nohighlight">\(S\)</span> with its <span class="math notranslate nohighlight">\(m\)</span>-fold <a href="https://en.wikipedia.org/wiki/Cartesian_product#n-ary_Cartesian_power">cartesian power</a>, but we won’t worry about these details. For us, it’s enough to understand this process at the intuitive level.</p>
<p>Now, what about probability? Remember, I called the population <span class="math notranslate nohighlight">\(S\)</span> of all listings a <em>probability</em> space, so evidently it must come equipped with a probability measure <span class="math notranslate nohighlight">\(P\)</span>. But here’s the truth:</p>
<blockquote>
<div><p>We don’t actually <em>care</em> about <span class="math notranslate nohighlight">\(P\)</span>.</p>
</div></blockquote>
<p>Indeed, <span class="math notranslate nohighlight">\(P\)</span> is a purely academic object whose only role in this business is to make the theory under the hood tick along. It’s mostly mathematicians like me that spend time worrying about <span class="math notranslate nohighlight">\(P\)</span>, but it is <em>never</em>, <em>ever</em> mentioned or acknowledged in real-world scenarios.</p>
<p>On the other hand, we do very much(!) care about the probability distributions of the random variables <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span>. We will be devoting a huge amount of time and effort over the rest of this course trying to figure out the distribution of this or that random variable. In the context of our Airbnb prices, the distributions of the <span class="math notranslate nohighlight">\(X_i\)</span>’s tell us the distribution of prices:</p>
<a class="reference internal image-reference" href="../_images/airbnb.svg"><img alt="../_images/airbnb.svg" class="align-center" src="../_images/airbnb.svg" width="90%" /></a>
<p> </p>
<p>But because each of the random variables <span class="math notranslate nohighlight">\(X_i\)</span> is essentially a “duplicate” of the single random variable <span class="math notranslate nohighlight">\(X\)</span>, they all have the <em>same</em> distribution, in the sense that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
P(X_1\in A) = P(X_2\in A) = \cdots = P(X_m\in A)
\end{equation*}\]</div>
<p>for all events <span class="math notranslate nohighlight">\(A\subset \mathbb{R}\)</span>. If we draw each of the random variables along with their distributions, we would get:</p>
<a class="reference internal image-reference" href="../_images/iid.svg"><img alt="../_images/iid.svg" class="align-center" src="../_images/iid.svg" width="100%" /></a>
<p> </p>
<p>Notice that all the distributions are the same! This leads us to one of the main definitions in this entire course:</p>
<div class="proof definition admonition" id="random-sample-defn">
<p class="admonition-title"><span class="caption-number">Definition 6.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be a sequence of random variables, all defined on the same probability space.</p>
<ul class="simple">
<li><p>The random variables are called a <em>random sample</em> if they are <em>independent</em> and <em>identically distributed</em> (IID).</p></li>
</ul>
<p>Provided that the sequence is a random sample, an <em>observed random sample</em>, or a <em>dataset</em>, is a sequence of real numbers <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> where <span class="math notranslate nohighlight">\(x_i\)</span> is an observed value of <span class="math notranslate nohighlight">\(X_i\)</span>. We shall also refer to a dataset as an <em>observation</em> of the corresponding random sample.</p>
</section>
</div><p>Two random variables are said to be <em>independent</em> if the probability of one of the random variables taking a particular value is not influenced or affected by the other random variable taking a particular value. This isn’t a precise definition, and it must be adapted to apply to an entire <em>sequence</em> of random variables, but it is good enough for now. (The precise definition will come in <a class="reference internal" href="07-random-vectors.html#independence-defn">Definition 7.11</a>.)</p>
<p>Take care to notice the difference between a <em>random sample</em> (without the modifier) and an <em>observed random sample</em>—the former is an IID sequence of random variables, while the latter is a sequence of real numbers!</p>
<p>Why have two different types of random samples? Answers:</p>
<div class="admonition-the-roles-of-random-samples admonition">
<p class="admonition-title">The roles of random samples</p>
<ul class="simple">
<li><p>Observed random samples <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> are the datasets that we work with in the real world. It is therefore obvious why we care about these.</p></li>
<li><p>We use random samples <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> when we want to reason theoretically about the observed random samples that we encounter in the real world. For example, suppose that you want to prove that some type of statistical estimator or machine learning algorithm works well for <em>any</em> dataset. Then you <em>must</em> argue using random samples consisting of IID random variables!</p></li>
</ul>
</div>
</section>
<section id="probabilistic-models-and-empirical-distributions">
<span id="prob-models-emp-sec"></span><h2><span class="section-number">6.2. </span>Probabilistic models and empirical distributions<a class="headerlink" href="#probabilistic-models-and-empirical-distributions" title="Permalink to this heading">#</a></h2>
<p>Suppose we conceptualize our Airbnb dataset <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> as an observation of a random sample <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span>. Since the random variables in the random sample are identically distributed, they all induce the <em>same</em> probability distribution on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. <em>But what is this distribution?</em></p>
<p>Whatever it is, it is <em>not</em> completely and uniquely determined by the dataset. Its specific identity is chosen by the analyst. Many factors are considered in this choice, but the most important is to pick a distribution that closely <em>fits</em> the empirical distribution of the dataset. This is the modeling process in a nutshell.</p>
<p>Let’s introduce some notation and terminology. Suppose that <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> is <em>any</em> random sample. If the common probability distribution among the <span class="math notranslate nohighlight">\(X_i\)</span>’s comes from one of the families studied in <a class="reference internal" href="05-examples-of-rvs.html#examples"><span class="std std-numref">Chapter 5</span></a>—for example, a normal distribution—we will write the usual</p>
<div class="math notranslate nohighlight" id="equation-normal-model-eqn">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-normal-model-eqn" title="Permalink to this equation">#</a></span>\[X_1,X_2,\ldots,X_m \sim \mathcal{N}(\mu,\sigma^2). \]</div>
<p>We will also sometimes write</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m \sim F
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(F\)</span> is the common distribution function shared among the <span class="math notranslate nohighlight">\(X_i\)</span>’s. We may also write the density function <span class="math notranslate nohighlight">\(f\)</span> or mass function <span class="math notranslate nohighlight">\(p\)</span> in place of <span class="math notranslate nohighlight">\(F\)</span>. In any case, a particular choice of <span class="math notranslate nohighlight">\(F\)</span> (or <span class="math notranslate nohighlight">\(f\)</span> or <span class="math notranslate nohighlight">\(p\)</span>) is called a <em>probabilistic model</em>. The modeling process may then be described as <em>choosing</em> a probabilistic model.</p>
<p>There are both <em>parametric</em> and <em>non-parametric</em> probabilistic models. A normal model of the form <a class="reference internal" href="#equation-normal-model-eqn">(6.1)</a> belongs to the former type, since it depends on the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Proposing a parametric probabilistic model is then only half the battle, for then the parameters must be determined to completely specify the model. A machine learning engineer would say that the parameters are <em>learned</em> from a given dataset (see <a class="reference internal" href="12-learning.html#learning"><span class="std std-numref">Chapter 12</span></a>), while a statistician would say that they are <em>inferred</em> from data.</p>
<p>As we will learn in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>, the random variables and parameters in a probabilistic model may be depicted graphically; for a normal model <a class="reference internal" href="#equation-normal-model-eqn">(6.1)</a> with <span class="math notranslate nohighlight">\(m=3\)</span>, we would draw the following picture:</p>
<a class="reference internal image-reference" href="../_images/norm-model.svg"><img alt="../_images/norm-model.svg" class="align-center" src="../_images/norm-model.svg" width="50%" /></a>
<p> </p>
<p>The arrows in this simple graph are intended to convey “influence”—that different parameter settings will alter or “influence” the distributions of the <span class="math notranslate nohighlight">\(X_i\)</span>’s. Admittedly, since the normal model is so simple, there isn’t much additional insight to be gained by drawing this graph. But it’s good practice for the models in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>.</p>
<p>How do we judge the <em>goodness-of-fit</em> of a proposed model distribution? Our first method will be based on a visual comparison of the model distribution function to the <em>empirical distribution function</em>. The latter is defined in the next box, which also contains a reminder of the definition of an <em>empirical distribution</em> that appeared back in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>.</p>
<div class="proof definition admonition" id="emp-dist-defn">
<p class="admonition-title"><span class="caption-number">Definition 6.2 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>empirical distribution</em> of a dataset <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> is the discrete probability measure on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> with probability mass function</p>
<div class="math notranslate nohighlight">
\[p(x) = \frac{\text{number of data points $x_i$ that match $x$}}{m}.\]</div>
<p>The <em>empirical cumulative distribution function</em> (<em>ECDF</em>) of the dataset is the CDF of the empiricical disribution. It is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
F(x) = \sum_{x^\star\leq x} p(x^\star) = \frac{\text{number of data points $x_i$ with $x_i \leq x$}}{m}.
\end{equation*}\]</div>
</section>
</div><p>Notice that the probability <span class="math notranslate nohighlight">\(p(x)\)</span> is the <em>relative frequency</em> of <span class="math notranslate nohighlight">\(x\in \mathbb{R}\)</span>; it is exactly the frequency that <span class="math notranslate nohighlight">\(x\)</span> appears in the dataset, divided by the size of the dataset.</p>
<p>To make this concrete, let’s bring back the Airbnb prices. I have asked the computer to generate the ECDF of this dataset:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7b69ee090eca79c9c424d221c729564be8ebaf74c9a4726de91fefd28a95a3ac.svg"><img alt="../_images/7b69ee090eca79c9c424d221c729564be8ebaf74c9a4726de91fefd28a95a3ac.svg" src="../_images/7b69ee090eca79c9c424d221c729564be8ebaf74c9a4726de91fefd28a95a3ac.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>Remember, the CDFs of discrete distributions are step functions. So why doesn’t <em>this</em> look like a step function? Essentially, the data points are so numerous and packed together so tightly along the <span class="math notranslate nohighlight">\(x\)</span>-axis that we can’t see the steps. But rest assured, if we were to zoom in, we’d see them.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Actually, it’s not quite right to say that the empirical variance <span class="math notranslate nohighlight">\(s^2\)</span> is the parameter estimate derived from the method of moments or maximum likelihood estimation. In fact, the latter estimates have the sample size <span class="math notranslate nohighlight">\(m\)</span> in the denominator, whereas the empirical variance <span class="math notranslate nohighlight">\(s^2\)</span> is usually defined with <span class="math notranslate nohighlight">\(m-1\)</span> in the denominator.</p>
</aside>
<p>Now, suppose that our Airbnb dataset is an observation of a random sample <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> (where <span class="math notranslate nohighlight">\(m=13{,}239\)</span>) and that we propose a normal model</p>
<div class="math notranslate nohighlight">
\[
X_1,X_2,\ldots,X_{m} ; \mu,\sigma^2 \sim \mathcal{N}(\mu,\sigma^2).
\]</div>
<p>How might we “learn” the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> from the data? In this case, we might choose <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> to be the <em>empirical mean</em> <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <em>empirical variance</em> <span class="math notranslate nohighlight">\(s^2\)</span>, which you saw back in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a> and which we will review in <a class="reference internal" href="#empirical-stats"><span class="std std-numref">Section 6.5</span></a> below. (This is the <em>method of moments</em> for estimating model parameters which, in this special case, is also the method of <em>maximum likelihood estimation</em>; see <a class="reference internal" href="12-learning.html#learning"><span class="std std-numref">Chapter 12</span></a>.) With these parameter settings, let’s plot the ECDF of the dataset along with the CDF of the normal model superimposed:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xbar</span> <span class="o">=</span> <span class="n">srs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">srs</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">xbar</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">srs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">srs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

<span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;normal CDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xbar</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;empirical mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/912540d9acdeeaaa29c2071b93a02ac2037726255740c0d6ca4461650af0a366.svg"><img alt="../_images/912540d9acdeeaaa29c2071b93a02ac2037726255740c0d6ca4461650af0a366.svg" src="../_images/912540d9acdeeaaa29c2071b93a02ac2037726255740c0d6ca4461650af0a366.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>Yikes. Those CDFs are quite different from each other, suggesting that the normal model is a bad fit.</p>
<p>All is not lost, however, since this comparison suggests a better model. Though technically the ECDF is a step function, it <em>appears</em> to us as a nice smooth-ish curve. If we pretend that it actually <em>is</em> a smooth curve, then the empirical distribution has a probability density function. This density function should have a local maximum around a price of <span class="math notranslate nohighlight">\(\$100\)</span> since the ECDF appears to have an inflection point near that price. As we continue scanning the graph from left to right beginning at <span class="math notranslate nohighlight">\(\$100\)</span>, we see that the tangent line slopes of the ECDF gradually decrease toward <span class="math notranslate nohighlight">\(0\)</span>, but that it takes awhile before the slopes are essentially <span class="math notranslate nohighlight">\(0\)</span>. This suggests that the dataset is <em>right-skew</em>, in the sense that it has a long tail reaching out to the right from its (single) peak.</p>
<p>This analysis suggests that a log transform (which you learned about in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>) might “un-skew” the data by removing the tail. In place of our (bad) normal model, we thus propose a normal model on the <em>transformed</em> data:</p>
<div class="math notranslate nohighlight">
\[
\log{X_1},\log{X_2},\ldots,\log{X_{m}} ; \mu,\sigma^2 \sim \mathcal{N}(\mu,\sigma^2).
\]</div>
<p>We take the model parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> as the empirical mean and variance of the log-transformed data. Here’s a CDF comparison:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">srs</span><span class="p">)</span>
<span class="n">xbar</span> <span class="o">=</span> <span class="n">srs_log</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">srs_log</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">xbar</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">srs_log</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">srs_log</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

<span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs_log</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF after log transform&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;normal CDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">srs_log</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xbar</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;empirical mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;log price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1">#plt.savefig(&#39;ecdf-compare-log.pdf&#39;)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/cc7224403b0bfb4b92e2a576130988c41e944b6b5d75d079d25509eb0e3a4d7f.svg"><img alt="../_images/cc7224403b0bfb4b92e2a576130988c41e944b6b5d75d079d25509eb0e3a4d7f.svg" src="../_images/cc7224403b0bfb4b92e2a576130988c41e944b6b5d75d079d25509eb0e3a4d7f.svg" width="70%" /></a>
</figure>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>By the way, datasets whose log transforms are well modeled by normal distributions occur quite frequently. So frequently, in fact, that we refer to these datasets as <a class="reference external" href="https://en.wikipedia.org/wiki/Log-normal_distribution#">log-normally distributed</a>.</p>
</aside>
<p>That looks pretty good! There is a slight discrepancy beginning around <span class="math notranslate nohighlight">\(6\)</span> on the horizontal axis where the normal CDF flattens while the ECDF remains steep. This means that the amount of probability density of the (log-transformed) dataset in this region is <em>larger</em> than predicted by the normal model.</p>
<p>If we elect to use the normal model on the log-transformed data, we need to remember that any information we learn about the data through the model needs to be “back-transformed” using the exponential function to translate to the original dataset.</p>
</section>
<section id="histograms">
<h2><span class="section-number">6.3. </span>Histograms<a class="headerlink" href="#histograms" title="Permalink to this heading">#</a></h2>
<p>In the previous section, we used a plot of the ECDF of the Airbnb prices in order to <em>visualize</em> the empirical distribution. From this plot, we were able to get a sense of the <em>shape</em> of the dataset that informed our choice of probabilistic model.</p>
<p>There are a few more ways that we might attempt to visualize the empirical distribution. Indeed, since the empirical distribution of the Airbnb prices is technically discrete, we might attempt to visualize it using the probability histograms that we saw in <a class="reference internal" href="02-prob-spaces.html#prob-histo"><span class="std std-numref">Section 2.7</span></a> and implemented in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">thrid programming assignment</a>. However, as we saw in the previous section, the data points in our sample are so numerous and packed so closely together that such a probability histogram would be essentially useless. We would have <span class="math notranslate nohighlight">\(10{,}666\)</span> bars in our histogram, one for each unique price in our dataset!</p>
<p>But here’s an idea: Suppose we “bin together” nearby prices along the <span class="math notranslate nohighlight">\(x\)</span>-axis <em>before</em> we draw the histogram. In other words, imagine we chop up the <span class="math notranslate nohighlight">\(x\)</span>-axis into smaller subintervals called <em>bins</em>, and then group together nearby data points in these bins. Here’s an example using a toy dataset consisting of <span class="math notranslate nohighlight">\(m=14\)</span> points:</p>
<a class="reference internal image-reference" href="../_images/bins1.svg"><img alt="../_images/bins1.svg" class="align-center" src="../_images/bins1.svg" width="100%" /></a>
<p> </p>
<p>In the figure on the right, there are seven bins, labeled <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_7\)</span>. The number of bins can either be chosen explicitly by us, or we can let the computer choose the “optimal” number of bins based on some rule of thumb. Either way, the number <span class="math notranslate nohighlight">\(k\)</span> of bins need not be <span class="math notranslate nohighlight">\(7\)</span>.</p>
<p>Now, to draw the “binned” histogram, we put a rectangle on top of each bin:</p>
<a class="reference internal image-reference" href="../_images/bins.svg"><img alt="../_images/bins.svg" class="align-center" src="../_images/bins.svg" width="70%" /></a>
<p> </p>
<p>The heights of the rectangles must satisfy <em>two</em> properties: (1) Their areas must sum to <span class="math notranslate nohighlight">\(1\)</span>, and (2) their heights must be proportional to the number of data points that fall within the bins. For example, you can see that the taller rectangles in the figure contain <em>more</em> data points in their base bins. If <span class="math notranslate nohighlight">\(k\)</span> is the total number of bins and <span class="math notranslate nohighlight">\(m\)</span> is the size of the data set, then to satisfy both these properties, we can choose the <span class="math notranslate nohighlight">\(j\)</span>-th rectangle (for <span class="math notranslate nohighlight">\(j=1,2,\ldots,k\)</span>) to have height</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\frac{\text{number of data points in $j$-th bin}}{m \times (\text{width of $j$-th bin})}.
\end{equation*}\]</div>
<p>Of course, computers are capable of plotting these types of histograms. Here is one for our Airbnb prices:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/c74d35df611a1e1f12ba32af7ce82f5c7087e75243726a282bef973b76a4638e.svg"><img alt="../_images/c74d35df611a1e1f12ba32af7ce82f5c7087e75243726a282bef973b76a4638e.svg" src="../_images/c74d35df611a1e1f12ba32af7ce82f5c7087e75243726a282bef973b76a4638e.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>The histogram shows the right-skew suggested by the plot of the ECDF in the previous section. There is also a peak in the data around <span class="math notranslate nohighlight">\(\$500\)</span>, which on the log scale is near <span class="math notranslate nohighlight">\(6.2 \approx \log{500}\)</span>. This peak was suggested by the plot of the ECDF of the log-transformed data.</p>
<p>Be warned, however, that the shapes of histograms are sensitive to the number of bins. Here’s a histogram of the Airbnb data with 200 bins:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/62575a00909a9af36308054e90fad729b7d8bf67d8d0f29f79401a507b67ada8.svg"><img alt="../_images/62575a00909a9af36308054e90fad729b7d8bf67d8d0f29f79401a507b67ada8.svg" src="../_images/62575a00909a9af36308054e90fad729b7d8bf67d8d0f29f79401a507b67ada8.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>When too many bins are used, the resulting histogram is “spiky”. Some spikes might be significant, but often they are just reflections of random variability and anomalies in the data that we don’t want to model.</p>
<p>At the other extreme, here’s a histogram with three bins:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/b5b54e7f758729ac713268913207dadd8d7919021f635b22a55aaf069894e39f.svg"><img alt="../_images/b5b54e7f758729ac713268913207dadd8d7919021f635b22a55aaf069894e39f.svg" src="../_images/b5b54e7f758729ac713268913207dadd8d7919021f635b22a55aaf069894e39f.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>Except for the right-skew, there’s not much else that we could learn about the shape of the dataset from this histogram.</p>
<p>We may use histograms for model checking by generating or simulating new data. To do this, we sample <span class="math notranslate nohighlight">\(m=13{,}239\)</span> new data points from our log-normal model from the previous section. Then, we back-transform using the exponential function to move the simulated data from the log-price scale to the price scale. If we then generate a combined histogram of the true dataset and the simulated one, we get this:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">srs</span><span class="p">)))</span>
<span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">srs</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">sample</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">sample</span> <span class="o">&lt;=</span> <span class="n">srs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">df_sample</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="n">mask</span><span class="p">]})</span>
<span class="n">df_sample</span><span class="p">[</span><span class="s1">&#39;data type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;simulated&#39;</span>
<span class="n">df_true</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">srs</span><span class="p">})</span>
<span class="n">df_true</span><span class="p">[</span><span class="s1">&#39;data type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;true&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">objs</span><span class="o">=</span><span class="p">[</span><span class="n">df_true</span><span class="p">,</span> <span class="n">df_sample</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;data type&#39;</span><span class="p">,</span> <span class="n">multiple</span><span class="o">=</span><span class="s1">&#39;dodge&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/efdd50cf2be2d550261cb71a993836c7cd7aa9acc8c30fd01016d24c7b33ba33.svg"><img alt="../_images/efdd50cf2be2d550261cb71a993836c7cd7aa9acc8c30fd01016d24c7b33ba33.svg" src="../_images/efdd50cf2be2d550261cb71a993836c7cd7aa9acc8c30fd01016d24c7b33ba33.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>Again, the discrepancy around <span class="math notranslate nohighlight">\(\$500\)</span> is apparent; the log-normal model is incapable of capturing that local peak.</p>
<p>How do you choose the number of bins in a histogram? First, you might let the computer decide for you, using the default setting. This default setting is often based off of rules of thumb that have been demonstrated to work well for some reason or another. Then, you can play with the number of bins manually, fine-tuning the number until you get a figure that you believe most accurately represents the shape of the data.</p>
</section>
<section id="kernel-density-estimation">
<h2><span class="section-number">6.4. </span>Kernel density estimation<a class="headerlink" href="#kernel-density-estimation" title="Permalink to this heading">#</a></h2>
<p>Though the empirical distribution of the Airbnb dataset is discrete, its ECDF <em>appears</em> to be a continuous curve. If we pretend that it <em>is</em> continuous, then the dataset should have a density function. What would this density function look like?</p>
<p>If you recall that the density function is the derivative of the distribution function, then you may use the shape of the latter along with your curve sketching skills from calculus to get a sense of the shape of the density curve. But computers are capable of estimating density functions directly from the data. In this section, we will briefly explore one such way, called <em>kernel density estimation</em> (<em>KDE</em>).</p>
<p>Naturally, a kernel density estimation begins by choosing the <em>kernel</em>, which is a function that has a “bell shape,” not unlike the normal density curve. In fact, one can even <em>choose</em> the normal density curve as the kernel, and then one obtains <em>Gaussian KDE</em>. These will be the only types of kernels that we will consider.</p>
<p>Imagine for simplicity that we have three data points along the <span class="math notranslate nohighlight">\(x\)</span>-axis. The idea is then to place three kernels (i.e., normal density curves) directly over top of the data points. We then <em>sum</em> the kernels and divide by <span class="math notranslate nohighlight">\(3\)</span> (to normalize the area under the curve to <span class="math notranslate nohighlight">\(1\)</span>), obtaining a <em>kernel density estimate</em>. The width of the kernel is controlled by a parameter called <em>bandwidth</em>, denoted <span class="math notranslate nohighlight">\(h\)</span>, which coincides with the standard deviation of the normal distribution in the case of Gaussian KDE. So, large values of <span class="math notranslate nohighlight">\(h\)</span> correspond to wide kernels, and smaller values correspond to narrow or “spiky” kernels. Here are three examples of Gaussian KDE with different values of <span class="math notranslate nohighlight">\(h\)</span> for our toy dataset consisting of three points:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">bandwidths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>

<span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">bandwidths</span><span class="p">:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bandwidths</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
    <span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

    <span class="n">y1</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">y3</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;bandwidth $h=</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y1</span> <span class="o">+</span> <span class="n">y2</span> <span class="o">+</span> <span class="n">y3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sum of kernels&#39;</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y1</span> <span class="o">+</span> <span class="n">y2</span> <span class="o">+</span> <span class="n">y3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sum of kernels&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/f0b94ac870aed623805d2a3a3fec33bfe2f0c4a47b2ddf22b74f45edc48710c3.svg"><img alt="../_images/f0b94ac870aed623805d2a3a3fec33bfe2f0c4a47b2ddf22b74f45edc48710c3.svg" src="../_images/f0b94ac870aed623805d2a3a3fec33bfe2f0c4a47b2ddf22b74f45edc48710c3.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>Reading from left to right, here’s what’s going on:</p>
<ul class="simple">
<li><p>The plots in the first column display the original three data points.</p></li>
<li><p>In the second column, you see the normal density curves over top of the data points for three different values of the bandwidth <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p>In the third column, I have plotted the <em>sum</em> of the three normal density curves (divided by <span class="math notranslate nohighlight">\(3\)</span>), which are, by definition, the KDEs generated from the data set.</p></li>
<li><p>In the last column, I have plotted the KDEs on their own.</p></li>
</ul>
<p>Of course, nobody would ever use KDE for a dataset with only three numbers. But a KDE for our Airbnb dataset would consist of <span class="math notranslate nohighlight">\(m=13{,}239\)</span> kernels, one for each data point! Here are three KDEs for the Airbnb prices, with different bandwidths:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bandwidths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">bandwidths</span><span class="p">:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bandwidths</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">bw_method</span><span class="o">=</span><span class="n">h</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bandwidth $h=</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/b3e4e7f7e25c60deb52c5e14e07f2787d659d8b565032fa6a331bd055a72fcc2.svg"><img alt="../_images/b3e4e7f7e25c60deb52c5e14e07f2787d659d8b565032fa6a331bd055a72fcc2.svg" src="../_images/b3e4e7f7e25c60deb52c5e14e07f2787d659d8b565032fa6a331bd055a72fcc2.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>Smaller values of the bandwidth <span class="math notranslate nohighlight">\(h\)</span> correspond to “spikier” kernels and hence “spikier” density estimates. Larger values of <span class="math notranslate nohighlight">\(h\)</span> correspond to smoother estimates. In the first two KDEs above, the local peak around <span class="math notranslate nohighlight">\(\$500\)</span> is apparent, but the (relatively) large bandwidth <span class="math notranslate nohighlight">\(h=0.3\)</span> in the last KDE appears to have “over-smoothed” the estimate and we’ve lost most of that peak.</p>
<p>How do you choose the bandwidth in a KDE? Just like the “number of bins” parameter for histograms, you are best off first letting the computer decide the bandwidth for you, and then manually fine-tune it (if needed) until you get a KDE that you believe best represents the data.</p>
</section>
<section id="empirical-statistics">
<span id="empirical-stats"></span><h2><span class="section-number">6.5. </span>Empirical statistics<a class="headerlink" href="#empirical-statistics" title="Permalink to this heading">#</a></h2>
<p>Before we continue discussing more ways to <em>visualize</em> datasets, we need to discuss numerical summaries of datasets. This section is essentially a recapitulation of what you learned in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a> with a few new things thrown in at the end.</p>
<p>Let’s begin our discussion by returning to a general IID random sample</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m \sim F,
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(F\)</span> represents the (unknown) distribution function corresponding to a probabilistic model. Suppose that the model distribution has a mean <span class="math notranslate nohighlight">\(\mu\)</span> and a variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, so that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
E(X_i) = \mu \quad \text{and} \quad V(X_i) = \sigma^2,
\end{equation*}\]</div>
<p>for each <span class="math notranslate nohighlight">\(i=1,2,\ldots,n\)</span>. Based on an observed random sample</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_1,x_2,\ldots,x_m,
\end{equation*}\]</div>
<p>how might we estimate the unknown model parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>?</p>
<div class="proof definition admonition" id="empirical-stats-def">
<p class="admonition-title"><span class="caption-number">Definition 6.3 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>empirical mean</em> of a dataset <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> is defined to be the number</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\bar{x} = \frac{1}{m} \sum_{i=1}^m x_i,
\end{equation*}\]</div>
<p>while the <em>empirical variance</em> is defined to be the number</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
s^2 = \frac{1}{m-1} \sum_{i=1}^m (x_i - \bar{x})^2.
\end{equation*}\]</div>
<p>The <em>empirical standard deviation</em> <span class="math notranslate nohighlight">\(s\)</span> is defined as the positive square root of the empirical variance, <span class="math notranslate nohighlight">\(s = \sqrt{s^2}\)</span>.</p>
</section>
</div><p>The empirical mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and standard deviation <span class="math notranslate nohighlight">\(s\)</span> are supposed to serve as data-based estimates for the model mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>I should mention that the empirical quantities we just defined are often called the <em>sample mean</em>, <em>sample variance</em>, and <em>sample standard deviation</em>. However, as we will see later, our empirical quantities turn out to be observed values of certain <em>estimators</em> that are also called the <em>sample mean</em>, <em>sample variance</em>, and <em>sample standard deviation</em>. Since I believe that it is important—at least at first—to distinguish between an <strong>estimate</strong> and an <strong>estimator</strong>, I have decided to refer to <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(s^2\)</span>, and <span class="math notranslate nohighlight">\(s\)</span> as <em>empirical quantities</em> rather than <em>sample quantities</em>. In later chapters, however, I will not be so careful, and will refer to <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(s^2\)</span>, and <span class="math notranslate nohighlight">\(s\)</span> as <em>sample quantities</em>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>By the way, the replacement of <span class="math notranslate nohighlight">\(1/m\)</span> with <span class="math notranslate nohighlight">\(1/(m-1)\)</span> in the empirical variance is sometimes called <a class="reference external" href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>.</p>
</aside>
<p>The definitions of <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(s^2\)</span> are surely quite natural, <em>except</em> that the empirical variance involves division by <span class="math notranslate nohighlight">\(m-1\)</span> instead of the sample size <span class="math notranslate nohighlight">\(m\)</span> like you might have expected. The reason for this is that, if we had a factor of <span class="math notranslate nohighlight">\(1/m\)</span> in <span class="math notranslate nohighlight">\(s^2\)</span> instead of <span class="math notranslate nohighlight">\(1/(m-1)\)</span>, then the value of <span class="math notranslate nohighlight">\(s^2\)</span> would <em>systematically</em> underestimate the true value <span class="math notranslate nohighlight">\(\sigma^2\)</span> over repeated sampling. This can be demonstrated empirically through computer simulation, and it can also be <em>proved</em> theoretically as we will see later when we study bias of estimators. So for now, we will just take the above definition of <span class="math notranslate nohighlight">\(s^2\)</span> on faith, postponing till a later discussion the explanation regarding <em>why</em> it’s a good estimator.</p>
<p>Though the empirical quantities <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(s^2\)</span>, and <span class="math notranslate nohighlight">\(s\)</span> all have definitions that closely mimic their counterparts <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, and <span class="math notranslate nohighlight">\(\sigma\)</span>, the definition of the <em>empirical quantiles</em> of a dataset is a bit further removed from the definition of quantiles that we learned back in <a class="reference internal" href="04-random-variables.html#random-variables"><span class="std std-numref">Chapter 4</span></a>. Here is the definition:</p>
<div class="proof definition admonition" id="emp-quantile-defn">
<p class="admonition-title"><span class="caption-number">Definition 6.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be a dataset, written in non-decreasing order:</p>
<div class="math notranslate nohighlight" id="equation-listing-eqn">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-listing-eqn" title="Permalink to this equation">#</a></span>\[x_1 \leq x_2 \leq \cdots \leq x_m.\]</div>
<p>For each <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>, the datapoint <span class="math notranslate nohighlight">\(x_i\)</span> is called the <em>empirical <span class="math notranslate nohighlight">\(q_i\)</span>-quantile</em> where</p>
<div class="math notranslate nohighlight" id="equation-quantile-eqn">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-quantile-eqn" title="Permalink to this equation">#</a></span>\[q_i  = \frac{i-1}{m-1}.\]</div>
</section>
</div><p>This definition appeared in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>, where I explained that the intuition for the formula <a class="reference internal" href="#equation-quantile-eqn">(6.3)</a> for <span class="math notranslate nohighlight">\(q_i\)</span> is that it is precisely the proportion of data points (excluding) <span class="math notranslate nohighlight">\(x_i\)</span> that fall to the <em>left</em> of <span class="math notranslate nohighlight">\(x_i\)</span> in the listing <a class="reference internal" href="#equation-listing-eqn">(6.2)</a>. I also explained in that assignment how one goes about computing the empirical <span class="math notranslate nohighlight">\(q\)</span>-quantile where <span class="math notranslate nohighlight">\(q\)</span> is a number (between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, inclusive) that is <em>not</em> of the form <a class="reference internal" href="#equation-quantile-eqn">(6.3)</a>: The default method in the Pandas library is linear interpolation.</p>
<p>The empirical 0.25-, 0.5-, and 0.75-quantiles are called the <em>first</em>, <em>second</em>, and <em>third quartiles</em>. For the Airbnb dataset, these are listed in the following printout on the lines labeled <span class="math notranslate nohighlight">\(25\%\)</span>, <span class="math notranslate nohighlight">\(50\%\)</span> and <span class="math notranslate nohighlight">\(75\%\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    13253.000000
mean       167.168658
std        108.275932
min          0.190000
25%         88.840000
50%        137.130000
75%        216.840000
max        516.960000
Name: price, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Along with the empirical quartiles, you also see that this method from the Pandas library conveniently outputs the empirical mean and standard deviation, as well as the size of the dataset (the <em>count</em>) and the minimum and maximum sample values.</p>
<p>The range over which the middle 50% of a dataset sits is defined in:</p>
<div class="proof definition admonition" id="empirical-iqr-def">
<p class="admonition-title"><span class="caption-number">Definition 6.5 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>empirical interquartile range</em> (<em>empirical IQR</em>) of a dataset <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> is the difference</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(\text{empirical 0.75-quantile}) - (\text{empirical 0.25-quantile}).
\end{equation*}\]</div>
</section>
</div><p>So, using the outputs above, we see that the empirical IQR of the Airbnb dataset is:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iqr_airbnb</span> <span class="o">=</span> <span class="n">srs</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">-</span> <span class="n">srs</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">iqr_airbnb</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>128.0
</pre></div>
</div>
</div>
</div>
<p>With the definition of <em>empirical IQR</em> in hand, we may now define <em>outliers</em>:</p>
<div class="proof definition admonition" id="outlier-def">
<p class="admonition-title"><span class="caption-number">Definition 6.6 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be a dataset. Then a data point <span class="math notranslate nohighlight">\(x_i\)</span> is called an <em>outlier</em> if it is above an upper threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_i &gt; (\text{empirical 0.75-quantile}) + 1.5\times (\text{empirical IQR}),
\end{equation*}\]</div>
<p>or if it is below a lower threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_i &lt; (\text{empirical 0.25-quantile}) - 1.5\times (\text{empirical IQR}).
\end{equation*}\]</div>
</section>
</div><p>There’s a very convenient way to <em>visually</em> summarize all these empirical statistics (along with outliers) which we will discuss in the last section of this chapter.</p>
</section>
<section id="qq-plots">
<h2><span class="section-number">6.6. </span>QQ-plots<a class="headerlink" href="#qq-plots" title="Permalink to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>I should mention that there are other types of plots closely related to QQ-plots, called <em>probability plots</em> and <em>PP-plots</em>. In fact, there seems to be some disagreement as to whether what I am describing in this section actually <em>is</em> a QQ-plot. But this all seems to me to be uninteresting academic pedantry.</p>
</aside>
<p>We learned in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a> how to produce a plot of the empirical quantiles of a dataset. In this section, we will learn how to produce a plot that compares these empirical quantiles to the (theoretical) quantiles of a proposed model distribution. These new types of plots are called <em>quantile-quantile plots</em> or <em>QQ-plots</em>.</p>
<p>Though the basic idea behind a QQ-plot is quite simple, it demands that we slightly alter the definition of <em>empirical quantiles</em> given in the previous section and the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>. Indeed, according to that definition, the minimum and maximum values in a dataset are the <span class="math notranslate nohighlight">\(0\)</span>- and <span class="math notranslate nohighlight">\(1\)</span>-quantiles, respectively. But we will run into trouble if we are going to compare these to the quantiles of theoretical model distributions which might not have <span class="math notranslate nohighlight">\(0\)</span>- and <span class="math notranslate nohighlight">\(1\)</span>-quantiles.</p>
<p>To help motivate the new definition, for convenience, let’s suppose that the points in our dataset are labeled with <span class="math notranslate nohighlight">\(y\)</span>’s instead of <span class="math notranslate nohighlight">\(x\)</span>’s. (You’ll see why this is convenient, in just a moment.) Suppose that we put our dataset in non-decreasing order,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
y_1 \leq y_2 \leq \cdots \leq y_m,
\end{equation*}\]</div>
<p>where (as usual) <span class="math notranslate nohighlight">\(m\)</span> is the size of the dataset. Then, instead of identifying quantiles through the association</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
y_i \leftrightarrow q_i = \frac{i-1}{m-1}
\end{equation*}\]</div>
<p>as we did in <a class="reference internal" href="#emp-quantile-defn">Definition 6.4</a>, we instead make the association</p>
<div class="math notranslate nohighlight" id="equation-quant-eqn">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-quant-eqn" title="Permalink to this equation">#</a></span>\[y_i \leftrightarrow q_i =\frac{i-1/2}{m},\]</div>
<p>for <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>. For a specific example, suppose that <span class="math notranslate nohighlight">\(m=5\)</span> and that all the data points are distinct. Then, if we plot our dataset along an axis along with the labels <a class="reference internal" href="#equation-quant-eqn">(6.4)</a>, we get the following picture:</p>
<a class="reference internal image-reference" href="../_images/quant.svg"><img alt="../_images/quant.svg" class="align-center" src="../_images/quant.svg" width="80%" /></a>
<p> </p>
<p>Notice that the minimum and maximum values are no longer the <span class="math notranslate nohighlight">\(0\)</span>- and <span class="math notranslate nohighlight">\(1\)</span>-quantiles, but instead the <span class="math notranslate nohighlight">\(0.1\)</span>- and <span class="math notranslate nohighlight">\(0.9\)</span>-quantiles.</p>
<p>Now, suppose that we thought that our data was well modeled by a probability distribution with continuous distribution function <span class="math notranslate nohighlight">\(F\)</span> and quantile function <span class="math notranslate nohighlight">\(Q = F^{-1}\)</span>. Then, to construct the <em>QQ-plot</em> that compares the empirical quantiles to the model quantiles, we define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_i = Q\left( \frac{i-1/2}{m} \right)
\end{equation*}\]</div>
<p>for each <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>. In particular, note that <span class="math notranslate nohighlight">\(x_i\)</span> really <em>is</em> the <span class="math notranslate nohighlight">\((i-1/2)/m\)</span>-quantile of the model distribution, according to our earlier definition of <em>quantile</em> in <a class="reference internal" href="04-random-variables.html#random-variables"><span class="std std-numref">Chapter 4</span></a>. The QQ-plot then consists of those points</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(x_i,y_i), \quad i=1,2,\ldots,m.
\end{equation*}\]</div>
<p>As I mentioned, QQ-plots serve as another type of diagnostic plot that allow us to compare an empirical distribution to a proposed model distribution. Let’s see how this might work with our dataset of Airbnb prices. Remember, we originally thought that the Airbnb dataset <em>itself</em> might be well modeled by a normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the empirical mean and variance. But we saw through comparison of CDFs that this model did <em>not</em> fit the data well. The QQ-plot suggests the same:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">srs</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="n">srs</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>

<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;(normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/bd5cc07512b55fa9a9c2e7538feff3b7c37cb00771f1d6512967bd6015579190.svg"><img alt="../_images/bd5cc07512b55fa9a9c2e7538feff3b7c37cb00771f1d6512967bd6015579190.svg" src="../_images/bd5cc07512b55fa9a9c2e7538feff3b7c37cb00771f1d6512967bd6015579190.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>How do we interpret this plot? The idea is that, if the model distribution fit the dataset well, then the empirical quantiles should be reasonably close to the model quantiles. One can judge this “reasonable closeness” in the QQ-plot by checking how well the scattered points fit the diagonal red line (which has a slope of <span class="math notranslate nohighlight">\(1\)</span>, or 45 degrees). So, in our Airbnb example, it is clear that the scattered points are a poor fit for the diagonal line, which suggests our dataset is <em>not</em> accurately modeled by the proposed normal distribution.</p>
<p>But what if we just chose our parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> poorly, and the dataset is accurately modeled by <em>another</em> normal distribution with different parameters? In particular, what if we thought that the dataset was accurately modeled by a <em>standard</em> normal distribution? Here’s the relevant QQ-plot, to test our hypothesis:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;(standard normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;with diagonal line&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;(standard normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;without diagonal line&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/ac8affbb41806af8b71647642bdb55faeb0516f0c5751fcc2b88ab151f66815b.svg"><img alt="../_images/ac8affbb41806af8b71647642bdb55faeb0516f0c5751fcc2b88ab151f66815b.svg" src="../_images/ac8affbb41806af8b71647642bdb55faeb0516f0c5751fcc2b88ab151f66815b.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>The QQ-plot on the left (with the diagonal line) shows us that we picked an <em>even worse</em> model. The horizontal axis on the QQ-plot on the right has been re-scaled so that the scattered points do not appear to fall along a (nearly) vertical line, as they do in the left-hand plot.</p>
<p>The point I want to illustrate now is that the QQ-plot on the right—without the diagonal line, and with axes on different scales—may be used to judge whether our data is fit well by <em>some</em> normal distribution. Indeed, my goal is to justify the following:</p>
<blockquote>
<div><p><strong>Observation</strong>: What we are looking for in the (standard normal) QQ-plot on the right is whether the scattered points fall along <em>some</em> straight line <span class="math notranslate nohighlight">\(y = ax + b\)</span> (with <span class="math notranslate nohighlight">\(a&gt;0\)</span>). If they do, then the data is fit well by the normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>.</p>
</div></blockquote>
<p>This observation rests upon the fact that affine transformations of normal variables are still normal (which we saw back in <a class="reference internal" href="05-examples-of-rvs.html#examples"><span class="std std-numref">Chapter 5</span></a>).</p>
<p>To explain, suppose that the points in the QQ-plot fell <em>exactly</em> on a straight line, so that</p>
<div class="math notranslate nohighlight" id="equation-norm-eqn">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-norm-eqn" title="Permalink to this equation">#</a></span>\[y_i = ax_i + b, \quad i=1,2,\ldots,m,\]</div>
<p>for some <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> with <span class="math notranslate nohighlight">\(a &gt;0\)</span>. Then, let</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\phi(x) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^2}{2}\right)
\end{equation*}\]</div>
<p>be the density of the standard normal distribution, with associated distribution function</p>
<div class="math notranslate nohighlight">
\[
\Phi(x) = \int_{-\infty}^x \phi(t) \ \text{d} t.
\]</div>
<p>Now, if <a class="reference internal" href="#equation-norm-eqn">(6.5)</a> were true, then</p>
<div class="math notranslate nohighlight" id="equation-trans-eqn">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-trans-eqn" title="Permalink to this equation">#</a></span>\[\frac{i-1/2}{m} = \Phi\left( \frac{y_i-b}{a} \right) = \int_{-\infty}^{(y_i-b)/a} \phi(t) \ \text{d} t = \int_{-\infty}^{y_i} \frac{1}{a} \phi \left( \frac{s-b}{a}\right) \ \text{d} s,\]</div>
<p>where I made the substitution <span class="math notranslate nohighlight">\(t = (s-b)/a\)</span> in going from the first integral to the second. But notice that the transformed function</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\frac{1}{a} \phi \left( \frac{x-b}{a}\right) = \frac{1}{a\sqrt{2\pi}} \exp\left[ -\frac{1}{2} \left(\frac{x-b}{a}\right)^2\right]
\end{equation*}\]</div>
<p>is the density of <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>, and so <a class="reference internal" href="#equation-trans-eqn">(6.6)</a> shows that, provided <a class="reference internal" href="#equation-norm-eqn">(6.5)</a> is true, the data point <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\((i-1/2)/m\)</span>-quantile of <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>. Thus, the empirical quantiles match the (theoretical) model quantiles of <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>, which justifies the observation in the box above.</p>
<p>So, the standard normal model is a bad fit—even worse than the first normal model. But remember that we discovered the Airbnb dataset is <em>log-normal</em>, in the sense that its log transform is well modeled by a normal distribution. To confirm this, let’s check a QQ-plot of the log-transformed data against standard normal quantiles:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs_log</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;(standard normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles (w/log transform)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/23dc0525f7506f8d2b9ea0a28b5803947b48078a120411d6974a1830ee1ae471.svg"><img alt="../_images/23dc0525f7506f8d2b9ea0a28b5803947b48078a120411d6974a1830ee1ae471.svg" src="../_images/23dc0525f7506f8d2b9ea0a28b5803947b48078a120411d6974a1830ee1ae471.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>Notice that the points fall mostly along a straight line, except for a few points with log prices <span class="math notranslate nohighlight">\(\leq 2\)</span>, and a chunk of points with log prices near <span class="math notranslate nohighlight">\(6\)</span>. This latter discrepancy is another manifestation of the local peak in the dataset near a log price of <span class="math notranslate nohighlight">\(6\)</span>, or a price of <span class="math notranslate nohighlight">\(\$500\)</span>.</p>
<p>By visual inspection, it appears that the scattered points fall mostly along the line <span class="math notranslate nohighlight">\(y = 0.6x + 4.8\)</span>. This suggests that the best-fit normal model for the log-transformed data should be <span class="math notranslate nohighlight">\(\mathcal{N}(4.8, 0.6^2)\)</span>. Note how closely these parameter values match the empirical statistics of the log transformed data:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs_log</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;log price&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    13253.000000
mean         4.911520
std          0.672302
min         -1.660731
25%          4.486837
50%          4.920929
75%          5.379160
max          6.247966
Name: log price, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Instead of comparing the empirical quantiles to the normal model after log-transforming the data, it is instructive to compare the untransformed empirical quantiles directly to the log-normal model via a QQ-plot. This is the result:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">srs_log</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">srs_log</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>

<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;(log-normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/57eeb33163a3c98259b533254d1f6e4ea5978bc98711d4b99cb6a400030d546c.svg"><img alt="../_images/57eeb33163a3c98259b533254d1f6e4ea5978bc98711d4b99cb6a400030d546c.svg" src="../_images/57eeb33163a3c98259b533254d1f6e4ea5978bc98711d4b99cb6a400030d546c.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>Again, the discrepancy occurs near a price of <span class="math notranslate nohighlight">\(\$500\)</span>. Above this price, notice that the model quantiles are <em>larger</em> than the empirical quantiles. This is consistent with a local peak in the dataset near <span class="math notranslate nohighlight">\(\$500\)</span> that is not captured by the model. The intuition for this is best explained by comparing the data density function (obtained via KDE) against the model density function:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">srs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="mi">600</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data KDE&#39;</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">600</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;log-normal PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/75e03335476d9dd17fa54bba3ba0f6108b276cec22d2f04f2b2aaee2f6e73d35.svg"><img alt="../_images/75e03335476d9dd17fa54bba3ba0f6108b276cec22d2f04f2b2aaee2f6e73d35.svg" src="../_images/75e03335476d9dd17fa54bba3ba0f6108b276cec22d2f04f2b2aaee2f6e73d35.svg" width="70%" /></a>
</figure>
</div>
</div>
<p>Imagine sweeping from left to right across the horizontal axis approaching the peak near <span class="math notranslate nohighlight">\(\$500\)</span>, all the while accumulating probability (as area under the curve) from both the empirical and model distributions. Since the points along the QQ-plot fall mostly along a straight line till this peak, the empirical quantiles <span class="math notranslate nohighlight">\(y_i\)</span> and model quantiles <span class="math notranslate nohighlight">\(x_i\)</span> are in nearly the same positions along the horizontal axis (i.e, <span class="math notranslate nohighlight">\(y_i \approx x_i\)</span>). But as we begin entering the region near the peak, we begin to accumulate <em>more</em> probability from the empirical distribution compared to the model distribution because the empirical density (and hence the area under the curve) begins to enter its local peak. This means that the empirical quantiles will begin to bunch more closely together compared to the model quantiles since you must step <em>further</em> along the horizontal axis to accumulate the same amount of probability from the model distribution as you would from the empirical distribution over a shorter step. Thus, the model quantiles will be larger than the empirical quantiles.</p>
</section>
<section id="box-plots-and-violin-plots">
<h2><span class="section-number">6.7. </span>Box plots and violin plots<a class="headerlink" href="#box-plots-and-violin-plots" title="Permalink to this heading">#</a></h2>
<p>We finish the chapter with a discussion of two more methods to visualize datasets and empirical distributions. To begin, let’s consider our Airbnb data and all the empirical statistics that we described and computed <a class="reference internal" href="#empirical-stats"><span class="std std-numref">Section 6.5</span></a>. We may combine all this information in something called a <em>box plot</em> (or <em>box and whisker plot</em>):</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/2821d46081cf789d0dfcd77d09ec6370fcfccdaf8016ac6561df1d1ea0f4d447.svg"><img alt="../_images/2821d46081cf789d0dfcd77d09ec6370fcfccdaf8016ac6561df1d1ea0f4d447.svg" src="../_images/2821d46081cf789d0dfcd77d09ec6370fcfccdaf8016ac6561df1d1ea0f4d447.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>The left edge of the blue box is at <span class="math notranslate nohighlight">\(x=88.84\)</span>, which is the empirical <span class="math notranslate nohighlight">\(0.25\)</span>-quantile or first quartile; its right edge is at <span class="math notranslate nohighlight">\(x=216.84\)</span>, which is the empirical 0.75-quantile or third quartile. Therefore, the <em>width</em> of the box is exactly the empirical IQR. The box thus represents where the middle 50% of the dataset lives. The vertical line through the box is at <span class="math notranslate nohighlight">\(x=137.13\)</span>, which is the empirical 0.5-quantile or the empirical median.</p>
<p>You notice that the box has “whiskers.” In general, the left whisker in a box plot either extends out to the minimum value in the dataset <em>or</em> to the threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(\text{empirical 0.25-quantile}) - 1.5\times (\text{empirical IQR})
\end{equation*}\]</div>
<p>for determining outliers, whichever is greater. In the case of our Airbnb data, the whisker extends to the minimum value at <span class="math notranslate nohighlight">\(x=0.19\)</span> (19 cents—really?).</p>
<p>Likewise, the right whisker in general either extends out to the maximum value in the dataset <em>or</em> to the upper threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(\text{empirical 0.75-quantile}) + 1.5\times (\text{empirical IQR})
\end{equation*}\]</div>
<p>for determining outliers, whichever is smaller. In the case of our Airbnb data, the dataset <em>does</em> contain outliers in its upper tail, so the whisker extends to the threshold value, and all the dots to the right are outliers. Notice that these outliers are bunched so closely together that they actually appear as a solid, thick grey line in the plot.</p>
<p>Now, what were to happen if we combined a box plot and a KDE? We’d get something like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;paper&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/efa3944aa29fd983df68d89ba021a44ccda0d8f684e4675929c28730e64a61e4.svg"><img alt="../_images/efa3944aa29fd983df68d89ba021a44ccda0d8f684e4675929c28730e64a61e4.svg" src="../_images/efa3944aa29fd983df68d89ba021a44ccda0d8f684e4675929c28730e64a61e4.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>This is a <em>violin plot</em>—the reason for the name is evident. Along the central horizontal line is a box plot—can you see it? The white dot in the box represents the empirical median, while you can see the upper and lower whiskers as well. Then, above the central horizontal line is displayed a KDE of the dataset, and its mirror image is displayed below. For comparison, here’s a picture of a KDE of the dataset all on its own:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">clip</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">600</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/5ae75b9e5212e2960fb2cf2722c80eab1f61882e4c63b49cfc4cf550fe3f2cd0.svg"><img alt="../_images/5ae75b9e5212e2960fb2cf2722c80eab1f61882e4c63b49cfc4cf550fe3f2cd0.svg" src="../_images/5ae75b9e5212e2960fb2cf2722c80eab1f61882e4c63b49cfc4cf550fe3f2cd0.svg" width="100%" /></a>
</figure>
</div>
</div>
<p>So, violin plots are tools belonging to both graphical and numerical exploratory data analysis since they combine KDEs with box plots. They have advantages over just plain box plots because they are better able to convey a sense of the <em>shape</em> of a dataset. For example, box plots cannot display multiple data modes (multiple peaks in the distribution), whereas KDEs <em>can</em>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05-examples-of-rvs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Examples of random variables</p>
      </div>
    </a>
    <a class="right-next"
       href="07-random-vectors.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Random vectors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-and-random-samples">6.1. Data and random samples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-models-and-empirical-distributions">6.2. Probabilistic models and empirical distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">6.3. Histograms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-density-estimation">6.4. Kernel density estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-statistics">6.5. Empirical statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qq-plots">6.6. QQ-plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#box-plots-and-violin-plots">6.7. Box plots and violin plots</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>