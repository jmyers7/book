
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>11. Optimization &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bpi": "\\boldsymbol\\pi", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bf": "\\mathbf{f}", "bg": "\\mathbf{g}", "bp": "\\mathbf{p}", "br": "\\mathbf{r}", "bs": "\\mathbf{s}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bD": "\\mathbf{D}", "bH": "\\mathbf{H}", "bI": "\\mathbf{I}", "bK": "\\mathbf{K}", "bS": "\\mathbf{S}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bW": "\\mathbf{W}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calJ": "\\mathcal{J}", "calH": "\\mathcal{H}", "calI": "\\mathcal{I}", "calL": "\\mathcal{L}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "calS": "\\mathcal{S}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/11-optim';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12. Probabilistic graphical models" href="12-models.html" />
    <link rel="prev" title="10. Information theory" href="10-info-theory.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-halfway.html">9. The halfway point: pivoting toward models and data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-info-theory.html">10. Information theory</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-models.html">12. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-learning.html">13. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-stats-estimators.html">14. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-asymptotic.html">15. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-CIs.html">16. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-hyp-test.html">17. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-lin-reg.html">18. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">19. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/11-optim.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">11.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-geometry">11.2. Differential geometry</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">11.3. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">11.4. Stochastic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-convex-functions">11.5. Appendix: convex functions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<span id="optim"></span><h1><span class="section-number">11. </span>Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h1>
<p>Lest the reader wonder why an entire chapter on optimization is included in a textbook on probability and statistics, we suggest that they recall the discussion in the expository chapter <a class="reference internal" href="09-halfway.html#halfway"><span class="std std-numref">Chapter 9</span></a>. There, we described the rationale: Our overarching goal over the current sequence of chapters is to train probabilistic models on data. To do this, we minimize the KL divergence between the proposed model distribution <span class="math notranslate nohighlight">\(p(x,y;\theta)\)</span> and the empirical distribution <span class="math notranslate nohighlight">\(\hat{p}(x,y)\)</span> of the dataset, and thereby turn “learning” into an optimization problem:</p>
<a class="reference internal image-reference" href="../_images/prob-distance.svg"><img alt="../_images/prob-distance.svg" class="align-center" src="../_images/prob-distance.svg" width="75%" /></a>
<p> </p>
<p>Many of the optimization problems we will encounter do not have closed-form solutions, and so we will need to study methods for approximation. All those studied in this chapter are versions of an iterative method called <em>gradient descent</em>. For a warm-up, we will study a simple single-variable version of this method in <a class="reference internal" href="#univariate-grad-desc-sec"><span class="std std-numref">Section 11.1</span></a> before proceeding to the full multi-variable version in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Section 11.3</span></a>. Sandwiched between these two sections is <a class="reference internal" href="#curvature-der-sec"><span class="std std-numref">Section 11.2</span></a>, where we review the tools from elementary differential geometry that allow the generalization from one variable to many. Then, we finish with <a class="reference internal" href="#sgd-sec"><span class="std std-numref">Section 11.4</span></a>, where we study a particular form of gradient descent, called <em>stochastic gradient descent</em>, that is specifically tailored for the objective functions encountered in training probabilistic models. An <a class="reference internal" href="#app-conv-sec"><span class="std std-ref">appendix</span></a> is incuded at the end of the chapter, containing some basic theorems on convex functions and their (rather fussy) proofs.</p>
<p>As hinted in the first paragraph, the inclusion of gradient-based optimization algorithms and their applications to parameter estimation is what distinguishes this book from a traditional book on mathematical statistics. This material is often included in texts on machine learning, but it is not in any text on statistics (that I know of). However, we are just <em>barely</em> scratching the surface of optimization and machine learning. If you are new to these fields and want to learn more, I suggest beginning with the fifth chapter of <span id="id1">[<a class="reference internal" href="bib.html#id10" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT Press, 2016.">GBC16</a>]</span> for a quick overview. After this, you can move on to <span id="id2">[<a class="reference internal" href="bib.html#id11" title="M. Hardt and B. Recht. Patterns, predictions, and actions. Foundations of machine learning. Princeton University Press, 2022.">HR22</a>]</span>, before tackling the massive, encyclopedic texts <span id="id3">[<a class="reference internal" href="bib.html#id8" title="K. P. Murphy. Probabilistic machine learning. An introduction. MIT Press, 2022.">Mur22</a>]</span> and <span id="id4">[<a class="reference internal" href="bib.html#id9" title="K. P. Murphy. Probabilistic machine learning. Advanced topics. MIT Press, 2023.">Mur23</a>]</span>.</p>
<section id="gradient-descent-in-one-variable">
<span id="univariate-grad-desc-sec"></span><h2><span class="section-number">11.1. </span>Gradient descent in one variable<a class="headerlink" href="#gradient-descent-in-one-variable" title="Link to this heading">#</a></h2>
<p>In this section, we describe the single-variable version of the gradient descent algorithm to help motivate the general algorithm in arbitrary dimensions. To begin, consider the <em>optimization problem</em> of locating the minimum values of the polynomial function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \theta^4 - 6\theta^3 + 11\theta^2 - 7\theta + 4.
\]</div>
<p>This function is called the <em>objective function</em> of the optimization problem. Its graph is displayed in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions.multivariate_normal</span> <span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">from</span> <span class="nn">math_stats_ml.gd</span> <span class="kn">import</span> <span class="n">GD</span><span class="p">,</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">plot_gd</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">11</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">7</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/8226566f8af09e9a73e196864537a8df70ba6cd5c8504678dad41f59c08c63cb.svg" src="../_images/8226566f8af09e9a73e196864537a8df70ba6cd5c8504678dad41f59c08c63cb.svg" /></figure>
</div>
</div>
<p>From the graph, we see that the objective function has minimums of approximately <span class="math notranslate nohighlight">\(J(0.5)\)</span> and <span class="math notranslate nohighlight">\(J(2.7)\)</span>.</p>
<p>It will be convenient to introduce the following terminology for our optimization problems:</p>
<div class="proof definition admonition" id="extrema-def">
<p class="admonition-title"><span class="caption-number">Definition 11.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J: \bbr^n \to \bbr\)</span> be a function. A vector <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a <em>local minimizer</em> of <span class="math notranslate nohighlight">\(J(\btheta)\)</span> provided that</p>
<div class="math notranslate nohighlight">
\[
J(\btheta^\star) \leq J(\btheta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\btheta\)</span> in a neighborhood of <span class="math notranslate nohighlight">\(\btheta^\star\)</span>; if this inequality holds for <em>all</em> <span class="math notranslate nohighlight">\(\btheta\)</span>, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is called a <em>global minimizer</em> of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>. If we flip the inequality the other direction, then we obtain the definitions of <em>local</em> and <em>global maximizers</em>. Collectively, local and global minimizers and maximizers of <span class="math notranslate nohighlight">\(J(\btheta)\)</span> are called <em>extremizers</em>, and the values <span class="math notranslate nohighlight">\(J(\btheta^\star)\)</span> of the function where <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is an extremizer are called <em>extrema</em> or <em>extreme values</em>.</p>
</section>
</div><p>Using this terminology, we would say that <span class="math notranslate nohighlight">\(0.5\)</span> is (approximately) a local minimizer of our polynomial objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span>, while <span class="math notranslate nohighlight">\(2.7\)</span> is (approximately) a global minimizer.</p>
<p>Let’s see how the single-variable version of the <em>gradient descent algorithm</em> would solve our optimization problem. This algorithm depends on an initial guess for a minimizer, as well as two parameters called the <em>learning rate</em> and the <em>number of gradient steps</em>. We will state the algorithm first, and then walk through some intuition for why it works:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The loop runs from <span class="math notranslate nohighlight">\(t=0\)</span> to <span class="math notranslate nohighlight">\(t=N-1\)</span>, inclusive. This convention is intended to match the implementation of <code class="docutils literal notranslate"><span class="pre">for</span></code> loops in Python for <code class="docutils literal notranslate"><span class="pre">t</span></code> ranging through the iterable <code class="docutils literal notranslate"><span class="pre">range(N)</span></code>.</p>
</aside>
<div class="proof algorithm admonition" id="single-var-gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.1 </span> (Single-variable gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<hr class="docutils" />
<p>   <span class="math notranslate nohighlight">\(\theta := \theta_0\)</span> <br>
   For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do: <br>
           <span class="math notranslate nohighlight">\(\theta := \theta - \alpha J'(\theta)\)</span> <br>
   Return <span class="math notranslate nohighlight">\(\theta\)</span></p>
</section>
</div><p>Though this simple description of the algorithm outputs just a single approximation to a local minimizer, in practice it is often convenient to output the entire sequence of <span class="math notranslate nohighlight">\(\theta\)</span>’s produced as the algorithm iterates through the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop:</p>
<div class="math notranslate nohighlight">
\[
\theta_0,\theta_1,\ldots,\theta_t,\ldots,\theta_{N}.
\]</div>
<p>Then, as we will see below, by tracking the associated objective values</p>
<div class="math notranslate nohighlight">
\[
J(\theta_0), J(\theta_1),\ldots, J(\theta_t), \ldots, J(\theta_N),
\]</div>
<p>one may monitor convergence of the algorithm.</p>
<p>The assignment</p>
<div class="math notranslate nohighlight" id="equation-update-rule-eqn">
<span class="eqno">(11.1)<a class="headerlink" href="#equation-update-rule-eqn" title="Link to this equation">#</a></span>\[
\theta := \theta - \alpha J'(\theta)
\]</div>
<p>in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is called the <em>update rule</em>. This form is convenient for implementation in code, but for theoretical analysis, it is often convenient to rewrite the rule as a <em>recurrence relation</em> in the form</p>
<div class="math notranslate nohighlight">
\[
\theta_{t+1} = \theta_{t} - \alpha J'(\theta_{t}),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 0\)</span>. We say that the new parameter <span class="math notranslate nohighlight">\(\theta_{t+1}\)</span> is obtained by taking a <em>gradient step</em> from <span class="math notranslate nohighlight">\(\theta_{t}\)</span>. The first update occurs when <span class="math notranslate nohighlight">\(t=0\)</span>, yielding</p>
<div class="math notranslate nohighlight">
\[
\theta_1 = \theta_{0} - \alpha J'(\theta_{0}).
\]</div>
<p>To understand the intuition behind the algorithm, consider the two cases that the derivative <span class="math notranslate nohighlight">\(J'(\theta_0)\)</span> is positive or negative:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J_prime</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="mi">18</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">22</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="mi">7</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">+</span> <span class="mf">0.4</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="mf">3.3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">-</span> <span class="mf">3.3</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">3.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="s2">&quot;$J&#39;(</span><span class="se">\\</span><span class="s2">theta_0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="s2">&quot;$J&#39;(</span><span class="se">\\</span><span class="s2">theta_0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">12.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/71723f0a68c5b0856b89c0f8b0e5c6d0aca2206937813e40012c37382ef0339a.svg" src="../_images/71723f0a68c5b0856b89c0f8b0e5c6d0aca2206937813e40012c37382ef0339a.svg" /></figure>
</div>
</div>
<p>In this plot, we’ve drawn the tangent lines to the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> at two initial values <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and <span class="math notranslate nohighlight">\(\theta_0=3.3\)</span>. Since the derivatives are the slopes of these tangent lines, the sign of the derivative is negative when <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and positive when <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span>. In the first case, we have</p>
<div class="math notranslate nohighlight" id="equation-first-update-eqn">
<span class="eqno">(11.2)<a class="headerlink" href="#equation-first-update-eqn" title="Link to this equation">#</a></span>\[\theta_1 = -0.4 - \alpha J'(-0.4) &gt; -0.4\]</div>
<p>since <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, while in the second case we have</p>
<div class="math notranslate nohighlight" id="equation-second-update-eqn">
<span class="eqno">(11.3)<a class="headerlink" href="#equation-second-update-eqn" title="Link to this equation">#</a></span>\[\theta_1 = 3.3 - \alpha J'(3.3) &lt; 3.3.\]</div>
<p>But notice that the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = -0.4\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 0.5\)</span>, and so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-first-update-eqn">(11.2)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than the initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span>, provided that the (scaled) negative derivative</p>
<div class="math notranslate nohighlight" id="equation-neg-derivative-eqn">
<span class="eqno">(11.4)<a class="headerlink" href="#equation-neg-derivative-eqn" title="Link to this equation">#</a></span>\[-\alpha J'(\theta_0)\]</div>
<p>is not too large (in magnitude) causing the new <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” the minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Similarly, the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>, so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-second-update-eqn">(11.3)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than <span class="math notranslate nohighlight">\(\theta_0\)</span>, again provided that the (scaled) negative derivative <a class="reference internal" href="#equation-neg-derivative-eqn">(11.4)</a> is not too large in magnitude.</p>
<p>From these considerations, we conclude the following:</p>
<div class="proof observation admonition" id="gd-obs">
<p class="admonition-title"><span class="caption-number">Observation 11.1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>The negative derivative <span class="math notranslate nohighlight">\(-J'(\theta)\)</span> always “points downhill.”</p></li>
<li><p>When the gradient descent algorithm works, it locates a minimizer by following the negative derivative “downhill.”</p></li>
</ul>
</section>
</div><p>The sense in which the negative derivative “points downhill” is made precise by our observation that it is positive if the point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> sits on a decreasing portion of the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>, and it is negative if <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> is on an increasing portion of the graph. The role of the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is to scale down the magnitude of the negative derivative so that the gradient step in the update rule does not cause <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” a nearby minimizer.</p>
<p>Let’s run the gradient algorithm four times, with various settings of the parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">])],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">2e-1</span><span class="p">]}</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/695cb0ae4b8db21bc42d43d0f2c5d901d70cc8c10578eb0a4bb1da07d83d7d8b.svg" src="../_images/695cb0ae4b8db21bc42d43d0f2c5d901d70cc8c10578eb0a4bb1da07d83d7d8b.svg" /></figure>
</div>
</div>
<p>In all four plots, the large magenta dot represents the initial point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span>, while the smaller dots represent the remaining <span class="math notranslate nohighlight">\(N\)</span> points</p>
<div class="math notranslate nohighlight">
\[
(\theta_1,J(\theta_1)), (\theta_2, J(\theta_2)),\ldots, (\theta_N,J(\theta_N)),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of gradient steps. In the first row of the figure, the algorithm appears to be converging in both cases to the nearest minimizer to the initial guesses. In the second row, the learning rate is (relatively) large, causing the first gradient steps to “overshoot” the nearest minimizers to the initial guesses. However, the algorithm still appears to converge in both cases.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 1 on the worksheet.</p>
</div>
<p>It is possible for the gradient descent algorithm to diverge, especially if the learning rate is too large. For example, suppose that we set the learning rate to <span class="math notranslate nohighlight">\(\alpha = 0.2\)</span> and use <span class="math notranslate nohighlight">\(\theta_0 = 3.5\)</span> as our initial guess. Then three steps of gradient descent produce the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">55</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/8cedfbec6230b355b406dce8c7fccb054f0f4352323c1506a12dff0baca23bad.svg" src="../_images/8cedfbec6230b355b406dce8c7fccb054f0f4352323c1506a12dff0baca23bad.svg" /></figure>
</div>
</div>
<p>We see already that <span class="math notranslate nohighlight">\(J(\theta_3) \approx 10^7\)</span>; in fact, we have <span class="math notranslate nohighlight">\(J(\theta_t) \to \infty\)</span> as <span class="math notranslate nohighlight">\(t\to\infty\)</span> for these particular parameters.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problems 2 and 3 on the worksheet.</p>
</div>
<p>Of course, one can often prevent divergence by simply using a smaller learning rate, but sometimes a large <em>initial</em> learning rate is desirable to help the algorithm quickly find the neighborhood of a minimizer. So, what we desire is a scheme to shrink the learning rate from large values to smaller ones as the algorithm runs. This scheme is called a <em>learning rate schedule</em>, and it is implemented by adding an extra <em>decay rate</em> parameter to the gradient descent algorithm:</p>
<div class="proof algorithm admonition" id="single-variable-gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.2 </span> (Single-variable gradient descent with learning rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\beta \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<hr class="docutils" />
<p>   <span class="math notranslate nohighlight">\(\theta := \theta_0\)</span> <br>
   For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do: <br>
           <span class="math notranslate nohighlight">\(\theta := \theta - \alpha (1-\beta)^{t+1} J'(\theta)\)</span> <br>
   Return <span class="math notranslate nohighlight">\(\theta\)</span></p>
</section>
</div><p>The new parameter <span class="math notranslate nohighlight">\(\beta\)</span>, called the <em>decay rate</em>, shrinks the learning rate as</p>
<div class="math notranslate nohighlight">
\[
\alpha (1-\beta) &gt; \alpha (1-\beta)^2 &gt;\cdots &gt; \alpha (1-\beta)^t &gt; \cdots &gt; \alpha (1-\beta)^N,
\]</div>
<p>provided that <span class="math notranslate nohighlight">\(\beta&gt; 0\)</span>. Setting <span class="math notranslate nohighlight">\(\beta=0\)</span> results in <em>no</em> change in the learning rate. In our diverging example above, setting the decay rate to <span class="math notranslate nohighlight">\(\beta=0.1\)</span> results in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2e-1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/92ebf1f8f5d2b94ee3bbc2c2c1b267481f71459dcb489f6410a639461863c37a.svg" src="../_images/92ebf1f8f5d2b94ee3bbc2c2c1b267481f71459dcb489f6410a639461863c37a.svg" /></figure>
</div>
</div>
<p>We have carried out <span class="math notranslate nohighlight">\(N=8\)</span> gradient steps, and it appears that the algorithm has successfully located the minimizer <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 4 on the worksheet.</p>
</div>
<p>The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> and decay rate <span class="math notranslate nohighlight">\(\beta\)</span> are often chosen by experimentation, and they may be tuned by the analyst by closely monitoring the values of the objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> as the algorithm runs. This is easy in the single-variable case, since one can plot the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>. In the multi-variable case, however, the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> may live in many more dimensions than we can visualize, so the analyst might track the values of the objective function against the number of gradient steps. For example, with our polynomial objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> from above and</p>
<div class="math notranslate nohighlight">
\[
\theta_0 = -0.5, \quad \alpha = 0.01, \quad \beta = 0.1,
\]</div>
<p>we would plot the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">parameter_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">,</span> <span class="n">per_step_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c848a68cc31faa3a5cd5ee829e1423c87b5e581b8c9cb522eaf3fad1d6b20272.svg" src="../_images/c848a68cc31faa3a5cd5ee829e1423c87b5e581b8c9cb522eaf3fad1d6b20272.svg" /></figure>
</div>
</div>
<p>One may use this plot to decide on the total number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps; simply choose <span class="math notranslate nohighlight">\(N\)</span> large enough to reach a point where the plot “plateaus” or “levels out,” indicating that the algorithm is converging on a minimizer. Alternatively, the analyst may build an automatic stopping condition into the algorithm that halts when the magnitude between successive objective values is less than some chosen threshold, say</p>
<div class="math notranslate nohighlight">
\[
|J(\theta_t) - J(\theta_{t-1}) | &lt; \epsilon,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> is a small number.</p>
</section>
<section id="differential-geometry">
<span id="curvature-der-sec"></span><h2><span class="section-number">11.2. </span>Differential geometry<a class="headerlink" href="#differential-geometry" title="Link to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(\theta^\star\)</span> is an extremizer of a differentiable function <span class="math notranslate nohighlight">\(J:\bbr \to \bbr\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> must be a <em>stationary point</em> in the sense that</p>
<div class="math notranslate nohighlight" id="equation-stationary-eqn">
<span class="eqno">(11.5)<a class="headerlink" href="#equation-stationary-eqn" title="Link to this equation">#</a></span>\[
J'(\theta^\star)=0.
\]</div>
<p>The name arises from the observation that small (first-order infinitesimal) perturbations of <span class="math notranslate nohighlight">\(\theta^\star\)</span> do not change the value <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span>, i.e., the value <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span> remains <em>stationary</em> under small perturbations. In certain very favorable situations, we may be able to solve the stationarity equation <a class="reference internal" href="#equation-stationary-eqn">(11.5)</a> for <span class="math notranslate nohighlight">\(\theta^\star\)</span> to obtain a formula in closed form. In this case, the iterative gradient descent algorithm is not needed. But <span class="math notranslate nohighlight">\(\theta^\star\)</span> being a solution to the stationarity equation <a class="reference internal" href="#equation-stationary-eqn">(11.5)</a> is only a <em>necessary</em> condition for it to be an extremizer—sufficient conditions may be obtained by considering the <em>local curvature</em> of the graph of <span class="math notranslate nohighlight">\(J\)</span> near <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<p>Our goal in this section is twofold: First, we briefly recall how these curvature considerations help us identify extremizers in the single-variable case—the relevant tools are the first and second derivatives. Then, we generalize these derivatives to higher dimensions to obtain <em>gradient vectors</em> and <em>Hessian matrices</em>. We indicate how local curvature in higher dimensions may be computed using these new tools and, in particular, how we may use them to identify extremizers.</p>
<p>So, let’s begin with the familiar routine from single-variable calculus called the <em>Second Derivative Test</em>. Given a point <span class="math notranslate nohighlight">\(\theta^\star\)</span> and a twice-differentiable function <span class="math notranslate nohighlight">\(J:\bbr \to \bbr\)</span>, the test splits into two cases:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(J'(\theta^\star) = 0\)</span> and <span class="math notranslate nohighlight">\(J''(\theta^\star) &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a local minimizer.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(J'(\theta^\star) = 0\)</span> and <span class="math notranslate nohighlight">\(J''(\theta^\star) &lt; 0\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a local maximizer.</p></li>
</ol>
<p>Provided that the second derivative of <span class="math notranslate nohighlight">\(J\)</span> not only exists but is also <em>continuous</em>, then intuition for the Second Derivative Test may be explained via local curvature. Indeed, in the first case, positivity of the second derivative means that the graph of <span class="math notranslate nohighlight">\(J\)</span> is <em>convex</em> near <span class="math notranslate nohighlight">\(\theta^\star\)</span>, while in the second case negativity of the second derivative means that the graph is <em>concave</em>. The nature of the curvature helps us distinguish between minimizers and maximizers:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">functions</span><span class="p">,</span> <span class="n">axes</span><span class="p">)):</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;$J &#39;(0) = 0$, $J &#39;&#39;(0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;convex $</span><span class="se">\\</span><span class="s2">Rightarrow$ minimizer&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;$J &#39;(0) = 0$, $J &#39;&#39;(0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;concave $</span><span class="se">\\</span><span class="s2">Rightarrow$ maximizer&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/0b1cb3eee0b7d8786c7cb81bb5567854f306c6cf00b02c50530619a91f2d865b.svg" src="../_images/0b1cb3eee0b7d8786c7cb81bb5567854f306c6cf00b02c50530619a91f2d865b.svg" /></figure>
</div>
</div>
<p>To see <em>why</em> second derivatives encode local curvature, let’s suppose <span class="math notranslate nohighlight">\(J''(\theta^\star) &gt;0\)</span> at some point <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Then continuity of <span class="math notranslate nohighlight">\(J''\)</span> means that there is a number <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> such that <span class="math notranslate nohighlight">\(J''(\theta)&gt;0\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span> in the open interval <span class="math notranslate nohighlight">\(I = (\theta^\star - \epsilon, \theta^\star + \epsilon)\)</span> centered at <span class="math notranslate nohighlight">\(\theta^\star\)</span>. But the second derivative is the first derivative of the first derivative, and thus positivity of <span class="math notranslate nohighlight">\(J''\)</span> over <span class="math notranslate nohighlight">\(I\)</span> means that <span class="math notranslate nohighlight">\(J'\)</span> is increasing over <span class="math notranslate nohighlight">\(I\)</span>. Since the first derivative <span class="math notranslate nohighlight">\(J'\)</span> measures the slope of the graph of <span class="math notranslate nohighlight">\(J\)</span>, this must mean that the slopes increase as we move from left to right across <span class="math notranslate nohighlight">\(I\)</span>. Thus, <span class="math notranslate nohighlight">\(J\)</span> is convex near <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<p>We already met the notion of <em>concavity</em> back in <a class="reference internal" href="10-info-theory.html#kl-div-sec"><span class="std std-numref">Section 10.2</span></a> where it was fundamental in our proof of Gibb’s inequality in <a class="reference internal" href="10-info-theory.html#gibbs-thm">Theorem 10.3</a>. As shown in the figure above, a function is <em>concave</em> if it lies below its secant lines, while it is <em>convex</em> if it lies above. Both these shapes have implications for the search for extremizers—in particular, stationary points of concave and convex functions are <em>always</em> global extremizers. The proof of this claim, along with equivalent characterizations of concavity and convexity in terms of tangent lines and tangent (hyper)planes are given in the two main theorems in the <a class="reference internal" href="#app-conv-sec"><span class="std std-ref">appendix</span></a>. (The claims are easily believable, while the proofs are annoyingly fussy. At least glance at the statements of the theorems to convince yourself that your intuition is on point, but do not feel compelled to go through the proofs line by line.)</p>
<p>How might we generalize the Second Derivative Test to higher dimensions? To help gain insight into the answer, let’s first add only one additional dimension, going from a function of a single variable to a twice-differentiable function of two variables:</p>
<div class="math notranslate nohighlight">
\[
J:\bbr^2 \to \bbr, \quad \btheta \mapsto J(\btheta).
\]</div>
<p>Actually, to obtain the best results relating curvature to second derivatives, we shall assume that the second-order partial derivatives are <em>continuous</em>, though this isn’t strictly needed for some definitions and results below. Functions with continuous first- and second-order partial derivatives are said to be of <em>class <span class="math notranslate nohighlight">\(C^2\)</span></em> in the mathematical literature.</p>
<p>For example, let’s suppose that the graph of <span class="math notranslate nohighlight">\(J\)</span> is an upside down paraboloid:</p>
<a class="reference internal image-reference" href="../_images/paraboloid-plot.png"><img alt="../_images/paraboloid-plot.png" class="align-center" src="../_images/paraboloid-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>At any given point on this surface (like the one above the black dot) there is not a <em>single</em> slope and curvature, but rather <em>infinitely many</em> slopes and curvatures in all the different directions that one may step in the plane <span class="math notranslate nohighlight">\(\bbr^2\)</span>. These different directions may be represented as <em>directional vectors</em> in the plane; here are three examples:</p>
<a class="reference internal image-reference" href="../_images/directional-plot.png"><img alt="../_images/directional-plot.png" class="align-center" src="../_images/directional-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>Taking advantage of the very special circumstance that our graph is embedded as a surface in <span class="math notranslate nohighlight">\(\bbr^3\)</span>, we may visualize the slopes and curvatures in these three directions by first intersecting the graph with three vertical planes:</p>
<a class="reference internal image-reference" href="../_images/sectional-plot.png"><img alt="../_images/sectional-plot.png" class="align-center" src="../_images/sectional-plot.png" style="width: 75%;" /></a>
<p> </p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This figure and discussion might be slightly misleading, for in dimensions <span class="math notranslate nohighlight">\(\geq 3\)</span>, the curves on the surface that we are after are <em>not</em> obtained as intersections with hyperplanes. This figure very much relies on the fact that our graph is embedded in <span class="math notranslate nohighlight">\(\bbr^3\)</span>.</p>
</aside>
<p>The intersections of these vertical planes and the surface yield curves called <em>sections</em>—for the planes displayed in the plot above, the sections are a trio of downward opening parabolas. The slopes and curvatures on the surface in the three directions are then the slopes and curvatures of these sectional curves.</p>
<p>To obtain these slopes and curvatures, let’s suppose that <span class="math notranslate nohighlight">\(\bv\)</span> is one of the three directional vectors in the plane, with its tail hooked to the point <span class="math notranslate nohighlight">\(\btheta\)</span> represented by the black dot. As we let <span class="math notranslate nohighlight">\(t\in \bbr\)</span> vary, the vector sum</p>
<div class="math notranslate nohighlight">
\[
t \bv + \btheta
\]</div>
<p>traces out the line in the plane <span class="math notranslate nohighlight">\(\bbr^2\)</span> through <span class="math notranslate nohighlight">\(\btheta\)</span> and in the direction of <span class="math notranslate nohighlight">\(\bv\)</span>. It follows that the mapping</p>
<div class="math notranslate nohighlight">
\[
t\mapsto J(t\bv + \btheta)
\]</div>
<p>is exactly the sectional curve on the surface. Notice that this mapping is a real-valued function of a single real variable <span class="math notranslate nohighlight">\(t\)</span>, and thus it has first and second derivatives in the ordinary sense from single-variable calculus. These considerations motivate the following definition, which applies to functions of <span class="math notranslate nohighlight">\(n\)</span> variables (not just two).</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>We are <em>not</em> requiring directional vectors to have unit length!</p>
</aside>
<div class="proof definition admonition" id="directional-der-def">
<p class="admonition-title"><span class="caption-number">Definition 11.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point, and <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span> a vector. We define the <em>directional first derivative of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> in the direction <span class="math notranslate nohighlight">\(\bv\)</span></em> to be</p>
<div class="math notranslate nohighlight">
\[
J_\bv'(\btheta) \def \frac{\text{d}}{\text{d}t} \bigg|_{t=0} J(t\bv + \btheta),
\]</div>
<p>while we define the <em>directional second derivative</em> to be</p>
<div class="math notranslate nohighlight">
\[
J_\bv''(\btheta) \def \frac{\text{d}^2}{\text{d}t^2} \bigg|_{t=0} J(t\bv + \btheta).
\]</div>
<p>In this context, the vector <span class="math notranslate nohighlight">\(\bv\)</span> is called the <em>directional vector</em>.</p>
</section>
</div><div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 5 on the worksheet.</p>
</div>
<p>The familiar relations between these directional derivatives and partial derivatives pass through the gadgets defined in the following box. The first is familiar to us from our course in multi-variable calculus:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The notation “<span class="math notranslate nohighlight">\(\nabla^2\)</span>” is also sometimes used for the <a class="reference external" href="https://en.wikipedia.org/wiki/Laplace_operator#">Laplace</a> operator!</p>
</aside>
<div class="proof definition admonition" id="grad-vec-def">
<p class="admonition-title"><span class="caption-number">Definition 11.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point. We define the <em>gradient vector</em> to be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla J(\btheta) \def \begin{bmatrix} \displaystyle \frac{\partial J}{\partial \theta_i}(\btheta) \end{bmatrix} =
\begin{bmatrix}
\displaystyle\frac{\partial J}{\partial \theta_1}(\btheta) \\
\vdots \\
\displaystyle \frac{\partial J}{\partial \theta_n}(\btheta)
\end{bmatrix} \in \bbr^n,
\end{split}\]</div>
<p>while we define the the <em>Hessian matrix</em> to be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla^2 J(\btheta) \def \begin{bmatrix} \displaystyle \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}(\btheta) \end{bmatrix}
= \begin{bmatrix}
\displaystyle\frac{\partial^2 J}{\partial \theta_1^2}(\btheta) &amp; \cdots &amp;\displaystyle \frac{\partial^2 J}{\partial \theta_1 \partial \theta_n}(\btheta) \\
\vdots &amp; \ddots &amp; \vdots \\
\displaystyle\frac{\partial^2 J}{\partial \theta_n \partial \theta_1} (\btheta) &amp; \cdots &amp; \displaystyle\frac{\partial^2 J}{\partial \theta_n^2}(\btheta)
\end{bmatrix} \in \bbr^{n\times n}.
\end{split}\]</div>
</section>
</div><p>Note that since <span class="math notranslate nohighlight">\(J\)</span> is of class <span class="math notranslate nohighlight">\(C^2\)</span>, the Hessian matrix is symmetric.</p>
<p>We have already met the symbol “<span class="math notranslate nohighlight">\(\nabla\)</span>” back in <a class="reference internal" href="08-more-prob.html#gradient-mat-def">Definition 8.2</a> where we defined the gradient matrix of a vector-valued function</p>
<div class="math notranslate nohighlight">
\[
\bf: \bbr^n \to \bbr^m, \quad \bx \mapsto \bf(\bx).
\]</div>
<p>In fact, if we write</p>
<div class="math notranslate nohighlight">
\[
\bf(\bx)^\intercal = (f_1(\bx),\ldots,f_m(\bx)),
\]</div>
<p>then the gradient matrix of <span class="math notranslate nohighlight">\(\bf\)</span> is exactly the matrix whose columns are the gradient vectors of the component functions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla \bf(\bx) = \begin{bmatrix}
\uparrow &amp; \cdots &amp; \uparrow \\
\nabla f_1(\bx) &amp; \cdots &amp; \nabla f_m(\bx) \\
\downarrow &amp; \cdots &amp; \downarrow
\end{bmatrix}.
\end{split}\]</div>
<p>Of course, a function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> may be considered a vector-valued function when we view the target space <span class="math notranslate nohighlight">\(\bbr\)</span> as a <span class="math notranslate nohighlight">\(1\)</span>-dimensional vector space, and then clearly there is no inconsistency in the notation since the gradient vector of <span class="math notranslate nohighlight">\(J\)</span> is the same as its gradient matrix (at <span class="math notranslate nohighlight">\(\btheta\)</span>). This dual usage of the symbol <span class="math notranslate nohighlight">\(\nabla\)</span> then helps explain the notation <span class="math notranslate nohighlight">\(\nabla^2\)</span> for the Hessian matrix; precisely, we have:</p>
<div class="proof theorem admonition" id="hess-jac-grad-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.1 </span> (Hessian matrices are gradient matrices of gradient vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point. Then we have the equality of matrices</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 J(\btheta) = \nabla\left(\nabla J \right)(\btheta),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla J\)</span> on the right-hand side stands for the vector-valued function</p>
<div class="math notranslate nohighlight">
\[
\nabla J: \bbr^n \to \bbr^n, \quad \btheta \mapsto \nabla J(\btheta).
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. By definition, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla J(\btheta) = \begin{bmatrix}
\displaystyle\frac{\partial J}{\partial \theta_1}(\btheta) \\
\vdots \\
\displaystyle \frac{\partial J}{\partial \theta_n}(\btheta)
\end{bmatrix},
\end{split}\]</div>
<p>and so</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla\left(\nabla J \right)(\btheta) = \begin{bmatrix}
\uparrow &amp; \cdots &amp; \uparrow \\
\nabla \left( \displaystyle\frac{\partial J}{\partial \theta_1}\right)(\btheta) &amp; \cdots &amp; \nabla \left( \displaystyle\frac{\partial J}{\partial \theta_n}\right)(\btheta) \\
\downarrow &amp; \cdots &amp; \downarrow
\end{bmatrix} = \nabla^2 J(\btheta).
\end{split}\]</div>
</div>
<p>The following important theorem expresses the relations between the first and second directional derivatives and the gradient vector and Hessian matrix.</p>
<div class="proof theorem admonition" id="directional-der-grad-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.2 </span> (Slopes, curvatures, and partial derivatives)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point, and <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span> a directional vector.</p>
<ol class="arabic">
<li><p>We have</p>
<div class="math notranslate nohighlight">
\[
    J_{\bv}'(\btheta) = \bv^\intercal \nabla J(\btheta).
    \]</div>
</li>
<li><p>We have</p>
<div class="math notranslate nohighlight">
\[
    J_{\bv}''(\btheta) = \bv^\intercal \nabla^2 J(\btheta) \bv.
    \]</div>
</li>
</ol>
</section>
</div><p>So, the directional first derivative is obtained through an inner product with the gradient vector, while the directional second derivative is the quadratic form induced by the Hessian matrix.</p>
<div class="proof admonition" id="proof">
<p>Proof. The proofs are simple exercises using the multi-variable Chain Rule. Indeed, note that</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d}}{\text{d}t} J(t \bv + \btheta) = \sum_{i=1}^n v_i \frac{\partial J}{\partial \theta_i} (t\bv + \btheta).
\]</div>
<p>Plugging in <span class="math notranslate nohighlight">\(t=0\)</span> to both sides of this last equality then yields (1.). On the other hand, differentiating both sides of the equation with respect to <span class="math notranslate nohighlight">\(t\)</span> (and using the Chain Rule a second time) gives</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d}^2}{\text{d}t^2} J(t \bv + \btheta) = \sum_{i=1}^n v_i \frac{\text{d}}{\text{d}t}\frac{\partial J}{\partial \theta_i} (t\bv + \btheta)  = \sum_{i,j=1}^n v_i v_j \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}(t\bv + \btheta).
\]</div>
<p>Plugging in <span class="math notranslate nohighlight">\(t=0\)</span> to both ends yields (2.). Q.E.D.</p>
</div>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 6 on the worksheet.</p>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Notice that the value of <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is the same if <span class="math notranslate nohighlight">\(\bv\)</span> is replaced with <span class="math notranslate nohighlight">\(-\bv\)</span>, by <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>. Thus, the local curvature at <span class="math notranslate nohighlight">\(\btheta\)</span> only depends on the <em>line</em> indicated by <span class="math notranslate nohighlight">\(\bv\)</span>, not the direction that it points.</p>
</aside>
<p>When the directional vector <span class="math notranslate nohighlight">\(\bv\)</span> is a <em>unit</em> vector, the value of the directional first derivative <span class="math notranslate nohighlight">\(J'_\bv(\btheta)\)</span> is interpreted as the (instantaneous) rate of change of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> in the direction indicated by <span class="math notranslate nohighlight">\(\bv\)</span>. Likewise, if <span class="math notranslate nohighlight">\(\bv\)</span> is unit vector, then the value of the directional second derivative <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is interpreted as the local curvature of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> through the line indicated by <span class="math notranslate nohighlight">\(\bv\)</span>.</p>
<p>For our purposes, the most important properties of the gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> are (1) that it points in the direction of <em>maximum</em> rate of change, (2) its negative points in the direction of <em>minimum</em> rate of change, and (3) it is orthogonal to the <em>level surfaces</em> of <span class="math notranslate nohighlight">\(J\)</span>, otherwise called <em>contours</em>. By definition, such a surface is the <span class="math notranslate nohighlight">\((n-1)\)</span>-dimensional set of solutions <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> to an equation</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = c
\]</div>
<p>for fixed <span class="math notranslate nohighlight">\(c\in \bbr\)</span>. In the case that <span class="math notranslate nohighlight">\(n=2\)</span>, these level surfaces are actually <em>level curves</em>; for our upside down paraboloid above, they are the blue ellipses in the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">15</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">8</span> <span class="o">/</span> <span class="mf">3.5</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.75</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.75</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.5</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f58093ce06d80ad5beec6238b3e738ec30e7f0682233ba615c5892a04cfe95c5.svg" src="../_images/f58093ce06d80ad5beec6238b3e738ec30e7f0682233ba615c5892a04cfe95c5.svg" /></figure>
</div>
</div>
<p>The vector in the figure is the gradient vector, and the magenta line is the tangent line to the contour that passes through the point at the tail of the gradient. Note that the gradient is orthogonal to this tangent line, and therefore also orthogonal to the contour.</p>
<p>In general, it should be intuitively clear from the equation</p>
<div class="math notranslate nohighlight">
\[
J_{\bv}'(\btheta) = \bv^\intercal J(\btheta)
\]</div>
<p>in <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a> that the gradient is orthogonal to level surfaces. Indeed, if <span class="math notranslate nohighlight">\(\bv\)</span> is a tangent vector to the level surface passing through <span class="math notranslate nohighlight">\(\btheta\)</span>, then <span class="math notranslate nohighlight">\(J\)</span> should <em>not</em> change (at least up to first order) as we step in the direction of <span class="math notranslate nohighlight">\(\bv\)</span> since (by definition) the function <span class="math notranslate nohighlight">\(J\)</span> is constant along its level surfaces. Thus, we have <span class="math notranslate nohighlight">\(J_\bv'(\btheta)=0\)</span>, and so <span class="math notranslate nohighlight">\(\bv^\intercal \nabla J(\btheta) =0\)</span>. This shows the gradient vector is indeed orthogonal to the level surface passing through <span class="math notranslate nohighlight">\(\btheta\)</span>. (For a more rigorous argument, see the proposition on page 23 of <span id="id5">[<a class="reference internal" href="bib.html#id13" title="V. Guillemin and A. Pollack. Differential topology. AMS Chelsea Publishing, 2010.">GP10</a>]</span>.)</p>
<p>Let’s return to the first two properties of the gradient vector mentioned above, that it points in the direction of maximum rate of change and its negative points in the direction of minimum rate of change. In informal treatments, these claims are often justified by an appeal to the “angle” <span class="math notranslate nohighlight">\(\phi\in [0,\pi]\)</span> between two vectors <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span>, which allegedly fits into an equation</p>
<div class="math notranslate nohighlight" id="equation-geom-dot-eq">
<span class="eqno">(11.6)<a class="headerlink" href="#equation-geom-dot-eq" title="Link to this equation">#</a></span>\[
\bv^\intercal \bu = |\bu||\bv| \cos{\phi}.
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(\bu = \nabla J(\btheta)\)</span> and supposing <span class="math notranslate nohighlight">\(\bv\)</span> has unit length, we get</p>
<div class="math notranslate nohighlight">
\[
J_\bv'(\btheta) = \bv^\intercal \nabla J(\btheta) = |\nabla J(\btheta) | \cos{\phi}
\]</div>
<p>from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>. Since <span class="math notranslate nohighlight">\(\cos{\phi}\)</span> is maximized and minimized over <span class="math notranslate nohighlight">\([0,\pi]\)</span> when <span class="math notranslate nohighlight">\(\phi=0\)</span> and <span class="math notranslate nohighlight">\(\phi=\pi\)</span>, respectively, it then follows that the gradient points in the direction of maximum rate of change, while its negative points in the direction of minimum rate of change. However, this argument does not address what is meant by the “angle” <span class="math notranslate nohighlight">\(\phi\)</span> between the vectors—certainly in two and three dimensions we have some idea of what this angle might be, but what about in 1000 dimensions?</p>
<p>But the “angle” <span class="math notranslate nohighlight">\(\phi\)</span> is just a distraction. It is much cleaner logically to work directly with the <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a>, which is the true reason that the gradient has these extremizing properties. An immediate corollary of this inequality says that an innner product of one vector against a unit vector is maximized (minimized) when the unit vector points in the same (opposite) direction as the first vector. In the following, we give a proof of the extremizing properties of the gradient vector using the Cauchy-Schwarz inequality, and then show afterwards how it may be used to give a rigorous definition of the angle <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<div class="proof theorem admonition" id="grad-uphill-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.3 </span> (Properties of gradient vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point, and suppose the gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> is nonzero.</p>
<ol class="arabic simple">
<li><p>The gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> points in the direction of maximum rate of change.</p></li>
<li><p>The negative gradient vector <span class="math notranslate nohighlight">\(-\nabla J(\btheta)\)</span> points in the direction of minimum rate of change.</p></li>
<li><p>The gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> is orthogonal to level surfaces.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We already offered a “proof” of the third statement above, so we need only prove the first two. For this, we recall that for any two vectors <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span>, the Cauchy-Schwarz inequality states that</p>
<div class="math notranslate nohighlight" id="equation-cs-ineq-eq">
<span class="eqno">(11.7)<a class="headerlink" href="#equation-cs-ineq-eq" title="Link to this equation">#</a></span>\[
|\bu^\intercal \bv| \leq |\bu | |\bv|,
\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span> are parallel (i.e., one is a scalar multiple of the other). In particular, if we take <span class="math notranslate nohighlight">\(\bu = \nabla J(\btheta)\)</span>, let <span class="math notranslate nohighlight">\(\bv\)</span> be a unit vector, and use <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>, we get</p>
<div class="math notranslate nohighlight">
\[
-|\nabla J(\btheta )| \leq J_\bv'(\btheta) = \bv^ \intercal \nabla J(\btheta) \leq |\nabla J(\btheta )|.
\]</div>
<p>The goal is then to identify unit vectors <span class="math notranslate nohighlight">\(\bv\)</span> that extremize the derivative <span class="math notranslate nohighlight">\(J_\bv'(\btheta)\)</span> in these bounds.</p>
<p>But if the derivative achieves the upper bound, then by the criterion for equality in the Cauchy-Schwarz inequality <a class="reference internal" href="#equation-cs-ineq-eq">(11.7)</a>, there must be a nonzero scalar <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\bv = \alpha \nabla J(\btheta).
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\alpha |\nabla J(\btheta)|^2 = \bv^\intercal \nabla J(\btheta) = |\nabla J(\btheta)|,
\]</div>
<p>and so <span class="math notranslate nohighlight">\(\alpha = 1/ |\nabla J(\btheta)|\)</span>. Hence, the derivative <span class="math notranslate nohighlight">\(J_\bv'(\btheta)\)</span> achieves its maximum value exactly when <span class="math notranslate nohighlight">\(\bv\)</span> is the normalized gradient vector. It is just as easy to show that the derivative achieves its minimum value when <span class="math notranslate nohighlight">\(\bv\)</span> is the negative of the normalized gradient vector. Q.E.D.</p>
</div>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 7 on the worksheet.</p>
</div>
<p>Now, let’s return to the “angle” <span class="math notranslate nohighlight">\(\phi\)</span> between two nonzero vectors <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span> in <span class="math notranslate nohighlight">\(\bbr^n\)</span>. From the Cauchy-Schwarz inequality <a class="reference internal" href="#equation-cs-ineq-eq">(11.7)</a>, it follows that</p>
<div class="math notranslate nohighlight">
\[
-1 \leq \frac{\bv^\intercal \bu}{|\bu||\bv|} \leq 1.
\]</div>
<p>We then <em>define</em> the <em>angle</em> <span class="math notranslate nohighlight">\(\phi\)</span> between the two vectors to be the unique number <span class="math notranslate nohighlight">\(\phi \in [0,\pi]\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\cos{\phi} = \frac{\bv^\intercal \bu}{|\bu||\bv|}.
\]</div>
<p>Thus, the fact that the angle <span class="math notranslate nohighlight">\(\phi\)</span> even <em>exists</em> is a consequence of the Cauchy-Schwarz inequality.</p>
<p>Observe that the part in <a class="reference internal" href="#grad-uphill-thm">Theorem 11.3</a> about the negative gradient vector “pointing downhill” is the higher-dimensional version of the observation in <a class="reference internal" href="#gd-obs">Observation 11.1</a> regarding the negative derivative of a single-variable function. This property will be key to the general multi-variable gradient descent algorithm that we will discuss in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Section 11.3</span></a> below.</p>
<p>Let’s now turn toward extremizers of a multi-variable function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span> in arbitrary dimension <span class="math notranslate nohighlight">\(n\)</span>. As you may recall from multi-variable calculus, the stationarity equation</p>
<div class="math notranslate nohighlight">
\[
\nabla J(\btheta^\star) = 0
\]</div>
<p>is a <em>necessary</em> condition for <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to be an extremizer of <span class="math notranslate nohighlight">\(J\)</span>. As in the single-variable case, one might hope to classify stationary points (i.e., solutions <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to the stationarity equation) as minimizers and maximizers based on the local curvature of <span class="math notranslate nohighlight">\(J\)</span>. For if <span class="math notranslate nohighlight">\(J\)</span> is convex (concave) in <em>all</em> directions at <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, then intuition suggests that <span class="math notranslate nohighlight">\(\btheta^\star\)</span> should be a local minimizer (maximizer). But from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>, the local directional curvatures at <span class="math notranslate nohighlight">\(\btheta^\star\)</span> are measured by the quadratic form</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal \nabla^2 J(\btheta^\star)  \bv
\]</div>
<p>as <span class="math notranslate nohighlight">\(\bv\)</span> cycles through all nonzero vectors in <span class="math notranslate nohighlight">\(\bbr^n\)</span>. Thus, if these numbers are <em>always</em> positive (negative), then we would expect the stationary point <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer (maximizer). However, to say that these numbers are either always positive or negative means exactly that the Hessian matrix is positive definite or negative definite, in the language of <a class="reference internal" href="08-more-prob.html#first-semidefinite-def">Definition 8.9</a>. So, the question becomes: If we know that the Hessian matrix is positive (negative) definite at a stationary point <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, is <span class="math notranslate nohighlight">\(\btheta^\star\)</span> necessarily a local minimizer (maximizer)?</p>
<p>The answer is <em>yes</em>!</p>
<div class="proof theorem admonition" id="second-der-test-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.4 </span> (Second Derivative Test)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star \in \bbr^n\)</span> a stationary point.</p>
<ol class="arabic simple">
<li><p>If the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta^\star)\)</span> is positive definite, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer.</p></li>
<li><p>If the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta^\star)\)</span> is negative definite, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local maximizer.</p></li>
</ol>
</section>
</div><p>For a proof of this result, see Theorem 13.10 in <span id="id6">[<a class="reference internal" href="bib.html#id14" title="T. Apostol. Mathematical analysis. Addison-Weley Publishing Comparny, Inc., second edition, 1974.">Apo74</a>]</span>. Note also that if the Hessian matrix is either positive semidefinite or negative semidefinite <em>everywhere</em>, then every stationary point is a global extremizer; see <a class="reference internal" href="#main-convex-multi-thm">Theorem 11.8</a> in the appendix.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 8 on the worksheet.</p>
</div>
<p>With infinitely many local (directional) curvatures at a point on the graph of a function <span class="math notranslate nohighlight">\(J:\bbr^n\to \bbr\)</span>, it will be convenient to obtain a single number that attempts to summarize the complexity of the local curvature. The first step toward obtaining such a summary is given in the following:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Remember, as we saw in <a class="reference internal" href="08-more-prob.html#psd-char-thm">Theorem 8.19</a>, the eigenvalues of a positive definite matrix are all real and positive. Hence the linear ordering <a class="reference internal" href="#equation-ordering-eqn">(11.8)</a>. The existence of the orthonormal basis of eigenvectors is guaranteed by the Spectral Theorem (see the proof of <a class="reference internal" href="08-more-prob.html#psd-char-thm">Theorem 8.19</a>).</p>
</aside>
<div class="proof theorem admonition" id="max-min-curve-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.5 </span> (Eigenvalues, eigenvectors, and local curvature)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point with positive definite Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta)\)</span>. Suppose we linearly order the eigenvalues of the Hessian matrix as</p>
<div class="math notranslate nohighlight" id="equation-ordering-eqn">
<span class="eqno">(11.8)<a class="headerlink" href="#equation-ordering-eqn" title="Link to this equation">#</a></span>\[
0 &lt; \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n.
\]</div>
<p>Then:</p>
<ol class="arabic simple">
<li><p>The directional curvature <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is maximized exactly when <span class="math notranslate nohighlight">\(\bv\)</span> lies in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_n\)</span>, in which case <span class="math notranslate nohighlight">\(J''_\bv(\btheta) = \lambda_n\)</span>.</p></li>
<li><p>The directional curvature <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is minimized exactly when <span class="math notranslate nohighlight">\(\bv\)</span> lies in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_1\)</span>, in which case <span class="math notranslate nohighlight">\(J''_\bv(\btheta) = \lambda_1\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(\be_1,\ldots,\be_n\)</span> be the associated orthonormal basis of eigenvectors with</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 J(\btheta) \be_i = \lambda_i\be_i
\]</div>
<p>for each <span class="math notranslate nohighlight">\(i\)</span>. Given a unit vector <span class="math notranslate nohighlight">\(\bv\)</span>, there are unique scalars <span class="math notranslate nohighlight">\(\alpha_1,\ldots,\alpha_n\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\bv = \alpha_1 \be_1 + \cdots + \alpha_n \be_n
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-sum-to-one-eqn">
<span class="eqno">(11.9)<a class="headerlink" href="#equation-sum-to-one-eqn" title="Link to this equation">#</a></span>\[
\alpha_1^2 + \cdots + \alpha_n^2 =1.
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
J''_\bv(\btheta) = \bv^\intercal \nabla^2 J(\btheta)  \bv = \sum_{i,j=1}^n \alpha_i\alpha_j \be_i^\intercal \nabla^2 J(\btheta)  \be_j = \sum_{i,j=1}^n \alpha_i\alpha_j\lambda_j \be_i^\intercal \be_j = \sum_{i=1}^n \alpha_i^2 \lambda_i,
\]</div>
<p>where the first equality follows from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>. Using <a class="reference internal" href="#equation-sum-to-one-eqn">(11.9)</a>, eliminate <span class="math notranslate nohighlight">\(\alpha_n^2\)</span> from the last sum in favor of the other <span class="math notranslate nohighlight">\(\alpha\)</span>’s to get</p>
<div class="math notranslate nohighlight">
\[
J''_\bv(\btheta) = \sum_{i=1}^{n-1}(\lambda_i - \lambda_n)\alpha_i^2 + \lambda_n.
\]</div>
<p>Letting <span class="math notranslate nohighlight">\(m\)</span> be the smallest integer such that</p>
<div class="math notranslate nohighlight">
\[
\lambda_{m-1} &lt; \lambda_m = \lambda_{m+1} = \cdots = \lambda _n,
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
J''_\bv(\btheta) = \sum_{i=1}^{m-1}(\lambda_i - \lambda_n)\alpha_i^2 + \lambda_n.
\]</div>
<p>(If <span class="math notranslate nohighlight">\(m=1\)</span>, then we interpret this expression as <span class="math notranslate nohighlight">\(J''_\bv(\btheta) = \lambda_n\)</span>.) But <span class="math notranslate nohighlight">\(\lambda_i -\lambda_n &lt; 0\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m-1\)</span>, and so <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is clearly maximized when</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 = \cdots = \alpha_{m-1} = 0,
\]</div>
<p>which implies that <span class="math notranslate nohighlight">\(\bv\)</span> lies in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_n\)</span>. This establishes the claim in the first statement, and the one in the second follows from the same type of argument with the obvious changes. Q.E.D.</p>
</div>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 9 on the worksheet.</p>
</div>
<p>If the Hessian matrix is positive definite, then its extreme eigenvalues are exactly the extreme local (directional) curvatures. The ratio of the largest curvature to the smallest should then convey the “variance” or the “range” of these curvatures. This ratio has a name:</p>
<div class="proof definition admonition" id="condition-num-def">
<p class="admonition-title"><span class="caption-number">Definition 11.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\bA\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> square matrix.</p>
<ol class="arabic">
<li><p>The <em>spectrum</em> of <span class="math notranslate nohighlight">\(\bA\)</span>, denoted <span class="math notranslate nohighlight">\(\sigma(\bA)\)</span>, is the set of eigenvalues of <span class="math notranslate nohighlight">\(\bA\)</span>.</p></li>
<li><p>The <em>spectral radius</em> of <span class="math notranslate nohighlight">\(\bA\)</span>, denoted <span class="math notranslate nohighlight">\(\rho(\bA)\)</span>, is given by</p>
<div class="math notranslate nohighlight">
\[
    \rho(\bA) \def \max_{\lambda \in \sigma(\bA)} |\lambda|.
    \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(\bA\)</span> is positive definite, the <em>condition number</em> of <span class="math notranslate nohighlight">\(\bA\)</span>, denoted <span class="math notranslate nohighlight">\(\kappa(\bA)\)</span>, is the ratio</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\bA) \def \frac{\lambda_\text{max}}{\lambda_\text{min}}
    \]</div>
<p>of the largest eigenvalue of <span class="math notranslate nohighlight">\(\bA\)</span> to the smallest.</p>
</li>
</ol>
</section>
</div><p>This definition of <em>condition number</em> applies only in the case that <span class="math notranslate nohighlight">\(\bA\)</span> is positive definite and hence all its eigenvalues are positive. In the general case, the definition needs to be altered; see <a class="reference external" href="https://en.wikipedia.org/wiki/Condition_number#Matrices">here</a>, for example.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 10 on the worksheet.</p>
</div>
<p>Intuitively, when the condition number of a positive definite Hessian matrix is large (in which case the Hessian matrix is called <em>ill-conditioned</em>), the curvatures vary widely as we look in all different directions; conversely, when the condition number is near <span class="math notranslate nohighlight">\(1\)</span>, the directional curvatures are all nearly the same. As we will see in the next section, ill-conditioned Hessian matrices inflate an important upper-bound on the speed of convergence of gradient descent. In other words, ill-conditioned Hessian matrices <em>may</em> signal slow convergence of gradient descent.</p>
</section>
<section id="gradient-descent-in-multiple-variables">
<span id="multivariate-grad-desc-sec"></span><h2><span class="section-number">11.3. </span>Gradient descent in multiple variables<a class="headerlink" href="#gradient-descent-in-multiple-variables" title="Link to this heading">#</a></h2>
<p>With the gradient vector taking the place of the derivative, it is easy to generalize the single-variable gradient descent algorithm from <a class="reference internal" href="#single-variable-gd-alg">Algorithm 11.2</a> to multiple variables:</p>
<div class="proof algorithm admonition" id="gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.3 </span> (Multi-variable gradient descent with learning rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable function <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^n\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\beta \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<hr class="docutils" />
<p>   <span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span> <br>
   For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do: <br>
           <span class="math notranslate nohighlight">\(\btheta := \btheta - \alpha(1-\beta)^{t+1} \nabla J(\btheta)\)</span> <br>
   Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</section>
</div><p>Just like the single-variable version, in practice it is convenient to track the entire sequence of <span class="math notranslate nohighlight">\(\btheta\)</span>’s produced by the algorithm:</p>
<div class="math notranslate nohighlight">
\[
\btheta_0,\btheta_1, \ldots,\btheta_t,\ldots,\btheta_N.
\]</div>
<p>For an example, let’s consider the polynomial objective function</p>
<div class="math notranslate nohighlight" id="equation-two-dim-poly-eq">
<span class="eqno">(11.10)<a class="headerlink" href="#equation-two-dim-poly-eq" title="Link to this equation">#</a></span>\[
J:\bbr^2 \to \bbr, \quad J(\btheta) = J(\theta_1,\theta_2) = (\theta_1^2 + 10 \theta_2^2)\big((\theta_1-1)^2 + 10(\theta_2-1)^2 \big)
\]</div>
<p>in two dimensions. Its graph looks like</p>
<a class="reference internal image-reference" href="../_images/objective-plot.png"><img alt="../_images/objective-plot.png" class="align-center" src="../_images/objective-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>while its contour plot is</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
   <span class="n">theta1</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
   <span class="n">theta2</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
   <span class="k">return</span> <span class="p">(</span><span class="n">theta1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">theta2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">theta1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mf">0.50</span><span class="p">:</span><span class="mf">1.5</span><span class="p">:</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">:</span><span class="mf">1.3</span><span class="p">:</span><span class="mf">0.01</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/02a7825da1cb4f7c59bb8e90d2730a64f4e4cf1d3a86ff97da0df2f6ab9e738c.svg" src="../_images/02a7825da1cb4f7c59bb8e90d2730a64f4e4cf1d3a86ff97da0df2f6ab9e738c.svg" /></figure>
</div>
</div>
<p>The function has two minimizers at</p>
<div class="math notranslate nohighlight">
\[
\btheta^\star = (0, 0), (1,1),
\]</div>
<p>as well as a “saddle point” at <span class="math notranslate nohighlight">\((0.5, 0.5)\)</span> where the gradient <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> vanishes. Let’s run the gradient descent algorithm four times, beginning with <em>no</em> learning rate decay, and track the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> plotted over the contours of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">])]}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                   <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                   <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                   <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
        
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;first runs of gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/1703e09b6d115e8da09a319460084304825f17c71d7a3b6c58bdb216390ddb57.svg" src="../_images/1703e09b6d115e8da09a319460084304825f17c71d7a3b6c58bdb216390ddb57.svg" /></figure>
</div>
</div>
<p>The large magenta dots in the plots indicate the initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span>, while the smaller dots indicate the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> for <span class="math notranslate nohighlight">\(t&gt;0\)</span>. The algorithm <em>appears</em> to be converging nicely to the minimizer <span class="math notranslate nohighlight">\(\btheta^\star = (1,1)\)</span> in the upper-left plot, while in the other three plots, the algorithm finds a neighborhood of a minimizer, but then oscillates back and forth and never appears to settle down. This is due jointly to the elliptical (non-circular) shape of the contours and poorly chosen learning rates.</p>
<p>Let’s carry the runs out further, to <span class="math notranslate nohighlight">\(N=125\)</span> gradient steps, and plot the objective values versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">])]}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">125</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                   <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                   <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                   <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="o">=</span><span class="n">gd_output</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">per_step_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;first runs of gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/31d409939738cd1c4b76ce0a5a83cda8d1e738d7b838cb5ff393308e4fcec4f5.svg" src="../_images/31d409939738cd1c4b76ce0a5a83cda8d1e738d7b838cb5ff393308e4fcec4f5.svg" /></figure>
</div>
</div>
<p>The oscillatory nature of the runs is even more clear. In particular, we now see that even the first run, which <em>appeared</em> to be converging, begins to oscillate after about 40 gradient steps. Let’s take a closer look at the trace of this first run, and also plot the components <span class="math notranslate nohighlight">\(\theta_1\)</span> and <span class="math notranslate nohighlight">\(\theta_2\)</span> versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">125</span>
<span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.15</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/b8479d356fa9c1b71a2ebedf52364b5d99515efe70e7e6c755cb2b1df2d768b9.svg" src="../_images/b8479d356fa9c1b71a2ebedf52364b5d99515efe70e7e6c755cb2b1df2d768b9.svg" /></figure>
</div>
</div>
<p>Both plots make clear that the oscillations occur mostly along the semi-minor axes of the elliptical contours, which coincides with the direction of largest curvature. From the previous section, we know that local curvatures are encoded in the Hessian matrix. This suggests that studying the Hessian matrix might lead to insights into the convergence properties of gradient descent. But first, to prepare for this general study, let’s do:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 11 on the worksheet.</p>
</div>
<p>To begin the theoretical study of convergence of gradient descent, let’s start more generally with any function <span class="math notranslate nohighlight">\(J:\bbr^2 \to \bbr\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star\)</span> a point. We then take a degree-<span class="math notranslate nohighlight">\(2\)</span> Taylor polynomial approximation centered at <span class="math notranslate nohighlight">\(\btheta^\star\)</span>:</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \approx J(\btheta^\star) + (\btheta - \btheta^\star)^\intercal \nabla J(\btheta^\star) + \frac{1}{2} (\btheta - \btheta^\star)^\intercal \nabla^2 J(\btheta^\star)  (\btheta - \btheta^\star).
\]</div>
<p>If we believe that the Taylor polynomial accurately reflects the local geometry of the graph of <span class="math notranslate nohighlight">\(J\)</span> near <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to within whatever degree of approximation we require, then we may as well replace <span class="math notranslate nohighlight">\(J\)</span> with its Taylor polynomial, and thereby assume that <span class="math notranslate nohighlight">\(J\)</span> is a degree-<span class="math notranslate nohighlight">\(2\)</span> (inhomogeneous) polynomial:</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{2}\btheta^\intercal \bH \btheta + \bb^\intercal \btheta + c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bH \in \bbr^{n\times n}\)</span> is a symmetric matrix, <span class="math notranslate nohighlight">\(\bb\in \bbr^n\)</span> is a vector, and <span class="math notranslate nohighlight">\(c\in \bbr\)</span> is a scalar. As you may easily compute (see the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/homework/11-homework.md#problem-2-derivatives-of-quadratic-functions">homework</a>), the gradient vector and Hessian matrix are given by</p>
<div class="math notranslate nohighlight">
\[
\nabla J(\btheta) = \bH \btheta + \bb \quad \text{and} \quad  \nabla^2 J(\btheta) = \bH.
\]</div>
<p>Assuming that the decay rate is <span class="math notranslate nohighlight">\(\beta=0\)</span> while the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is arbitrary, the update rule in the algorithm is given by</p>
<div class="math notranslate nohighlight">
\[
\btheta_{t+1} = \btheta_t - \alpha(\bH\btheta_t + \bb),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 0\)</span>. If <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a stationary point (like a local minimizer), we may rewrite the update rule as</p>
<div class="math notranslate nohighlight">
\[
\btheta_{t+1} - \btheta^\star = (\bI - \alpha \bH)(\btheta_t - \btheta^\star),
\]</div>
<p>which leads us to the closed form</p>
<div class="math notranslate nohighlight" id="equation-gd-closed-eqn">
<span class="eqno">(11.11)<a class="headerlink" href="#equation-gd-closed-eqn" title="Link to this equation">#</a></span>\[
\btheta_t - \btheta^\star = (\bI - \alpha \bH)^t (\btheta_0 - \btheta^\star),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 1\)</span>.</p>
<p>Now, let’s suppose that <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer of <span class="math notranslate nohighlight">\(J\)</span> with positive definite Hessian matrix <span class="math notranslate nohighlight">\(\bH\)</span>. We are in <span class="math notranslate nohighlight">\(2\)</span>-dimensions, so <span class="math notranslate nohighlight">\(\bH\)</span> has only two eigenvalues:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \lambda_1 \leq \lambda_2.
\]</div>
<p>Then the eigenvalues of the matrix <span class="math notranslate nohighlight">\(\bI - \alpha \bH\)</span> are <span class="math notranslate nohighlight">\(1 - \alpha \lambda_1\)</span> and <span class="math notranslate nohighlight">\(1-\alpha \lambda_2\)</span>, and they may be linearly ordered as</p>
<div class="math notranslate nohighlight" id="equation-both-evecs-eq">
<span class="eqno">(11.12)<a class="headerlink" href="#equation-both-evecs-eq" title="Link to this equation">#</a></span>\[
1 - \alpha \lambda_2 \leq 1 - \alpha \lambda_1.
\]</div>
<p>The eigenvectors <span class="math notranslate nohighlight">\(\be_1,\be_2\)</span> of <span class="math notranslate nohighlight">\(\bI - \alpha\bH\)</span> are the same as those of <span class="math notranslate nohighlight">\(\bH\)</span> (which may be chosen to form an orthonormal basis of <span class="math notranslate nohighlight">\(\bbr^2\)</span>), so if we have</p>
<div class="math notranslate nohighlight">
\[
\btheta_0 - \btheta^\star = \gamma_1 \be_1 + \gamma_2 \be_2
\]</div>
<p>for some scalars <span class="math notranslate nohighlight">\(\gamma_1,\gamma_2\in \bbr\)</span>, then from <a class="reference internal" href="#equation-gd-closed-eqn">(11.11)</a> we get</p>
<div class="math notranslate nohighlight" id="equation-evec-gd-eq">
<span class="eqno">(11.13)<a class="headerlink" href="#equation-evec-gd-eq" title="Link to this equation">#</a></span>\[
\btheta_t - \btheta^\star = \gamma_1 (1-\alpha \lambda_1)^t \be_1 + \gamma_2 (1-\alpha \lambda_2)^t \be_2
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 1\)</span>.</p>
<p>For the particular run of the algorithm shown in the last plot, we have</p>
<div class="math notranslate nohighlight">
\[
\be_1^\intercal = (1,0), \quad \be_2^\intercal = (0,1), \quad \alpha = 0.01, \quad \lambda_1 = 22, \quad \lambda_2 = 220,
\]</div>
<p>so that <a class="reference internal" href="#equation-evec-gd-eq">(11.13)</a> is</p>
<div class="math notranslate nohighlight">
\[
\btheta_t - \btheta^\star = \gamma_1 (0.78)^t \be_1 + \gamma_2 (-1.2)^t \be_2.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(|0.78|&lt; 1\)</span> while <span class="math notranslate nohighlight">\(|-1.2| &gt;1\)</span>, this last equation shows why the algorithm diverges along the direction <span class="math notranslate nohighlight">\(\be_2\)</span> of maximum curvature, while converging along the direction <span class="math notranslate nohighlight">\(\be_1\)</span> of minimum curvature. The trick, then, is to choose the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> so that both eigenvalues in <a class="reference internal" href="#equation-both-evecs-eq">(11.12)</a> have magnitude less than <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>To do this, we apply the triangle inequality to the right-hand side of <a class="reference internal" href="#equation-evec-gd-eq">(11.13)</a> to obtain the upper bound</p>
<div class="math notranslate nohighlight" id="equation-op-norm-eq">
<span class="eqno">(11.14)<a class="headerlink" href="#equation-op-norm-eq" title="Link to this equation">#</a></span>\[
|\btheta_t - \btheta^\star| \leq \rho\left(\bI - \alpha \bH\right)^t |\btheta_0 - \btheta^\star |,
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 1\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
\rho\left(\bI - \alpha \bH\right) = \max \{ |1-\alpha \lambda_1|, |1-\alpha \lambda_2|\}
\]</div>
<p>is the spectral radius of the matrix <span class="math notranslate nohighlight">\(\bI - \alpha \bH\)</span>. To guarantee fastest convergence, we want to choose the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> that minimizes the spectral radius. But an easy computation shows that this optimal learning rate is given by</p>
<div class="math notranslate nohighlight">
\[
\alpha = \frac{2}{\lambda_1 + \lambda_2},
\]</div>
<p>in which case we get</p>
<div class="math notranslate nohighlight">
\[
\rho\left( \bI - \alpha \bH \right) = |1-\alpha \lambda_1| = |1-\alpha \lambda_2|.
\]</div>
<p>Then, with this choice of <span class="math notranslate nohighlight">\(\alpha\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-conv-bound-eq">
<span class="eqno">(11.15)<a class="headerlink" href="#equation-conv-bound-eq" title="Link to this equation">#</a></span>\[
|\btheta_t - \btheta^\star| \leq \rho\left(1 - \alpha \lambda_1\right)^t |\btheta_0 - \btheta^\star | = \left( 1 - \frac{2}{1 + \kappa(\bH)} \right)^t |\btheta_0 - \btheta^\star |,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa(\bH)\)</span> is the condition number of the Hessian matrix <span class="math notranslate nohighlight">\(\bH\)</span>.</p>
<p>This shows that the fastest rates of convergence guaranteed by our arguments are obtained for those objective functions that have Hessian matrices at local minimizers with condition numbers near <span class="math notranslate nohighlight">\(1\)</span>. In the extreme case that <span class="math notranslate nohighlight">\(\kappa(\bH)=1\)</span>, notice that the bound shows <span class="math notranslate nohighlight">\(\btheta_1 = \btheta^\star\)</span>, i.e., the algorithm lands on the local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span> in just one gradient step. The inequality <a class="reference internal" href="#equation-conv-bound-eq">(11.15)</a> is tight, by which we mean that there are worst-case initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span> for which equality holds. This is the case, for example, if the difference <span class="math notranslate nohighlight">\(\btheta_0 - \btheta^\star\)</span> is equal to the eigenvector corresponding to <span class="math notranslate nohighlight">\(\lambda_1\)</span>.</p>
<p>This argument easily adapts to the case that the objective function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> is defined on a Euclidean space of arbitrary dimension. We present the general result in a theorem:</p>
<div class="proof theorem admonition" id="quadratic-conv-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.6 </span> (Curvature and gradient descent)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star\)</span> a local minimizer with positive definite Hessian matrix <span class="math notranslate nohighlight">\(\bH = \nabla^2 J(\btheta^\star)\)</span> with minimum and maximum eigenvalues <span class="math notranslate nohighlight">\(\lambda_\text{min}\)</span> and <span class="math notranslate nohighlight">\(\lambda_\text{max}\)</span>. For initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span> sufficiently near <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to allow a degree-<span class="math notranslate nohighlight">\(2\)</span> Taylor polynomial approximation, the gradient descent algorithm with learning rate</p>
<div class="math notranslate nohighlight">
\[
\alpha = \frac{2}{\lambda_\text{min} + \lambda_\text{max}} \quad \text{and} \quad \beta=0
\]</div>
<p>converges to <span class="math notranslate nohighlight">\(\btheta^\star\)</span> exponentially fast, with</p>
<div class="math notranslate nohighlight">
\[
|\btheta_t - \btheta^\star| \leq \left( 1 - \frac{2}{1 + \kappa(\bH)} \right)^t |\btheta_0 - \btheta^\star |
\]</div>
<p>for each <span class="math notranslate nohighlight">\(t\geq 1\)</span>. Here, <span class="math notranslate nohighlight">\(\kappa(\bH)\)</span> is the condition number of <span class="math notranslate nohighlight">\(\bH\)</span>.</p>
</section>
</div><p>The ratio <span class="math notranslate nohighlight">\(2/(1 + \kappa(\bH))\)</span> is sometimes called the <em>rate of convergence</em>. In the special case that <span class="math notranslate nohighlight">\(n=2\)</span>, so that the graph of our function <span class="math notranslate nohighlight">\(J:\bbr^2 \to \bbr\)</span> is a surface in <span class="math notranslate nohighlight">\(\bbr^3\)</span>, the value <span class="math notranslate nohighlight">\((\lambda_\text{min} + \lambda_\text{max})/2\)</span> is significant in differential geometry, called the <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_curvature"><em>mean curvature</em></a> of the surface. Thus, in this case and using this language, the optimal value for the learning rate is given by the reciprocal mean curvature of the graph at the local minimum. Larger mean curvatures require smaller learning rates, while smaller mean curvatures allow larger learning rates.</p>
<p>Of course, in order to obtain the exponentially quick convergence guaranteed by the theorem, one needs to place their initial guess <span class="math notranslate nohighlight">\(\btheta_0\)</span> “sufficiently close” to the minimizer. But this would require the analyst to already have some sense of where the minimizer is likely to be located! This restricts its usefulness in practice.</p>
<p>For our polynomial objective <span class="math notranslate nohighlight">\(J\)</span> given in <a class="reference internal" href="#equation-two-dim-poly-eq">(11.10)</a> above, we compute the spectra of the Hessian matrices at the minimizers <span class="math notranslate nohighlight">\((0,0)\)</span> and <span class="math notranslate nohighlight">\((1,1)\)</span> to be <span class="math notranslate nohighlight">\(\{22,220\}\)</span> in both cases. Thus, if we choose learning rate <span class="math notranslate nohighlight">\(\alpha = 2/(22+220) \approx 0.008\)</span> and re-run the gradient descent algorithm with the same initial guesses to test <a class="reference internal" href="#quadratic-conv-thm">Theorem 11.6</a>, we get the new plots:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">])]}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">8e-3</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                   <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                   <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                   <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;second runs of gradient descent with smaller learning rates&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c2652cd0b1f4867c77ee2f6150e022242054ef86a8abb49952f5ab71c5aa1c22.svg" src="../_images/c2652cd0b1f4867c77ee2f6150e022242054ef86a8abb49952f5ab71c5aa1c22.svg" /></figure>
</div>
</div>
<p>Just like magic, the undesirable oscillations have vanished. We take a closer look at the third run of the algorithm, in the bottom left of the figure:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">8e-3</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">125</span>
<span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/bc3c2ad54236d9e25dff02c4c3b9bc458ab4fb4903950a6fa72033ca59203b6d.svg" src="../_images/bc3c2ad54236d9e25dff02c4c3b9bc458ab4fb4903950a6fa72033ca59203b6d.svg" /></figure>
</div>
</div>
<p>We may also dampen the oscillations and keep the original (relatively large) learning rates by adding a slight learning rate decay at <span class="math notranslate nohighlight">\(\beta = 0.05\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">])]}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                   <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                   <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                   <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>

    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;third runs of gradient descent with learning rate decay&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/fd41aa5da9e3dbbfd6ac900662505165c24a7e71d4fdad0dd73ba0814f31ce3c.svg" src="../_images/fd41aa5da9e3dbbfd6ac900662505165c24a7e71d4fdad0dd73ba0814f31ce3c.svg" /></figure>
</div>
</div>
<p>Here are the values of the objective function for these last runs with learning rate decay, plotted against the number of gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_parameters&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span>
                                     <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">])]}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                   <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                   <span class="n">num_steps</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                   <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                   <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="o">=</span><span class="n">gd_output</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">per_step_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;third runs of gradient descent with learning rate decay&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f3f94de8a81f75d4a18b5b5ceb1d7c5490d11d045d05541fd7e9acc2b082a9cd.svg" src="../_images/f3f94de8a81f75d4a18b5b5ceb1d7c5490d11d045d05541fd7e9acc2b082a9cd.svg" /></figure>
</div>
</div>
<p>Notice the initial “overshoot” in the plot in the bottom left, causing the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> to <em>increase</em> after the first gradient step. Recall also that the initial value <span class="math notranslate nohighlight">\(\btheta_0\)</span> in the bottom right plot is near the saddle point <span class="math notranslate nohighlight">\((0.5,0.5)\)</span>, causing <span class="math notranslate nohighlight">\(\nabla J(\btheta_0) \approx 0\)</span>. This accounts for the small initial changes in the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> indicated by the (nearly) horizontal stretch early in the run of the algorithm.</p>
<p>Of course, an objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}^2 \to \mathbb{R}\)</span> defined on a <span class="math notranslate nohighlight">\(2\)</span>-dimensional input space is still not a realistic example of the objective functions encountered in the real world. In two dimensions, we have the ability to trace the algorithm’s progress through <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> on contour plots, as we did multiple times above. In higher dimensions we lose this valuable visual aid. But no matter the input dimension, we may always plot the objective values against the number of gradient steps as a diagnostic plot for convergence.</p>
</section>
<section id="stochastic-gradient-descent">
<span id="sgd-sec"></span><h2><span class="section-number">11.4. </span>Stochastic gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Link to this heading">#</a></h2>
<p>Many of the objective functions that we will see in <a class="reference internal" href="13-learning.html#learning"><span class="std std-numref">Chapter 13</span></a> have a special form making them amenable to optimization via a variation of the gradient descent algorithm called <em>stochastic gradient descent</em>. These special objective functions are called <em>stochastic objective functions</em>, which look like</p>
<div class="math notranslate nohighlight" id="equation-stoch-obj-eqn">
<span class="eqno">(11.16)<a class="headerlink" href="#equation-stoch-obj-eqn" title="Link to this equation">#</a></span>\[J(\btheta) = E_{\bx \sim p(\bx)}\big[ L(\btheta, \bx) \big] = \sum_{\mathbf{x}\in \mathbb{R}^n} L(\btheta,\bx)p(\mathbf{x}),\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\bx)\)</span> is a probability mass function, <span class="math notranslate nohighlight">\(\btheta \in \mathbb{R}^k\)</span> is a <em>parameter vector</em>, and <span class="math notranslate nohighlight">\(L:\mathbb{R}^{n+k} \to \mathbb{R}\)</span> is a differentiable function called the <em>loss function</em>. Very often, the mass function will be an empirical mass function of an observed multivariate dataset</p>
<div class="math notranslate nohighlight">
\[
\bx_1,\bx_2,\ldots,\bx_m \in \mathbb{R}^n,
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight" id="equation-obj-approx-eq">
<span class="eqno">(11.17)<a class="headerlink" href="#equation-obj-approx-eq" title="Link to this equation">#</a></span>\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m L\big(\btheta, \bx_i\big).
\]</div>
<p>By linearity of the gradient operation, we have</p>
<div class="math notranslate nohighlight" id="equation-batch-eqn">
<span class="eqno">(11.18)<a class="headerlink" href="#equation-batch-eqn" title="Link to this equation">#</a></span>\[
\nabla_\btheta J(\btheta) = \frac{1}{m} \sum_{i=1}^m \nabla_\btheta L\big(\btheta, \bx_i\big)
\]</div>
<p>where we write <span class="math notranslate nohighlight">\(\nabla_\btheta\)</span> instead of just <span class="math notranslate nohighlight">\(\nabla\)</span> to emphasize that the gradient of the loss function <span class="math notranslate nohighlight">\(L\)</span> is computed with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. In this context, the gradient descent algorithm applied to the objective function <a class="reference internal" href="#equation-obj-approx-eq">(11.17)</a> is given a new name:</p>
<div class="proof definition admonition" id="batch-gd-def">
<p class="admonition-title"><span class="caption-number">Definition 11.5 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>batch gradient descent algorithm</em> is the gradient descent algorithm applied to a stochastic objective function of the form <a class="reference internal" href="#equation-obj-approx-eq">(11.17)</a>.</p>
</section>
</div><p>Let’s take a look at a simple example. Suppose that we define the loss function</p>
<div class="math notranslate nohighlight" id="equation-quadratic-eqn">
<span class="eqno">(11.19)<a class="headerlink" href="#equation-quadratic-eqn" title="Link to this equation">#</a></span>\[L: \bbr^4 \to \bbr, \quad L(\btheta, \bx) = \frac{1}{2}|\bx - \btheta|^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\bx,\btheta\in \bbr^2\)</span>. We create a bivariate dataset by drawing a random sample of size <span class="math notranslate nohighlight">\(1{,}024\)</span> from a <span class="math notranslate nohighlight">\(\mathcal{N}_2(\boldsymbol0,I)\)</span> distribution. A scatter plot of the dataset looks like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/9aa271abd8c72899f9feff9481325ef88b17d673a42ee83825bb126279c42eb4.svg" src="../_images/9aa271abd8c72899f9feff9481325ef88b17d673a42ee83825bb126279c42eb4.svg" /></figure>
</div>
</div>
<p>Before running the algorithm, let’s do:</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 12 on the worksheet.</p>
</div>
<p>Now, two runs of the batch gradient descent algorithm produce the following plots of the objective function versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the loss function</span>
<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">theta</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="c1"># define the objective function</span>
<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">L</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">]}</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">)</span>
    <span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="o">=</span><span class="n">gd_output</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">per_step_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;batch gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/71045bbf16275719a4e0c17ad55be5ff846d21223d5f1bfbde4d0de76e0a7d40.svg" src="../_images/71045bbf16275719a4e0c17ad55be5ff846d21223d5f1bfbde4d0de76e0a7d40.svg" /></figure>
</div>
</div>
<p>If we track the parameters <span class="math notranslate nohighlight">\(\btheta = (\theta_1,\theta_2)\)</span> during the runs, we get the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mf">0.05</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">)</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, gradient steps$=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;batch gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/fe097308871572056c3146a70628b04127f47c84a43e31c63897ecbf9faae2d9.svg" src="../_images/fe097308871572056c3146a70628b04127f47c84a43e31c63897ecbf9faae2d9.svg" /></figure>
</div>
</div>
<p>In both cases, notice that the algorithm is nicely converging toward the minimizer at <span class="math notranslate nohighlight">\(\btheta^\star \approx (0,0)\)</span>.</p>
<p>One of the drawbacks of the batch algorithm is that it needs the <em>entire</em> dataset in order to take just a single gradient step. This isn’t an issue for our small toy dataset of size <span class="math notranslate nohighlight">\(m=1{,}024\)</span>, but for the large datasets that you may encounter in the real world, this can be a serious hindrance to fast convergence.</p>
<p>One method for dealing with this bottleneck is to use <em>mini-batches</em> of the data to compute gradient steps. To do so, we begin by randomly partitioning the dataset into subsets <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_p\)</span> called <em>mini-batches</em>:</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-eqn">
<span class="eqno">(11.20)<a class="headerlink" href="#equation-mini-batch-eqn" title="Link to this equation">#</a></span>\[B_1 \cup B_2 \cup \cdots \cup B_p = \{\bx_1,\bx_2,\ldots,\bx_m\}.\]</div>
<p>We would then expect from <a class="reference internal" href="#equation-obj-approx-eq">(11.17)</a> and <a class="reference internal" href="#equation-batch-eqn">(11.18)</a> that</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \approx \frac{1}{|B_j|} \sum_{\bx \in B_j} L\big(\btheta, \bx\big)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-grad-eqn">
<span class="eqno">(11.21)<a class="headerlink" href="#equation-mini-batch-grad-eqn" title="Link to this equation">#</a></span>\[
\nabla J(\btheta) \approx \frac{1}{|B_j|} \sum_{\bx \in B_j} \nabla_\btheta L\big(\btheta, \bx\big),
\]</div>
<p>for each <span class="math notranslate nohighlight">\(j=1,2,\ldots,p\)</span>, where <span class="math notranslate nohighlight">\(|B_j|\)</span> is the cardinality (or size) of the <span class="math notranslate nohighlight">\(j\)</span>-th mini-batch. Very often, the mini-batch sizes are chosen to be equal to a common value <span class="math notranslate nohighlight">\(k\)</span>, except (possibly) for one to compensate for the fact that <span class="math notranslate nohighlight">\(m\)</span> may not be evenly divisible by <span class="math notranslate nohighlight">\(k\)</span>. For example, if <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(k=30\)</span>, then we would have four mini-batches, three of size <span class="math notranslate nohighlight">\(30\)</span> and the fourth of size <span class="math notranslate nohighlight">\(10\)</span>.</p>
<p>The mini-batch version of the gradient descent algorithm loops over the mini-batches <a class="reference internal" href="#equation-mini-batch-eqn">(11.20)</a> and computes gradient steps using the approximation in <a class="reference internal" href="#equation-mini-batch-grad-eqn">(11.21)</a>. A single loop through <em>all</em> the mini-batches, covering the <em>entire</em> dataset, is called an <em>epoch</em>. This new version of the algorithm takes the number of epochs <span class="math notranslate nohighlight">\(N\)</span> as a parameter.</p>
<div class="proof algorithm admonition" id="sgd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.4 </span> (Stochastic gradient descent with learning rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A dataset <span class="math notranslate nohighlight">\(\bx_1,\bx_2\ldots,\bx_m\in \mathbb{R}^n\)</span>, a differentiable loss function <span class="math notranslate nohighlight">\(L:\bbr^{n+k} \to \bbr\)</span>, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^k\)</span> for a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span> of the stochastic objective function <a class="reference internal" href="#equation-batch-eqn">(11.18)</a>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\beta \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of epochs.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<hr class="docutils" />
<p>   <span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span> <br>
   <span class="math notranslate nohighlight">\(s := 0\)</span> <br>
   For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do: <br>
           Randomly partition the dataset into mini-batches <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_p\)</span> <br>
           For each mini-batch <span class="math notranslate nohighlight">\(B_j\)</span>, do: <br>
                   <span class="math notranslate nohighlight">\(\btheta := \btheta - \displaystyle \alpha(1-\beta)^{s+1} \frac{1}{|B_j|} \sum_{\bx \in B_j} \nabla_\btheta L\big(\btheta, \bx\big)\)</span> <br>
                   <span class="math notranslate nohighlight">\(s := s+1\)</span> <br>
   Return <span class="math notranslate nohighlight">\(\btheta\)</span></p>
</section>
</div><p>Notice the auxiliary variable <span class="math notranslate nohighlight">\(s\)</span>, which counts the number of gradient steps and is used to decay the learning rate. As we mentioned above, very often the mini-batches are chosen to be of equal size <span class="math notranslate nohighlight">\(k\)</span>, except (possibly) for one mini-batch to compensate for the fact that <span class="math notranslate nohighlight">\(m\)</span> may not be divisible by <span class="math notranslate nohighlight">\(k\)</span>. In this case, we pass in the mini-batch size <span class="math notranslate nohighlight">\(k\)</span> as an additional parameter.</p>
<p>As in the versions of gradient descent explored above, in practice is convenient to code the algorithm so that it returns the entire sequence of <span class="math notranslate nohighlight">\(\btheta\)</span>’s produced to help monitor convergence. If there are <span class="math notranslate nohighlight">\(p\)</span> mini-batches and <span class="math notranslate nohighlight">\(N\)</span> epochs, then the output of the algorithm may be partitioned like this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c|c}
\text{epoch number} &amp; \text{output} \\ \hline
0 &amp; \btheta_0 \\
1 &amp; \btheta_1, \btheta_2,\ldots, \btheta_p \\
2 &amp; \btheta_{p+1}, \btheta_{p+2},\ldots, \btheta_{2p} \\
\vdots &amp; \vdots \\
N &amp; \btheta_{(N-1)p+1}, \btheta_{(N-1)p+2},\ldots, \btheta_{Np}
\end{array}
\end{split}\]</div>
<p>This leads to the following sequences of objective values:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c|c}
\text{epoch number} &amp; \text{objectives} \\ \hline
0 &amp; J(\btheta_0) \\
1 &amp; J(\btheta_1), J(\btheta_2),\ldots, J(\btheta_p) \\
2 &amp; J(\btheta_{p+1}), J(\btheta_{p+2}),\ldots, J(\btheta_{2p}) \\
\vdots &amp; \vdots \\
N &amp; J(\btheta_{(N-1)p+1}), J(\btheta_{(N-1)p+2}),\ldots, J(\btheta_{Np})
\end{array}
\end{split}\]</div>
<p>However, rather than have the algorithm output the <em>exact</em> objective values <span class="math notranslate nohighlight">\(J(\btheta)\)</span>, we will have it output the <em>approximate</em> objective values obtained as realizations of the right-hand side of</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \approx \frac{1}{|B|} \sum_{\bx \in B} L(\btheta, \bx),
\]</div>
<p>where <span class="math notranslate nohighlight">\(B\)</span> is a mini-batch of data. In order to monitor convergence, we may plot these per-step (approximate) objective values versus gradient steps, as we have done in the previous two sections. But, depending on the mini-batch size, these plots may be quite noisy, so sometimes to get a better sense of the trend it is convenient to also track the <em>mean</em> (approximate) objective values per epoch. We will see examples below.</p>
<p>But first, we note that it is possible to select a mini-batch size of <span class="math notranslate nohighlight">\(k=1\)</span>, so that the algorithm computes a gradient step per data point. (Some references refer to <em>this</em> algorithm as <em>stochastic gradient descent</em>.) In our example <a class="reference internal" href="#equation-quadratic-eqn">(11.19)</a> from above, a batch size of <span class="math notranslate nohighlight">\(k=1\)</span> yields the following plots of objective values versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">],</span>
                 <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">160</span><span class="p">]}</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span>
                    <span class="n">L</span><span class="o">=</span><span class="n">L</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                    <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="p">,</span> <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">show_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">per_step_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;stochastic gradient descent with $k=1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f0d5181f968bda6e0117c027f885346472d6cb5763960e4a35a3af2d2ea8b738.svg" src="../_images/f0d5181f968bda6e0117c027f885346472d6cb5763960e4a35a3af2d2ea8b738.svg" /></figure>
</div>
</div>
<p>Note that we have halted the runs early, before the algorithm has had a chance to make it through even one epoch. The plots are very noisy, though a slight downward trend in objective values is detectable. The trace of the algorithm through parameter space is shown in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span>
                    <span class="n">L</span><span class="o">=</span><span class="n">L</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                    <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$,</span><span class="se">\n</span><span class="s1"> gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;stochastic gradient descent with $k=1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/e0f28754b2b31819411b07cf5d7819689bf061e10c47eb03c6610c374da03a4b.svg" src="../_images/e0f28754b2b31819411b07cf5d7819689bf061e10c47eb03c6610c374da03a4b.svg" /></figure>
</div>
</div>
<p>The traces are also very noisy, especially in the first row with the large learning rate <span class="math notranslate nohighlight">\(\alpha=0.1\)</span>. Nevertheless, it is clear that the algorithm has found the neighborhood of the minimizer at <span class="math notranslate nohighlight">\((0,0)\)</span>. We might try to tame the noise in these plots by increasing the decay rate, but it turns out that increasing the batch size is the better way to do this. So, let’s try four larger batch sizes:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                 <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
                 <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">]}</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span>
                    <span class="n">L</span><span class="o">=</span><span class="n">L</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                    <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span>
                    <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">k</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">]</span>
    
    <span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="p">,</span> <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">show_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">per_step_alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">,</span> <span class="n">per_step_label</span><span class="o">=</span><span class="s1">&#39;objective per step&#39;</span><span class="p">)</span>
    <span class="c1">#axis.plot(range(len(gd_output.per_step_objectives)), gd_output.per_step_objectives)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">epoch_step_nums</span><span class="p">,</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">[</span><span class="n">gd_output</span><span class="o">.</span><span class="n">epoch_step_nums</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;mini-batch gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/5bcb9113bce73e82469ca96bf9e19efa8555aa00fd274630d8db28be6e51dd76.svg" src="../_images/5bcb9113bce73e82469ca96bf9e19efa8555aa00fd274630d8db28be6e51dd76.svg" /></figure>
</div>
</div>
<p>The runs with batch sizes <span class="math notranslate nohighlight">\(k=2, 8, 32\)</span> halt before completing one epoch, while the run with batch size <span class="math notranslate nohighlight">\(k=128\)</span> completes three full epochs. Notice that the noise in the plots progressively decreases as the batch size increases. The traces through parameter space look like:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span>
                    <span class="n">L</span><span class="o">=</span><span class="n">L</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                    <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                    <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">k</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">]</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">$,</span><span class="se">\n</span><span class="s1"> gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;mini-batch gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/8456efd6845372f6f6fc6060f599bd0c299d5489a1166d0d6e026a4a65fe7b82.svg" src="../_images/8456efd6845372f6f6fc6060f599bd0c299d5489a1166d0d6e026a4a65fe7b82.svg" /></figure>
</div>
</div>
<p>As we mentioned above, it may be helpful to track the mean objective values per epoch to help see the general trend of the objective through the noise. To illustrate this point, let’s suppose that we continue our runs of mini-batch gradient descent so that they all complete six epochs. If we plot both the objective per gradient step and the mean objective per epoch, we get the following plots:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">]}</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span>
                    <span class="n">L</span><span class="o">=</span><span class="n">L</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                    <span class="n">init_parameters</span><span class="o">=</span><span class="n">theta0</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
                    <span class="n">decay_rate</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">plot_gd</span><span class="p">(</span><span class="n">gd_output</span><span class="p">,</span>
             <span class="n">plot_title</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">ax</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
             <span class="n">per_step_alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
             <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;objective&#39;</span><span class="p">,</span>
             <span class="n">per_epoch_color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span>
             <span class="n">per_step_label</span><span class="o">=</span><span class="s1">&#39;objective per step&#39;</span><span class="p">,</span>
             <span class="n">per_epoch_label</span><span class="o">=</span><span class="s1">&#39;mean objective per epoch&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;mini-batch gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/4d4fa74180490c99b695f7f89f0589e95774e009de2b98323ef3e3b038cb4e11.svg" src="../_images/4d4fa74180490c99b695f7f89f0589e95774e009de2b98323ef3e3b038cb4e11.svg" /></figure>
</div>
</div>
</section>
<section id="appendix-convex-functions">
<span id="app-conv-sec"></span><h2><span class="section-number">11.5. </span>Appendix: convex functions<a class="headerlink" href="#appendix-convex-functions" title="Link to this heading">#</a></h2>
<p>We begin with the definition of a convex function. Notice that it does not require that the function have any special properties, like differentiability.</p>
<div class="proof definition admonition" id="convex-concave-def">
<p class="admonition-title"><span class="caption-number">Definition 11.6 </span></p>
<section class="definition-content" id="proof-content">
<p>We shall say a function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> is <em>convex</em> provided that</p>
<div class="math notranslate nohighlight">
\[
J\big((1-t) \ba + t\bb\big) \leq (1-t) J(\ba) + t J(\bb)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\ba,\bb\in \bbr^n\)</span> and all <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>. If “<span class="math notranslate nohighlight">\(\leq\)</span>” is replaced with “<span class="math notranslate nohighlight">\(&lt;\)</span>”, then the function is called <em>strictly convex</em>; if the inequalities are reversed, we obtain the definitions of <em>concave</em> and <em>strictly concave</em>.</p>
</section>
</div><p>Essentially, the definition says that a function is convex provided that its graph “lies below its secant lines.” If the function is twice-differentiable, then there equivalent characterizations of convexity in terms of calculus:</p>
<div class="proof theorem admonition" id="main-convex-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.7 </span> (Main theorem on convex functions (single-variable version))</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J: \bbr \to \bbr\)</span> be a twice-differentiable function. The following statements are equivalent:</p>
<ol class="arabic">
<li><p>The function <span class="math notranslate nohighlight">\(J\)</span> is convex.</p></li>
<li><p>For all numbers <span class="math notranslate nohighlight">\(a,b,c\)</span> with <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    \frac{J(b)-J(a)}{b-a} \leq \frac{J(c)-J(b)}{c-b}.
    \]</div>
</li>
<li><p>The graph of <span class="math notranslate nohighlight">\(J\)</span> lies above its tangent lines, i.e., for all <span class="math notranslate nohighlight">\(\theta,a\in \bbr\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    J(\theta) \geq J'(a)(\theta-a) + J(a).
    \]</div>
</li>
<li><p>The second derivative is nonnegative everywhere, i.e., <span class="math notranslate nohighlight">\(J''(\theta)\geq 0\)</span> for all <span class="math notranslate nohighlight">\(\theta\in \bbr\)</span>.</p></li>
</ol>
<p>Moreover, if <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a stationary point of <span class="math notranslate nohighlight">\(J\)</span> and <span class="math notranslate nohighlight">\(J\)</span> is convex, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a global minimizer.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We shall prove the statements as (1) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (1), and then (3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (4) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2).</p>
<p>(1) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (3): For each <span class="math notranslate nohighlight">\(\theta,a\in \bbr\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-der-almost-eq">
<span class="eqno">(11.22)<a class="headerlink" href="#equation-der-almost-eq" title="Link to this equation">#</a></span>\[
J'(a)(\theta-a) = \lim_{t\to 0} \frac{J\big( a+t(\theta-a) \big) - J(a)}{t}.
\]</div>
<p>Provided <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
J\big( a+t(\theta-a) \big) = J \big( (1-t)a + t\theta\big) \leq (1-t)J(a) + tJ(\theta)
\]</div>
<p>and so</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \geq \frac{J\big( a+t(\theta-a) \big) - J(a)}{t} + J(a).
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(t\to 0^+\)</span> and using <a class="reference internal" href="#equation-der-almost-eq">(11.22)</a> yields</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \geq J'(a) (\theta-a) + J(a),
\]</div>
<p>as desired.</p>
<p>(3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2): Letting <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>, by hypothesis we have</p>
<div class="math notranslate nohighlight">
\[
J(a) \geq J'(b)(a-b) + J(b) \quad \text{and} \quad J(c) \geq J'(b)(c-b) + J(b).
\]</div>
<p>Then the inequalities</p>
<div class="math notranslate nohighlight">
\[
\frac{J(b)-J(a)}{b-a} \leq J'(b) \leq \frac{J(c)-J(b)}{c-b}
\]</div>
<p>follow immediately, establishing (2).</p>
<p>(2) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (1): Suppose that the inequality</p>
<div class="math notranslate nohighlight" id="equation-inc-sec-eq">
<span class="eqno">(11.23)<a class="headerlink" href="#equation-inc-sec-eq" title="Link to this equation">#</a></span>\[
\frac{J(b)-J(a)}{b-a} \leq \frac{J(c)-J(b)}{c-b}
\]</div>
<p>holds for all <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>. Fixing <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(c\)</span> with <span class="math notranslate nohighlight">\(a&lt;c\)</span>, let <span class="math notranslate nohighlight">\(t\in (0,1)\)</span> and set</p>
<div class="math notranslate nohighlight">
\[
b = (1-t)a + tc.
\]</div>
<p>Then we have both</p>
<div class="math notranslate nohighlight" id="equation-ratio-t-eq">
<span class="eqno">(11.24)<a class="headerlink" href="#equation-ratio-t-eq" title="Link to this equation">#</a></span>\[
t = \frac{b-a}{c-a},
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-second-choice-eq">
<span class="eqno">(11.25)<a class="headerlink" href="#equation-second-choice-eq" title="Link to this equation">#</a></span>\[
1-t = \frac{c-b}{c-a}
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\frac{J(b)-J(a)}{t} \leq \frac{J(c)-J(b)}{1-t}
\]</div>
<p>by <a class="reference internal" href="#equation-second-choice-eq">(11.25)</a> and <a class="reference internal" href="#equation-inc-sec-eq">(11.23)</a>. We may rearrange this inequality to obtain</p>
<div class="math notranslate nohighlight">
\[
J\big((1-t)a+tc \big) =J(b) \leq (1-t)J(a) + tJ(c),
\]</div>
<p>which is what we wanted to show.</p>
<p>(3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (4): By the Mean Value Theorem, to prove that <span class="math notranslate nohighlight">\(J''(\theta)\geq 0\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span>, it will suffice to show that <span class="math notranslate nohighlight">\(J'\)</span> is an increasing function. For this, suppose given <span class="math notranslate nohighlight">\(a,b\in \bbr\)</span> with <span class="math notranslate nohighlight">\(a&lt; b\)</span>. By hypothesis, we have both</p>
<div class="math notranslate nohighlight">
\[
J(b) \geq J'(a)(b-a) + J(a) \quad \text{and} \quad J(a) \geq J'(b)(a-b) + J(b).
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
J'(a)(b-a) + J(a) \leq J(b) \leq -J'(b)(a-b) + J(a)
\]</div>
<p>and so <span class="math notranslate nohighlight">\(J'(a) \leq J'(b)\)</span>.</p>
<p>(4) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2): Let <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>. The Mean Value Theorem is then used twice: Once to conclude that <span class="math notranslate nohighlight">\(J'\)</span> is an increasing function, and then to show that there are numbers <span class="math notranslate nohighlight">\(u\in (a,b)\)</span> and <span class="math notranslate nohighlight">\(v\in (b,c)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
J'(u) = \frac{J(b)-J(a)}{b-a} \quad \text{and} \quad J'(v) = \frac{J(c)-J(b)}{b-c}.
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\frac{J(b)-J(a)}{b-a} = J'(u) \leq J'(v) = \frac{J(c)-J(b)}{b-c}
\]</div>
<p>since <span class="math notranslate nohighlight">\(u&lt;v\)</span> and <span class="math notranslate nohighlight">\(J'\)</span> is increasing. Q.E.D.</p>
</div>
<p>Our goal now is to generalize the main theorem to convex functions of multiple variables. To do this, we will use the theorem for functions of a single variable, along with the following device: Suppose <span class="math notranslate nohighlight">\(g:\bbr \to \bbr\)</span> is a twice-differentiable function, <span class="math notranslate nohighlight">\(s,t\)</span> are fixed distinct real numbers, and we define</p>
<div class="math notranslate nohighlight" id="equation-aux-func-eq">
<span class="eqno">(11.26)<a class="headerlink" href="#equation-aux-func-eq" title="Link to this equation">#</a></span>\[
h:\bbr \to \bbr, \quad h(r) = g\big(r(s-t)+t\big).
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight" id="equation-aux-tangent-eq">
<span class="eqno">(11.27)<a class="headerlink" href="#equation-aux-tangent-eq" title="Link to this equation">#</a></span>\[
g(s) \geq g'(t) (s-t) + g(t) \quad \Leftrightarrow \quad h(1) \geq h'(0) + h(0)
\]</div>
<p>while</p>
<div class="math notranslate nohighlight" id="equation-aux-curv-eq">
<span class="eqno">(11.28)<a class="headerlink" href="#equation-aux-curv-eq" title="Link to this equation">#</a></span>\[
g''(t) \geq 0 \quad \Leftrightarrow \quad h''(0) \geq 0.
\]</div>
<div class="proof theorem admonition" id="main-convex-multi-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.8 </span> (Main theorem on convex functions (multi-variable version))</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J: \bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>. The following statements are equivalent:</p>
<ol class="arabic">
<li><p>The function <span class="math notranslate nohighlight">\(J\)</span> is convex.</p></li>
<li><p>The graph of <span class="math notranslate nohighlight">\(J\)</span> lies above its tangent planes, i.e., for all <span class="math notranslate nohighlight">\(\btheta,\bv\in \bbr^n\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    J(\bv + \btheta) \geq \bv^\intercal \nabla J(\btheta) + J(\btheta).
    \]</div>
</li>
<li><p>The Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta)\)</span> is positive semidefinite</p></li>
</ol>
<p>Moreover, if <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a stationary point of <span class="math notranslate nohighlight">\(J\)</span> and <span class="math notranslate nohighlight">\(J\)</span> is convex, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a global minimizer.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Throughout the proof we fix <span class="math notranslate nohighlight">\(\btheta,\bv\in \bbr^n\)</span> and we consider the function</p>
<div class="math notranslate nohighlight" id="equation-aux-2-eq">
<span class="eqno">(11.29)<a class="headerlink" href="#equation-aux-2-eq" title="Link to this equation">#</a></span>\[
g: \bbr \to \bbr, \quad g(t) \def J\big(t \bv + \btheta \big).
\]</div>
<p>Notice that this is exactly the function used in <a class="reference internal" href="#curvature-der-sec"><span class="std std-numref">Section 11.2</span></a> in the definitions of the directional first and second derivatives.</p>
<p>(1) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2): Supposing that <span class="math notranslate nohighlight">\(J\)</span> is convex, the function <span class="math notranslate nohighlight">\(g\)</span> is easily seen to be convex. Thus,</p>
<div class="math notranslate nohighlight">
\[
J(\bv + \btheta) = g(1) \geq g'(0) + g(0) = \bv^\intercal \nabla J(\btheta) + J(\btheta),
\]</div>
<p>where the second equality follows from <a class="reference internal" href="#directional-der-def">Definition 11.2</a> and <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>.</p>
<p>(2) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (3): For real numbers <span class="math notranslate nohighlight">\(s,t\in \bbr\)</span>, we shall prove</p>
<div class="math notranslate nohighlight" id="equation-goal-tan-line-eq">
<span class="eqno">(11.30)<a class="headerlink" href="#equation-goal-tan-line-eq" title="Link to this equation">#</a></span>\[
g(s) \geq g'(t)(s-t) + g(t)
\]</div>
<p>through <a class="reference internal" href="#equation-aux-tangent-eq">(11.27)</a> using the auxiliary function <span class="math notranslate nohighlight">\(h\)</span> defined in terms of <span class="math notranslate nohighlight">\(g\)</span> via <a class="reference internal" href="#equation-aux-func-eq">(11.26)</a>. But note that</p>
<div class="math notranslate nohighlight">
\[
h(1) = J(s\bv + \btheta) \quad \text{and} \quad h(0) = J(t\bv + \btheta)
\]</div>
<p>while</p>
<div class="math notranslate nohighlight">
\[
h'(0) = (s-t) \bv^\intercal \nabla J(t\bv + \btheta)
\]</div>
<p>by <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a>. But by hypothesis, we have</p>
<div class="math notranslate nohighlight">
\[
J(s\bv + \btheta) \geq (s-t) \bv^\intercal \nabla J(t\bv + \btheta) + J(t\bv + \btheta),
\]</div>
<p>from which <a class="reference internal" href="#equation-goal-tan-line-eq">(11.30)</a> follows. So, since the graph of <span class="math notranslate nohighlight">\(g\)</span> lies above its tangent lines, we know what <span class="math notranslate nohighlight">\(g''(t)\geq 0\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and, in particular, that <span class="math notranslate nohighlight">\(g''(0)\geq 0\)</span>. However, from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a> we get the equality in</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal \nabla^2 J(\btheta) \bv = g''(0)   \geq 0,
\]</div>
<p>which shows that the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta)\)</span> is positive semidefinite since <span class="math notranslate nohighlight">\(\bv\)</span> was chosen arbitrarily.</p>
<p>(3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (1): We need to prove that</p>
<div class="math notranslate nohighlight" id="equation-new-goal-eq">
<span class="eqno">(11.31)<a class="headerlink" href="#equation-new-goal-eq" title="Link to this equation">#</a></span>\[
J(t\bv + \btheta) \leq (1-t) J(\btheta) + t J (\bv + \btheta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>. But note that this is the same inequality as</p>
<div class="math notranslate nohighlight">
\[
g(t) \leq (1-t) g(0) + tg(1),
\]</div>
<p>so it will suffice to show that <span class="math notranslate nohighlight">\(g\)</span> is convex. But to do this, we shall show <span class="math notranslate nohighlight">\(g''(t)\geq 0\)</span> through <a class="reference internal" href="#equation-aux-curv-eq">(11.28)</a> and the auxiliary function <span class="math notranslate nohighlight">\(h\)</span>. However, we have</p>
<div class="math notranslate nohighlight">
\[
h''(0) = (s-t)^2\bv^\intercal \nabla^2 J(t\bv + \btheta)\bv \geq 0
\]</div>
<p>from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.2</a> and positive semidefiniteness of the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(t\bv + \btheta)\)</span>.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10-info-theory.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Information theory</p>
      </div>
    </a>
    <a class="right-next"
       href="12-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Probabilistic graphical models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">11.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differential-geometry">11.2. Differential geometry</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">11.3. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">11.4. Stochastic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-convex-functions">11.5. Appendix: convex functions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>