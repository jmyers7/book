

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>11. Optimization &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bpi": "\\boldsymbol\\pi", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bp": "\\mathbf{p}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bA": "\\mathbf{A}", "bB": "\\mathbf{B}", "bE": "\\mathbf{E}", "bF": "\\mathbf{F}", "bD": "\\mathbf{D}", "bK": "\\mathbf{K}", "bS": "\\mathbf{S}", "bP": "\\mathbf{P}", "bQ": "\\mathbf{Q}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calJ": "\\mathcal{J}", "calH": "\\mathcal{H}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/11-optim';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12. Probabilistic graphical models" href="12-models.html" />
    <link rel="prev" title="10. Information theory" href="10-info-theory.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-halfway.html">9. The halfway point: pivoting toward models and data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-info-theory.html">10. Information theory</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-models.html">12. Probabilistic graphical models</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-learning.html">13. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-stats-estimators.html">14. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-asymptotic.html">15. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-CIs.html">16. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-hyp-test.html">17. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="18-lin-reg.html">18. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">19. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/11-optim.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">11.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curvature-and-derivatives-in-higher-dimensions">11.2. Curvature and derivatives in higher dimensions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">11.3. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">11.4. Stochastic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-convex-functions">11.5. Appendix: Convex functions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="optimization">
<span id="optim"></span><h1><span class="section-number">11. </span>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h1>
<p>Chapters 9 through 12 form a sequence, with the ultimate goal the construction and training of probabilistic models. In the <a class="reference internal" href="12-models.html#prob-models"><span class="std std-ref">next chapter</span></a>, we begin building up our library of such models—to <em>train</em> them, or to have them <em>learn</em> on datasets, means that we should bring the model probability distribution <span class="math notranslate nohighlight">\(p(x;\theta)\)</span> as close as possible to the empirical distribution <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> of the data by finding optimal values of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>. The “distance” between these two distributions is scored by the <a class="reference internal" href="10-info-theory.html#kl-div-sec"><span class="std std-ref">Kullback Leibler divergence</span></a>. The problem of <em>learning</em> is thus turned into a minimization problem, calling for the techniques that we will study in the current chapter.</p>
<a class="reference internal image-reference" href="../_images/prob-distance.svg"><img alt="../_images/prob-distance.svg" class="align-center" src="../_images/prob-distance.svg" width="75%" /></a>
<p> </p>
<p>Many of the optimization problems we will encounter do not have closed-form solutions, and so we will need to study methods for approximation. All those studied in this chapter are versions of an iterative method called <em>gradient descent</em>. For a warm-up, we will study a simple single-variable version of this method in <a class="reference internal" href="#univariate-grad-desc-sec"><span class="std std-numref">Section 11.1</span></a> before proceeding to the full multi-variable version in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Section 11.3</span></a>. Sandwiched between these two sections is <a class="reference internal" href="#curvature-der-sec"><span class="std std-numref">Section 11.2</span></a>, where we recall and review the tools from multi-variable calculus that allow the generalization from one variable to many. Then, we finish with <a class="reference internal" href="#sgd-sec"><span class="std std-numref">Section 11.4</span></a>, where we study a particular form of gradient descent, called <em>stochastic gradient descent</em>, that is specifically tailored for the objective functions encountered in training probabilistic models.</p>
<p>The inclusion of gradient-based optimization algorithms and their applications to parameter estimation is what distinguishes this book from a traditional book on mathematical statistics. This material is often included in texts on machine learning, but it is not in any text on statistics (that I know of). However, we are just <em>barely</em> scratching the surface of optimization and machine learning. If you are new to these fields and want to learn more, I suggest beginning with the fifth chapter of <span id="id1">[<a class="reference internal" href="bib.html#id10" title="I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT Press, 2016.">GBC16</a>]</span> for a quick overview. After this, you can move on to <span id="id2">[<a class="reference internal" href="bib.html#id11" title="M. Hardt and B. Recht. Patterns, predictions, and actions. Foundations of machine learning. Princeton University Press, 2022.">HR22</a>]</span>, before tackling the massive, encyclopedic texts <span id="id3">[<a class="reference internal" href="bib.html#id8" title="K. P. Murphy. Probabilistic machine learning. An introduction. MIT Press, 2022.">Mur22</a>]</span> and <span id="id4">[<a class="reference internal" href="bib.html#id9" title="K. P. Murphy. Probabilistic machine learning. Advanced topics. MIT Press, 2023.">Mur23</a>]</span>.</p>
<section id="gradient-descent-in-one-variable">
<span id="univariate-grad-desc-sec"></span><h2><span class="section-number">11.1. </span>Gradient descent in one variable<a class="headerlink" href="#gradient-descent-in-one-variable" title="Permalink to this heading">#</a></h2>
<p>In this section, we describe the single-variable version of the gradient descent algorithm to help motivate the general algorithm in arbitrary dimensions. To begin, consider the <em>optimization problem</em> of locating the minimum values of the polynomial function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \theta^4 - 6\theta^3 + 11\theta^2 - 7\theta + 4.
\]</div>
<p>This function is called the <em>objective function</em> of the optimization problem. Its graph is displayed in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.distributions.multivariate_normal</span> <span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;/Users/johnmyers/code/stats-book-materials/notebooks&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">gd_utils</span> <span class="kn">import</span> <span class="n">GD</span><span class="p">,</span> <span class="n">SGD</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">11</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">7</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/cb5df22df41394e5aff9b64db18bce9c0c008153877bb722fe43ce2cda26dde8.svg" src="../_images/cb5df22df41394e5aff9b64db18bce9c0c008153877bb722fe43ce2cda26dde8.svg" /></figure>
</div>
</div>
<p>From the graph, we see that the objective function has minimums of approximately <span class="math notranslate nohighlight">\(J(0.5)\)</span> and <span class="math notranslate nohighlight">\(J(2.7)\)</span>.</p>
<p>It will be convenient to introduce the following terminology for our optimization problems:</p>
<div class="proof definition admonition" id="extrema-def">
<p class="admonition-title"><span class="caption-number">Definition 11.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J: \bbr^n \to \bbr\)</span> be a function. A vector <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a <em>local minimizer</em> of <span class="math notranslate nohighlight">\(J(\btheta)\)</span> provided that</p>
<div class="math notranslate nohighlight">
\[
J(\btheta^\star) \leq J(\btheta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\btheta\)</span> in a neighborhood of <span class="math notranslate nohighlight">\(\btheta^\star\)</span>; if this inequality holds for <em>all</em> <span class="math notranslate nohighlight">\(\btheta\)</span>, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is called a <em>global minimizer</em> of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>. If we flip the inequality the other direction, then we obtain the definitions of <em>local</em> and <em>global maximizers</em>. Collectively, local and global minimizers and maximizers of <span class="math notranslate nohighlight">\(J(\btheta)\)</span> are called <em>extremizers</em>, and the values <span class="math notranslate nohighlight">\(J(\btheta^\star)\)</span> of the function where <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is an extremizer are called <em>extrema</em> or <em>extreme values</em>.</p>
</section>
</div><p>Using this terminology, we would say that <span class="math notranslate nohighlight">\(0.5\)</span> is (approximately) a local minimizer of our polynomial objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span>, while <span class="math notranslate nohighlight">\(2.7\)</span> is (approximately) a global minimizer.</p>
<p>Let’s see how the single-variable version of the <em>gradient descent (GD) algorithm</em> would solve our optimization problem. In this context, the algorithm is called the <em>optimizer</em>. This algorithm depends on an initial guess for a minimizer, as well as two parameters called the <em>learning rate</em> and the <em>number of gradient steps</em>. We will state the algorithm first, and then walk through some intuition for why it works:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The loop runs from <span class="math notranslate nohighlight">\(t=0\)</span> to <span class="math notranslate nohighlight">\(t=N-1\)</span>, inclusive. This convention is intended to match the implementation of <code class="docutils literal notranslate"><span class="pre">for</span></code> loops in Python for <code class="docutils literal notranslate"><span class="pre">t</span></code> ranging through the iterable <code class="docutils literal notranslate"><span class="pre">range(N)</span></code>.</p>
</aside>
<div class="proof algorithm admonition" id="single-var-gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.1 </span> (Single-variable gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<p>   1. <span class="math notranslate nohighlight">\(\theta := \theta_0\)</span></p>
<p>   2. For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do:</p>
<p>           3. <span class="math notranslate nohighlight">\(\theta := \theta - \alpha J'(\theta)\)</span></p>
<p>   4. Return <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</section>
</div><p>Beginning from an initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span>, the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop in the algorithm produces a sequence of <span class="math notranslate nohighlight">\(N+1\)</span> approximations</p>
<div class="math notranslate nohighlight">
\[
\theta_0,\theta_1,\ldots,\theta_t,\ldots,\theta_{N}
\]</div>
<p>to a (local) minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>. The last value <span class="math notranslate nohighlight">\(\theta_{N}\)</span> in the sequence is taken as the output of the algorithm; if the algorithm converges to a minimizer, then we should have <span class="math notranslate nohighlight">\(\theta_{N} \approx \theta^\star\)</span>.</p>
<p>The assignment</p>
<div class="math notranslate nohighlight" id="equation-update-rule-eqn">
<span class="eqno">(11.1)<a class="headerlink" href="#equation-update-rule-eqn" title="Permalink to this equation">#</a></span>\[
\theta := \theta - \alpha J'(\theta)
\]</div>
<p>in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is called the <em>update rule</em>. This form is convenient for implementation in code. But for theoretical analysis, it is often convenient to rewrite the rule as a <em>recurrence relation</em> in the form</p>
<div class="math notranslate nohighlight">
\[
\theta_{t+1} = \theta_{t} - \alpha J'(\theta_{t}),
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 0\)</span>. We say that the new parameter <span class="math notranslate nohighlight">\(\theta_{t+1}\)</span> is obtained by taking a <em>gradient step</em> from <span class="math notranslate nohighlight">\(\theta_{t}\)</span>. The first update occurs when <span class="math notranslate nohighlight">\(t=0\)</span>, yielding</p>
<div class="math notranslate nohighlight">
\[
\theta_1 = \theta_{0} - \alpha J'(\theta_{0}).
\]</div>
<p>To understand the intuition behind the algorithm, consider the two cases that the derivative <span class="math notranslate nohighlight">\(J'(\theta_0)\)</span> is positive or negative:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J_prime</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="mi">18</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">22</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="mi">7</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">+</span> <span class="mf">0.4</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">12.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="s2">&quot;$J&#39;(</span><span class="se">\\</span><span class="s2">theta_0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="mf">3.3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">-</span> <span class="mf">3.3</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">3.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="s2">&quot;$J&#39;(</span><span class="se">\\</span><span class="s2">theta_0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/8b91f7905fdd141663f873c4b5340c8e1d497061ae857a8a2300e3d7e8814b6b.svg" src="../_images/8b91f7905fdd141663f873c4b5340c8e1d497061ae857a8a2300e3d7e8814b6b.svg" /></figure>
</div>
</div>
<p>In this plot, we’ve drawn the tangent lines to the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> at two initial values <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and <span class="math notranslate nohighlight">\(\theta_0=3.3\)</span>. Since the derivatives are the slopes of these tangent lines, the sign of the derivative is negative when <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and positive when <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span>. In the first case, we have</p>
<div class="math notranslate nohighlight" id="equation-first-update-eqn">
<span class="eqno">(11.2)<a class="headerlink" href="#equation-first-update-eqn" title="Permalink to this equation">#</a></span>\[\theta_1 = -0.4 - \alpha J'(-0.4) &gt; -0.4\]</div>
<p>since <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, while in the second case we have</p>
<div class="math notranslate nohighlight" id="equation-second-update-eqn">
<span class="eqno">(11.3)<a class="headerlink" href="#equation-second-update-eqn" title="Permalink to this equation">#</a></span>\[\theta_1 = 3.3 - \alpha J'(3.3) &lt; 3.3.\]</div>
<p>But notice that the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = -0.4\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 0.5\)</span>, and so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-first-update-eqn">(11.2)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than the initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span>, provided that the (scaled) negative derivative</p>
<div class="math notranslate nohighlight" id="equation-neg-derivative-eqn">
<span class="eqno">(11.4)<a class="headerlink" href="#equation-neg-derivative-eqn" title="Permalink to this equation">#</a></span>\[-\alpha J'(\theta_0)\]</div>
<p>is not too large (in magnitude) causing the new <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” the minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Similarly, the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>, so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-second-update-eqn">(11.3)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than <span class="math notranslate nohighlight">\(\theta_0\)</span>, again provided that the (scaled) negative derivative <a class="reference internal" href="#equation-neg-derivative-eqn">(11.4)</a> is not too large in magnitude.</p>
<p>From these considerations, we conclude the following:</p>
<div class="proof observation admonition" id="gd-obs">
<p class="admonition-title"><span class="caption-number">Observation 11.1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>The negative derivative <span class="math notranslate nohighlight">\(-J'(\theta)\)</span> always “points downhill.”</p></li>
<li><p>When the gradient descent algorithm works, it locates a minimizer by following the negative derivative “downhill.”</p></li>
</ul>
</section>
</div><p>The sense in which the negative derivative “points downhill” is made precise by our observation that it is positive if the point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> sits on a decreasing portion of the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>, and it is negative if <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> is on an increasing portion of the graph. The role of the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is to scale down the magnitude of the negative derivative so that the gradient step in the update rule does not cause <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” a nearby minimizer.</p>
<p>Let’s run the gradient algorithm four times, with various settings of the parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">2e-1</span><span class="p">]}</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7a93d3d3239dc9ec48dab614ec610d3db3b85de70f56795d0ca49c23371ef40e.svg" src="../_images/7a93d3d3239dc9ec48dab614ec610d3db3b85de70f56795d0ca49c23371ef40e.svg" /></figure>
</div>
</div>
<p>In all four plots, the large magenta dot represents the initial point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span>, while the smaller dots represent the remaining <span class="math notranslate nohighlight">\(N\)</span> points</p>
<div class="math notranslate nohighlight">
\[
(\theta_1,J(\theta_1)), (\theta_2, J(\theta_2)),\ldots, (\theta_N,J(\theta_N)),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of gradient steps. In the first row of the figure, the algorithm appears to be converging in both cases to the nearest minimizer to the initial guesses. In the second row, the learning rate is (relatively) large, causing the first gradient steps to “overshoot” the nearest minimizers to the initial guesses. However, the algorithm still appears to converge in both cases.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 1 on the worksheet.</p>
</div>
<p>It is possible for the gradient descent algorithm to diverge, especially if the learning rate is too large. For example, suppose that we set the learning rate to <span class="math notranslate nohighlight">\(\alpha = 0.2\)</span> and use <span class="math notranslate nohighlight">\(\theta_0 = 3.5\)</span> as our initial guess. Then three steps of gradient descent produce the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-1</span><span class="p">,}</span>

<span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">55</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c80eda59dea6b9b14e40b83cd47e4dcb370bf5882b14ff9d94817be4ac8fa0bf.svg" src="../_images/c80eda59dea6b9b14e40b83cd47e4dcb370bf5882b14ff9d94817be4ac8fa0bf.svg" /></figure>
</div>
</div>
<p>We see already that <span class="math notranslate nohighlight">\(J(\theta_3) \approx 10^7\)</span>; in fact, we have <span class="math notranslate nohighlight">\(J(\theta_t) \to \infty\)</span> as <span class="math notranslate nohighlight">\(t\to\infty\)</span> for these particular parameters.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problems 2 and 3 on the worksheet.</p>
</div>
<p>Of course, one can often prevent divergence by simply using a smaller learning rate, but sometimes a large <em>initial</em> learning rate is desirable to help the algorithm quickly find the neighborhood of a minimizer. So, what we desire is a scheme to shrink the learning rate from (relatively) large values to (relatively) smaller ones as the algorithm runs. This scheme is called a <em>learning rate schedule</em>.</p>
<div class="proof algorithm admonition" id="single-variable-gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.2 </span> (Single-variable gradient descent with learning rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\beta \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta - \alpha (1-\beta)^{t+1} J'(\theta)\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</section>
</div><p>The new parameter <span class="math notranslate nohighlight">\(\beta\)</span>, called the <em>decay rate</em>, shrinks the learning rate as</p>
<div class="math notranslate nohighlight">
\[
\alpha (1-\beta) &gt; \alpha (1-\beta)^2 &gt;\cdots &gt; \alpha (1-\beta)^t &gt; \cdots &gt; \alpha (1-\beta)^N,
\]</div>
<p>provided that <span class="math notranslate nohighlight">\(\beta&gt; 0\)</span>. Setting <span class="math notranslate nohighlight">\(\beta=0\)</span> results in <em>no</em> change in the learning rate. In our diverging example above, setting the decay rate to <span class="math notranslate nohighlight">\(\beta=0.1\)</span> results in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-1</span><span class="p">,}</span>

<span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7f698fb7d9489313a9acdac07807bb592e843ff8d8d70b85898b84aa9badd087.svg" src="../_images/7f698fb7d9489313a9acdac07807bb592e843ff8d8d70b85898b84aa9badd087.svg" /></figure>
</div>
</div>
<p>We have carried out <span class="math notranslate nohighlight">\(N=8\)</span> gradient steps, and it appears that the algorithm has successfully located the minimizer <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>.</p>
<div class="admonition-problem-prompt admonition">
<p class="admonition-title">Problem Prompt</p>
<p>Do problem 4 on the worksheet.</p>
</div>
<p>The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> and decay rate <span class="math notranslate nohighlight">\(\beta\)</span> are often chosen by experimentation.</p>
<div class="admonition-tip admonition">
<p class="admonition-title">Tip</p>
<p>When using the gradient descent algorithm to solve an optimization problem, try beginning with a learning rate and decay rate of around <span class="math notranslate nohighlight">\(\alpha \approx 0.01\)</span> and <span class="math notranslate nohighlight">\(\beta \approx 0.1\)</span>, respectively.</p>
</div>
<p>These values may be tuned by the analyst by closely monitoring the values of the objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> as the algorithm runs. This is easy in the single-variable case, since one can plot the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>. In the multi-variable case, however, the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> may live in many more dimensions than we can visualize, so the analyst might track the values of the objective function against the number of gradient steps. For example, with our polynomial objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> from above and</p>
<div class="math notranslate nohighlight">
\[
\theta_0 = -0.5, \quad \alpha = 0.01, \quad \beta = 0.1,
\]</div>
<p>we would plot the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">}</span>

<span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c7eb57d9f4922dd75ae46828ee9520894d82b724c0b287248082b72a12f101d0.svg" src="../_images/c7eb57d9f4922dd75ae46828ee9520894d82b724c0b287248082b72a12f101d0.svg" /></figure>
</div>
</div>
<p>One may use this plot to decide on the total number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps; simply choose <span class="math notranslate nohighlight">\(N\)</span> large enough to reach a point where the plot “plateaus” or “levels out,” indicating that the algorithm is converging on a minimizer. Alternatively, the analyst may build an automatic stopping condition into the algorithm that halts when the magnitude between successive objective values is less than some chosen threshold, say</p>
<div class="math notranslate nohighlight">
\[
|J(\theta_t) - J(\theta_{t-1}) | &lt; \epsilon,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> is a small number.</p>
</section>
<section id="curvature-and-derivatives-in-higher-dimensions">
<span id="curvature-der-sec"></span><h2><span class="section-number">11.2. </span>Curvature and derivatives in higher dimensions<a class="headerlink" href="#curvature-and-derivatives-in-higher-dimensions" title="Permalink to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(\theta^\star\)</span> is an extremizer of a differentiable function <span class="math notranslate nohighlight">\(J:\bbr \to \bbr\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> must be a <em>stationary point</em> in the sense that</p>
<div class="math notranslate nohighlight" id="equation-stationary-eqn">
<span class="eqno">(11.5)<a class="headerlink" href="#equation-stationary-eqn" title="Permalink to this equation">#</a></span>\[
J'(\theta^\star)=0.
\]</div>
<p>The name arises from the observation that small (first-order infinitesimal) perturbations of <span class="math notranslate nohighlight">\(\theta^\star\)</span> do not change the value <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span>, i.e., the value <span class="math notranslate nohighlight">\(J(\theta^\star)\)</span> remains <em>stationary</em> under small perturbations. In certain very favorable situations, we may be able to solve the stationarity equation <a class="reference internal" href="#equation-stationary-eqn">(11.5)</a> for <span class="math notranslate nohighlight">\(\theta^\star\)</span> to obtain a formula in closed form. In this case, the iterative gradient descent algorithm is not needed. But <span class="math notranslate nohighlight">\(\theta^\star\)</span> being a solution to the stationarity equation <a class="reference internal" href="#equation-stationary-eqn">(11.5)</a> is only a <em>necessary</em> condition for it to be an extremizer—sufficient conditions may be obtained by considering the <em>local curvature</em> of the graph of <span class="math notranslate nohighlight">\(J\)</span> near <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<p>Our goal in this section is twofold: First, we briefly recall how these curvature considerations help us identify extremizers in the single-variable case. The relevant tools are the first and second derivatives. Then, we generalize these derivatives to higher dimensions to obtain <em>gradient vectors</em> and <em>Hessian matrices</em>. We indicate how local curvature in higher dimensions may be computed using these new tools and, in particular, how we may use them to identify extremizers.</p>
<p>So, let’s begin with the familiar routine from single-variable calculus called the <em>Second Derivative Test</em>. Given a point <span class="math notranslate nohighlight">\(\theta^\star\)</span> and a twice-differentiable function <span class="math notranslate nohighlight">\(J:\bbr \to \bbr\)</span>, the test splits into two cases:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(J'(\theta^\star) = 0\)</span> and <span class="math notranslate nohighlight">\(J''(\theta^\star) &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a local minimizer.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(J'(\theta^\star) = 0\)</span> and <span class="math notranslate nohighlight">\(J''(\theta^\star) &lt; 0\)</span>, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a local maximizer.</p></li>
</ol>
<p>Provided that the second derivative of <span class="math notranslate nohighlight">\(J\)</span> not only exists but is also <em>continuous</em>, then intuition for the Second Derivative Test may be explained via local curvature. Indeed, in the first case, positivity of the second derivative means that the graph of <span class="math notranslate nohighlight">\(J\)</span> is <em>convex</em> near <span class="math notranslate nohighlight">\(\theta^\star\)</span>, while in the second case negativity of the second derivative means that the graph is <em>concave</em>. The nature of the curvature helps us distinguish between minimizers and maximizers:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">functions</span><span class="p">,</span> <span class="n">axes</span><span class="p">)):</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;$J &#39;(0) = 0$, $J&#39;&#39;(0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;convex $\Rightarrow$ minimizer&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;$J &#39;(0) = 0$, $J &#39;&#39;(0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
        <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;concave $\Rightarrow$ maximizer&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/1dc2f1599d08b2352f7edd925f0ac74abf3fe1faacb370c13204e4020e8d6b7c.svg" src="../_images/1dc2f1599d08b2352f7edd925f0ac74abf3fe1faacb370c13204e4020e8d6b7c.svg" /></figure>
</div>
</div>
<p>To see <em>why</em> second derivatives encode local curvature, let’s suppose <span class="math notranslate nohighlight">\(J''(\theta^\star) &gt;0\)</span> at some point <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Then continuity of <span class="math notranslate nohighlight">\(J''\)</span> means that there is a number <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> such that <span class="math notranslate nohighlight">\(J''(\theta)&gt;0\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span> in the open interval <span class="math notranslate nohighlight">\(I = (\theta^\star - \epsilon, \theta^\star + \epsilon)\)</span> centered at <span class="math notranslate nohighlight">\(\theta^\star\)</span>. But the second derivative is the first derivative of the first derivative, and thus positivity of <span class="math notranslate nohighlight">\(J''\)</span> over <span class="math notranslate nohighlight">\(I\)</span> means that <span class="math notranslate nohighlight">\(J'\)</span> is increasing over <span class="math notranslate nohighlight">\(I\)</span>. Since the first derivative <span class="math notranslate nohighlight">\(J'\)</span> measures the slope of the graph of <span class="math notranslate nohighlight">\(J\)</span>, this must mean that the slopes increase as we move from left to right across <span class="math notranslate nohighlight">\(I\)</span>. Thus, <span class="math notranslate nohighlight">\(J\)</span> is convex near <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<p>We already met the notion of <em>concavity</em> back in <a class="reference internal" href="10-info-theory.html#kl-div-sec"><span class="std std-numref">Section 10.2</span></a> where it was fundamental in our proof of Gibb’s inequality in <a class="reference internal" href="10-info-theory.html#gibbs-thm">Theorem 10.3</a>. As in the figure above, a function is <em>concave</em> if it always lies below its secant lines, while it is <em>convex</em> if it always lies above. The precise definitions are in the <a class="reference internal" href="#app-conv-sec"><span class="std std-ref">appendix</span></a> at the end of the chapter. Both these shapes have implications for the search for extremizers—in particular, stationary points of concave and convex functions are <em>always</em> global extremizers. The proof of this claim, along with equivalent characterizations of concavity and convexity in terms of tangent lines and tangent (hyper)planes are given in the two main theorems in the <a class="reference internal" href="#app-conv-sec"><span class="std std-ref">appendix</span></a>. The claims are easily believable, while the proofs are annoyingly fussy. At least glance at the statements of the theorems to convince yourself that your intuition is on point, but do not feel compelled to go through the proofs line by line.</p>
<p>How might we generalize the Second Derivative Test to higher dimensions? To help gain insight into the answer, let’s first add only one additional dimension, going from a function of a single variable to a twice-differentiable function of two variables:</p>
<div class="math notranslate nohighlight">
\[
J:\bbr^2 \to \bbr, \quad \btheta \mapsto J(\btheta).
\]</div>
<p>Actually, to obtain the best results relating curvature to second derivatives, we shall assume that the second-order partial derivatives are <em>continuous</em>, though this isn’t strictly needed for some definitions and results below. Functions with continuous first- and second-order partial derivatives are said to be of <em>class <span class="math notranslate nohighlight">\(C^2\)</span></em> in the mathematical literature.</p>
<p>For example, let’s suppose that the graph of <span class="math notranslate nohighlight">\(J\)</span> is an upside down paraboloid:</p>
<a class="reference internal image-reference" href="../_images/paraboloid-plot.png"><img alt="../_images/paraboloid-plot.png" class="align-center" src="../_images/paraboloid-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>At any given point on this surface (like the one above the black dot) there is not a <em>single</em> slope and curvature, but rather <em>infinitely many</em> slopes and curvatures in all the different directions that one may step in the plane <span class="math notranslate nohighlight">\(\bbr^2\)</span>. These different directions may be represented as <em>directional vectors</em> in the plane; here are three examples:</p>
<a class="reference internal image-reference" href="../_images/directional-plot.png"><img alt="../_images/directional-plot.png" class="align-center" src="../_images/directional-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>Taking advantage of the very special circumstance that our graph is embedded as a surface in <span class="math notranslate nohighlight">\(\bbr^3\)</span>, we may visualize the slopes and curvatures in these three directions by first intersecting the graph with three vertical planes:</p>
<a class="reference internal image-reference" href="../_images/sectional-plot.png"><img alt="../_images/sectional-plot.png" class="align-center" src="../_images/sectional-plot.png" style="width: 75%;" /></a>
<p> </p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This figure and discussion might be slightly misleading, for in dimensions <span class="math notranslate nohighlight">\(\geq 3\)</span>, the curves on the surface that we are after are <em>not</em> obtained as intersections with hyperplanes. This figure very much relies on the fact that our graph is embedded in <span class="math notranslate nohighlight">\(\bbr^3\)</span>.</p>
</aside>
<p>The intersections of these vertical planes and the surface yield curves called <em>sections</em>—for the planes displayed in the plot above, the sections are a trio of downward opening parabolas. The slopes and curvatures on the surface in the three directions are then the slopes and curvatures of these sectional curves.</p>
<p>To obtain these slopes and curvatures, let’s suppose that <span class="math notranslate nohighlight">\(\bv\)</span> is one of the three directional vectors in the plane, with its tail hooked to the point <span class="math notranslate nohighlight">\(\btheta\)</span> represented by the black dot. As we let <span class="math notranslate nohighlight">\(t\in \bbr\)</span> vary, the vector sum</p>
<div class="math notranslate nohighlight">
\[
t \bv + \btheta
\]</div>
<p>traces out the line in the plane <span class="math notranslate nohighlight">\(\bbr^2\)</span> through <span class="math notranslate nohighlight">\(\btheta\)</span> and in the direction of <span class="math notranslate nohighlight">\(\bv\)</span>. It follows that the mapping</p>
<div class="math notranslate nohighlight">
\[
t\mapsto J(t\bv + \btheta)
\]</div>
<p>is exactly the sectional curve on the surface. Notice that this mapping is a real-valued function of a single real variable <span class="math notranslate nohighlight">\(t\)</span>, and thus it has first and second derivatives in the ordinary sense from single-variable calculus. These considerations motivate the following definition, which applies to functions of <span class="math notranslate nohighlight">\(n\)</span> variables (not just two).</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>We are <em>not</em> requiring directional vectors to have unit length!</p>
</aside>
<div class="proof definition admonition" id="directional-der-def">
<p class="admonition-title"><span class="caption-number">Definition 11.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point, and <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span> a vector. We define the <em>directional first derivative of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> in the direction <span class="math notranslate nohighlight">\(\bv\)</span></em> to be</p>
<div class="math notranslate nohighlight">
\[
J_\bv'(\btheta) \def \frac{\text{d}}{\text{d}t} \bigg|_{t=0} J(t\bv + \btheta),
\]</div>
<p>while we define the <em>directional second derivative</em> to be</p>
<div class="math notranslate nohighlight">
\[
J_\bv''(\btheta) \def \frac{\text{d}^2}{\text{d}t^2} \bigg|_{t=0} J(t\bv + \btheta).
\]</div>
<p>In this context, the vector <span class="math notranslate nohighlight">\(\bv\)</span> is called the <em>directional vector</em>.</p>
</section>
</div><p>The familiar relations between these directional derivatives and partial derivatives pass through the gadgets defined in the following box. The first is familiar to us from our course in multi-variable calculus:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The notation “<span class="math notranslate nohighlight">\(\nabla^2\)</span>” is also sometimes used for the <a class="reference external" href="https://en.wikipedia.org/wiki/Laplace_operator#">Laplace</a> operator!</p>
</aside>
<div class="proof definition admonition" id="grad-vec-def">
<p class="admonition-title"><span class="caption-number">Definition 11.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J : \bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> a point. We define the <em>gradient vector</em> to be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla J(\btheta) \def \begin{bmatrix} \displaystyle \frac{\partial J}{\partial \theta_i}(\btheta) \end{bmatrix} =
\begin{bmatrix}
\displaystyle\frac{\partial J}{\partial \theta_1}(\btheta) \\
\vdots \\
\displaystyle \frac{\partial J}{\partial \theta_n}(\btheta)
\end{bmatrix} \in \bbr^n,
\end{split}\]</div>
<p>while we define the the <em>Hessian matrix</em> to be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla^2 J(\btheta) \def \begin{bmatrix} \displaystyle \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}(\btheta) \end{bmatrix}
= \begin{bmatrix}
\displaystyle\frac{\partial^2 J}{\partial \theta_1^2}(\btheta) &amp; \cdots &amp;\displaystyle \frac{\partial^2 J}{\partial \theta_1 \partial \theta_n}(\btheta) \\
\vdots &amp; \ddots &amp; \vdots \\
\displaystyle\frac{\partial^2 J}{\partial \theta_n \partial \theta_1} (\btheta) &amp; \cdots &amp; \displaystyle\frac{\partial^2 J}{\partial \theta_n^2}(\btheta)
\end{bmatrix} \in \bbr^{n\times n}.
\end{split}\]</div>
</section>
</div><p>Note that since <span class="math notranslate nohighlight">\(J\)</span> is of class <span class="math notranslate nohighlight">\(C^2\)</span>, the Hessian matrix is symmetric.</p>
<p>The following important theorem expresses the relations between the first and second directional derivatives and the gradient vector and Hessian matrix.</p>
<div class="proof theorem admonition" id="directional-der-grad-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.1 </span> (Slopes, curvatures, and partial derivatives)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point, and <span class="math notranslate nohighlight">\(\bv \in \bbr^n\)</span> a directional vector.</p>
<ol class="arabic">
<li><p>We have</p>
<div class="math notranslate nohighlight">
\[
    J_{\bv}'(\btheta) = \bv^\intercal \nabla J(\btheta).
    \]</div>
</li>
<li><p>We have</p>
<div class="math notranslate nohighlight">
\[
    J_{\bv}''(\btheta) = \bv^\intercal \big(\nabla^2 J(\btheta)\big) \bv.
    \]</div>
</li>
</ol>
</section>
</div><p>So, the directional first derivative is obtained through an inner product with the gradient vector, while the directional second derivative is the quadratic form induced by the Hessian matrix.</p>
<div class="proof admonition" id="proof">
<p>Proof. The proofs are simple exercises using the multi-variable Chain Rule. Indeed, note that</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d}}{\text{d}t} J(t \bv + \btheta) = \sum_{i=1}^n v_i \frac{\partial J}{\partial \theta_i} (t\bv + \btheta).
\]</div>
<p>Plugging in <span class="math notranslate nohighlight">\(t=0\)</span> to both sides of this last equality then yields (1.). On the other hand, differentiating both sides of the equation with respect to <span class="math notranslate nohighlight">\(t\)</span> (and using the Chain Rule a second time) gives</p>
<div class="math notranslate nohighlight">
\[
\frac{\text{d}^2}{\text{d}t^2} J(t \bv + \btheta) = \sum_{i=1}^n v_i \frac{\text{d}}{\text{d}t}\frac{\partial J}{\partial \theta_i} (t\bv + \btheta)  = \sum_{i,j=1}^n v_i v_j \frac{\partial^2 J}{\partial \theta_i \partial \theta_j}(t\bv + \btheta).
\]</div>
<p>Plugging in <span class="math notranslate nohighlight">\(t=0\)</span> to both ends yields (2.). Q.E.D.</p>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Notice that the value of <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is the same if <span class="math notranslate nohighlight">\(\bv\)</span> is replaced with <span class="math notranslate nohighlight">\(-\bv\)</span>, by <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>. Thus, the local curvature at <span class="math notranslate nohighlight">\(\btheta\)</span> only depends on the <em>line</em> indicated by <span class="math notranslate nohighlight">\(\bv\)</span>, not the direction that it points.</p>
</aside>
<p>When the directional vector <span class="math notranslate nohighlight">\(\bv\)</span> is a <em>unit</em> vector, the value of the directional first derivative <span class="math notranslate nohighlight">\(J'_\bv(\btheta)\)</span> is interpreted as the (instantaneous) rate of change of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> in the direction indicated by <span class="math notranslate nohighlight">\(\bv\)</span>. Likewise, if <span class="math notranslate nohighlight">\(\bv\)</span> is unit vector, then the value of the directional second derivative <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is interpreted as the local curvature of <span class="math notranslate nohighlight">\(J\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span> through the line indicated by <span class="math notranslate nohighlight">\(\bv\)</span>.</p>
<p>For our purposes, the most important properties of the gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> are (1) that it points in the direction of <em>maximum</em> rate of change, (2) its negative points in the direction of <em>minimum</em> rate of change, and (3) it is orthogonal to the <em>level surfaces</em> of <span class="math notranslate nohighlight">\(J\)</span>, otherwise called <em>contours</em>. By definition, such a surface is the <span class="math notranslate nohighlight">\((n-1)\)</span>-dimensional set of solutions <span class="math notranslate nohighlight">\(\btheta\in \bbr^n\)</span> to an equation</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = c
\]</div>
<p>for fixed <span class="math notranslate nohighlight">\(c\in \bbr\)</span>. In the case that <span class="math notranslate nohighlight">\(n=2\)</span>, these level surfaces are actually <em>level curves</em>; for our upside down paraboloid above, they are the blue ellipses in the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">15</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mf">0.1</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">8</span> <span class="o">/</span> <span class="mf">3.5</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.75</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.75</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.5</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/36f547da9f14e39d72f440fa1a65e32e9b67b294c3389ce0ceb3058dc154e3ef.svg" src="../_images/36f547da9f14e39d72f440fa1a65e32e9b67b294c3389ce0ceb3058dc154e3ef.svg" /></figure>
</div>
</div>
<p>The vector in the figure is the gradient vector, and the magenta line is the tangent line to the contour that passes through the point at the tail of the gradient. Note that the gradient is orthogonal to this tangent line, and therefore also orthogonal to the contour.</p>
<p>In general, it should be intuitively clear from the equation</p>
<div class="math notranslate nohighlight">
\[
J_{\bv}'(\btheta) = \bv^\intercal J(\btheta)
\]</div>
<p>in <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a> that the gradient is orthogonal to level surfaces. Indeed, if <span class="math notranslate nohighlight">\(\bv\)</span> is a tangent vector to the level surface passing through <span class="math notranslate nohighlight">\(\btheta\)</span>, then <span class="math notranslate nohighlight">\(J\)</span> should <em>not</em> change (at least up to first order) as we step in the direction of <span class="math notranslate nohighlight">\(\bv\)</span> since (by definition) the function <span class="math notranslate nohighlight">\(J\)</span> is constant along its level surfaces. Thus, we have <span class="math notranslate nohighlight">\(J_\bv'(\btheta)=0\)</span>, and so <span class="math notranslate nohighlight">\(\bv^\intercal \nabla J(\btheta) =0\)</span>. This shows the gradient vector is indeed orthogonal to the level surface passing through <span class="math notranslate nohighlight">\(\btheta\)</span>. (For a more rigorous argument, see the proposition on page 23 of <span id="id5">[<a class="reference internal" href="bib.html#id13" title="V. Guillemin and A. Pollack. Differential topology. AMS Chelsea Publishing, 2010.">GP10</a>]</span>.)</p>
<p>Let’s return to the first two properties of the gradient vector mentioned above, that it points in the direction of maximum rate of change and its negative points in the direction of minimum rate of change. In informal treatments, these claims are often justified by an appeal to the “angle” <span class="math notranslate nohighlight">\(\phi\in [0,\pi]\)</span> between two vectors <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span>, which allegedly fits into an equation</p>
<div class="math notranslate nohighlight" id="equation-geom-dot-eq">
<span class="eqno">(11.6)<a class="headerlink" href="#equation-geom-dot-eq" title="Permalink to this equation">#</a></span>\[
\bv^\intercal \bu = |\bu||\bv| \cos{\phi}.
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(\bu = \nabla J(\btheta)\)</span> and supposing <span class="math notranslate nohighlight">\(\bv\)</span> has unit length, we get</p>
<div class="math notranslate nohighlight">
\[
J_\bv'(\btheta) = \bv^\intercal \nabla J(\btheta) = |\nabla J(\btheta) | \cos{\phi}
\]</div>
<p>from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>. Since <span class="math notranslate nohighlight">\(\cos{\phi}\)</span> is maximized and minimized over <span class="math notranslate nohighlight">\([0,\pi]\)</span> when <span class="math notranslate nohighlight">\(\phi=0\)</span> and <span class="math notranslate nohighlight">\(\phi=\pi\)</span>, respectively, it then follows that the gradient points in the direction of maximum rate of change, while its negative points in the direction of minimum rate of change. However, this argument does not address what is meant by the “angle” <span class="math notranslate nohighlight">\(\phi\)</span> between the vectors—certainly in two and three dimensions we have some idea of what this angle might be, but what about in 1000 dimensions?</p>
<p>But the “angle” <span class="math notranslate nohighlight">\(\phi\)</span> is just a distraction. It is much cleaner logically to work directly with the <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a>, which is the true reason that the gradient has these extremizing properties. An immediate corollary of this inequality says that an innner product of one vector against a unit vector is maximized (minimized) when the unit vector points in the same (opposite) direction as the first vector. In the following, we give a proof using this inequality, and then show afterwards how it may be used to give a rigorous definition of the angle <span class="math notranslate nohighlight">\(\phi\)</span>.</p>
<div class="proof theorem admonition" id="grad-uphill-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.2 </span> (Properties of gradient vectors)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span>, <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point, and suppose the gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> is nonzero.</p>
<ol class="arabic simple">
<li><p>The gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> points in the direction of maximum rate of change.</p></li>
<li><p>The negative gradient vector <span class="math notranslate nohighlight">\(-\nabla J(\btheta)\)</span> points in the direction of minimum rate of change.</p></li>
<li><p>The gradient vector <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> is orthogonal to level surfaces.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We already offered a “proof” of the third statement above, so we need only prove the first two. For this, we recall that for any two vectors <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span>, the Cauchy-Schwarz inequality states that</p>
<div class="math notranslate nohighlight" id="equation-cs-ineq-eq">
<span class="eqno">(11.7)<a class="headerlink" href="#equation-cs-ineq-eq" title="Permalink to this equation">#</a></span>\[
|\bu^\intercal \bv| \leq |\bu | |\bv|,
\]</div>
<p>with equality if and only if <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span> are parallel (i.e., one is a scalar multiple of the other). In particular, if we take <span class="math notranslate nohighlight">\(\bu = \nabla J(\btheta)\)</span>, let <span class="math notranslate nohighlight">\(\bv\)</span> be a unit vector, and use <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>, we get</p>
<div class="math notranslate nohighlight">
\[
-|\nabla J(\btheta )| \leq J_\bv'(\btheta) = \bv^ \intercal \nabla J(\btheta) \leq |\nabla J(\btheta )|.
\]</div>
<p>The goal is then to identify unit vectors <span class="math notranslate nohighlight">\(\bv\)</span> that extremize the derivative <span class="math notranslate nohighlight">\(J_\bv'(\btheta)\)</span> in these bounds.</p>
<p>But if the derivative achieves the upper bound, then by the criterion for equality in the Cauchy-Schwarz inequality <a class="reference internal" href="#equation-cs-ineq-eq">(11.7)</a>, there must be a nonzero scalar <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\bv = \alpha \nabla J(\btheta).
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\alpha |\nabla J(\btheta)|^2 = \bv^\intercal \nabla J(\btheta) = |\nabla J(\btheta)|,
\]</div>
<p>and so <span class="math notranslate nohighlight">\(\alpha = 1/ |\nabla J(\btheta)|\)</span>. Hence, the derivative <span class="math notranslate nohighlight">\(J_\bv'(\btheta)\)</span> achieves its maximum value exactly when <span class="math notranslate nohighlight">\(\bv\)</span> is the normalized gradient vector. It is just as easy to show that the derivative achieves its minimum value when <span class="math notranslate nohighlight">\(\bv\)</span> is the negative of the normalized gradient vector. Q.E.D.</p>
</div>
<p>Now, let’s return to the “angle” <span class="math notranslate nohighlight">\(\phi\)</span> between two nonzero vectors <span class="math notranslate nohighlight">\(\bu\)</span> and <span class="math notranslate nohighlight">\(\bv\)</span> in <span class="math notranslate nohighlight">\(\bbr^n\)</span>. From the Cauchy-Schwarz inequality <a class="reference internal" href="#equation-cs-ineq-eq">(11.7)</a>, it follows that</p>
<div class="math notranslate nohighlight">
\[
-1 \leq \frac{\bv^\intercal \bu}{|\bu||\bv|} \leq 1.
\]</div>
<p>We then <em>define</em> the <em>angle</em> <span class="math notranslate nohighlight">\(\phi\)</span> between the two vectors to be the unique number <span class="math notranslate nohighlight">\(\phi \in [0,\pi]\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\cos{\phi} = \frac{\bv^\intercal \bu}{|\bu||\bv|}.
\]</div>
<p>Thus, the fact that the angle <span class="math notranslate nohighlight">\(\phi\)</span> even <em>exists</em> is a consequence of the Cauchy-Schwarz inequality.</p>
<p>Observe that the part in <a class="reference internal" href="#grad-uphill-thm">Theorem 11.2</a> about the negative gradient vector “pointing downhill” is the higher-dimensional version of the observation in <a class="reference internal" href="#gd-obs">Observation 11.1</a> regarding the negative derivative of a single-variable function. This property will be key to the general multi-variable gradient descent algorithm that we will discuss in <a class="reference internal" href="#multivariate-grad-desc-sec"><span class="std std-numref">Section 11.3</span></a> below.</p>
<p>Let’s now turn toward extremizers of a multi-variable function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span> in arbitrary dimension <span class="math notranslate nohighlight">\(n\)</span>. As you may recall from multi-variable calculus, the stationarity equation</p>
<div class="math notranslate nohighlight">
\[
\nabla(J(\btheta^\star)) = 0
\]</div>
<p>is a <em>necessary</em> condition for <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to be an extremizer of <span class="math notranslate nohighlight">\(J\)</span>. As in the single-variable case, one might hope to classify stationary points (i.e., solutions <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to the stationarity equation) as minimizers and maximizers based on the local curvature of <span class="math notranslate nohighlight">\(J\)</span>. For if <span class="math notranslate nohighlight">\(J\)</span> is convex (concave) in <em>all</em> directions at <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, then intuition suggests that <span class="math notranslate nohighlight">\(\btheta^\star\)</span> should be a local minimizer (maximizer). But from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>, the local directional curvatures at <span class="math notranslate nohighlight">\(\btheta^\star\)</span> are measured by the quadratic form</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal \big( \nabla^2 J(\btheta^\star) \big) \bv
\]</div>
<p>as <span class="math notranslate nohighlight">\(\bv\)</span> cycles through all nonzero vectors in <span class="math notranslate nohighlight">\(\bbr^n\)</span>. Thus, if these numbers are <em>always</em> positive (negative), then we would expect the stationary point <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer (maximizer). However, to say that these numbers are either always positive or negative means exactly that the Hessian matrix is positive definite or negative definite, in the language of <a class="reference internal" href="08-more-prob.html#first-semidefinite-def">Definition 8.8</a>. So, the question becomes: If we know that the Hessian matrix is positive (negative) definite at a stationary point <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, is <span class="math notranslate nohighlight">\(\btheta^\star\)</span> necessarily a local minimizer (maximizer)?</p>
<p>The answer is <em>yes</em>!</p>
<div class="proof theorem admonition" id="second-der-test-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.3 </span> (Second Derivative Test)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star \in \bbr^n\)</span> a stationary point.</p>
<ol class="arabic simple">
<li><p>If the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta^\star)\)</span> is positive definite, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer.</p></li>
<li><p>If the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta^\star)\)</span> is negative definite, then <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local maximizer.</p></li>
</ol>
</section>
</div><p>For a proof of this result, see Theorem 13.10 in <span id="id6">[<a class="reference internal" href="bib.html#id14" title="T. Apostol. Mathematical analysis. Addison-Weley Publishing Comparny, Inc., second edition, 1974.">Apo74</a>]</span>. Note also that if the Hessian matrix is either positive semidefinite or negative semidefinite <em>everywhere</em>, then every stationary point is a global extremizer; see <a class="reference internal" href="#main-convex-multi-thm">Theorem 11.7</a> in the appendix.</p>
<p>With infinitely many local (directional) curvatures at a point on the graph of a function <span class="math notranslate nohighlight">\(J:\bbr^n\to \bbr\)</span>, it will be convenient to obtain a single number that attempts to summarize the complexity of the local curvature. The first step toward obtaining such a summary is given in the following:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Remember, as we saw in <a class="reference internal" href="08-more-prob.html#psd-char-thm">Theorem 8.19</a>, the eigenvalues of a positive definite matrix are all real and positive. Hence the linear ordering <a class="reference internal" href="#equation-ordering-eqn">(11.8)</a>. The existence of the orthonormal basis of eigenvectors is guaranteed by the Spectral Theorem (see the proof of <a class="reference internal" href="08-more-prob.html#psd-char-thm">Theorem 8.19</a>).</p>
</aside>
<div class="proof theorem admonition" id="max-min-curve-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.4 </span> (Eigenvalues, eigenvectors, and local curvature)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta \in \bbr^n\)</span> a point with positive definite Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta)\)</span>. Suppose we linearly order the eigenvalues of the Hessian matrix as</p>
<div class="math notranslate nohighlight" id="equation-ordering-eqn">
<span class="eqno">(11.8)<a class="headerlink" href="#equation-ordering-eqn" title="Permalink to this equation">#</a></span>\[
0 &lt; \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n.
\]</div>
<p>Then:</p>
<ol class="arabic simple">
<li><p>The directional curvature <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is maximized exactly when <span class="math notranslate nohighlight">\(\bv\)</span> lies in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_n\)</span>, in which case <span class="math notranslate nohighlight">\(J''_\bv(\btheta) = \lambda_n\)</span>.</p></li>
<li><p>The directional curvature <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is minimized exactly when <span class="math notranslate nohighlight">\(\bv\)</span> lies in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_1\)</span>, in which case <span class="math notranslate nohighlight">\(J''_\bv(\btheta) = \lambda_1\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(\be_1,\ldots,\be_n\)</span> be the associated orthonormal basis of eigenvectors with</p>
<div class="math notranslate nohighlight">
\[
\nabla^2 J(\btheta) \be_i = \lambda_i\be_i
\]</div>
<p>for each <span class="math notranslate nohighlight">\(i\)</span>. Given a unit vector <span class="math notranslate nohighlight">\(\bv\)</span>, there are unique scalars <span class="math notranslate nohighlight">\(\alpha_1,\ldots,\alpha_n\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\bv = \alpha_1 \be_1 + \cdots + \alpha_n \be_n
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-sum-to-one-eqn">
<span class="eqno">(11.9)<a class="headerlink" href="#equation-sum-to-one-eqn" title="Permalink to this equation">#</a></span>\[
\alpha_1^2 + \cdots + \alpha_n^2 =1.
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
J''_\bv(\btheta) = \bv^\intercal \big(\nabla^2 J(\btheta) \big) \bv = \sum_{i,j=1}^n \alpha_i\alpha_j \be_i^\intercal \big(\nabla^2 J(\btheta) \big) \be_j = \sum_{i,j=1}^n \alpha_i\alpha_j\lambda_j \be_i^\intercal \be_j = \sum_{i=1}^n \alpha_i^2 \lambda_i,
\]</div>
<p>where the first equality follows from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>. Using <a class="reference internal" href="#equation-sum-to-one-eqn">(11.9)</a>, eliminate <span class="math notranslate nohighlight">\(\alpha_n^2\)</span> from the last sum in favor of the other <span class="math notranslate nohighlight">\(\alpha\)</span>’s to get</p>
<div class="math notranslate nohighlight">
\[
J''_\bv(\btheta) = \sum_{i=1}^{n-1}(\lambda_i - \lambda_n)\alpha_i^2 + \lambda_n.
\]</div>
<p>Letting <span class="math notranslate nohighlight">\(m\)</span> be the smallest integer such that</p>
<div class="math notranslate nohighlight">
\[
\lambda_{m-1} &lt; \lambda_m = \lambda_{m+1} = \cdots = \lambda _n,
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
J''_\bv(\btheta) = \sum_{i=1}^{m-1}(\lambda_i - \lambda_n)\alpha_i^2 + \lambda_n.
\]</div>
<p>(If <span class="math notranslate nohighlight">\(m=1\)</span>, then we interpret this expression as <span class="math notranslate nohighlight">\(J''_\bv(\btheta) = \lambda_n\)</span>.) But <span class="math notranslate nohighlight">\(\lambda_i -\lambda_n &lt; 0\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m-1\)</span>, and so <span class="math notranslate nohighlight">\(J''_\bv(\btheta)\)</span> is clearly maximized when</p>
<div class="math notranslate nohighlight">
\[
\alpha_1 = \cdots = \alpha_{m-1} = 0,
\]</div>
<p>which implies that <span class="math notranslate nohighlight">\(\bv\)</span> lies in the eigenspace of <span class="math notranslate nohighlight">\(\lambda_n\)</span>. This establishes the claim in the first statement, and the one in the second follows from the same type of argument with the obvious changes. Q.E.D.</p>
</div>
<p>If the Hessian matrix is positive definite, then its extreme eigenvalues are exactly the extreme local (directional) curvatures. The ratio of the largest curvature to the smallest should then convey the “variance” or the “range” of these curvatures. This ratio has a name:</p>
<div class="proof definition admonition" id="condition-num-def">
<p class="admonition-title"><span class="caption-number">Definition 11.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> square matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\ldots,\lambda_n\)</span>.</p>
<ol class="arabic">
<li><p>The <em>spectral radius</em> of <span class="math notranslate nohighlight">\(A\)</span>, denoted <span class="math notranslate nohighlight">\(\rho(A)\)</span>, is given by</p>
<div class="math notranslate nohighlight">
\[
    \rho(A) \def \max_{i=1,\ldots,n} |\lambda_i|.
    \]</div>
</li>
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> is positive definite, the <em>condition number</em> of <span class="math notranslate nohighlight">\(A\)</span>, denoted <span class="math notranslate nohighlight">\(\kappa(A)\)</span>, is the ratio</p>
<div class="math notranslate nohighlight">
\[
    \kappa(A) \def \frac{\lambda_\text{max}}{\lambda_\text{min}}
    \]</div>
<p>of the largest eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> to the smallest. Note that since all eigenvalues of <span class="math notranslate nohighlight">\(A\)</span> are positive, <span class="math notranslate nohighlight">\(\lambda_\text{max}\)</span> is exactly the spectral radius <span class="math notranslate nohighlight">\(\rho(A)\)</span>.</p>
</li>
</ol>
</section>
</div><p>This definition of <em>condition number</em> applies only in the case that <span class="math notranslate nohighlight">\(A\)</span> is positive definite and hence all its eigenvalues are positive. In the general case, the definition needs to be altered; see <a class="reference external" href="https://en.wikipedia.org/wiki/Condition_number#Matrices">here</a>, for example.</p>
<p>Intuitively, when the condition number of a positive definite Hessian matrix is large (in which case the Hessian matrix is called <em>ill-conditioned</em>), the curvatures vary widely as we look in all different directions; conversely, when the condition number is near <span class="math notranslate nohighlight">\(1\)</span>, the directional curvatures are all nearly the same. As we will see in the next section, ill-conditioned Hessian matrices inflate an important upper-bound on the speed of convergence of gradient descent. In other words, ill-conditioned Hessian matrices <em>may</em> signal slow convergence of gradient descent.</p>
</section>
<section id="gradient-descent-in-multiple-variables">
<span id="multivariate-grad-desc-sec"></span><h2><span class="section-number">11.3. </span>Gradient descent in multiple variables<a class="headerlink" href="#gradient-descent-in-multiple-variables" title="Permalink to this heading">#</a></h2>
<p>With the gradient vector taking the place of the derivative, it is easy to generalize the single-variable gradient descent algorithm from <a class="reference internal" href="#single-variable-gd-alg">Algorithm 11.2</a> to multiple variables:</p>
<div class="proof algorithm admonition" id="gd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.3 </span> (Multi-variable gradient descent with learning rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable function <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^n\)</span> for a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\beta \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta - \alpha(1-\beta)^{t+1} \nabla J(\btheta)\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ol>
</section>
</div><p>Just like the single-variable version, beginning from an initial guess <span class="math notranslate nohighlight">\(\btheta_0\)</span> for a (local) minimizer, the algorithm outputs a sequence of <span class="math notranslate nohighlight">\(N+1\)</span> approximations</p>
<div class="math notranslate nohighlight">
\[
\btheta_0,\btheta_1, \ldots,\btheta_t,\ldots,\btheta_N
\]</div>
<p>to a local minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<p>For an example, let’s consider the polynomial objective function</p>
<div class="math notranslate nohighlight" id="equation-two-dim-poly-eq">
<span class="eqno">(11.10)<a class="headerlink" href="#equation-two-dim-poly-eq" title="Permalink to this equation">#</a></span>\[
J(\btheta) = J(\theta_1,\theta_2) = (\theta_1^2 + 10 \theta_2^2)\big((\theta_1-1)^2 + 10(\theta_2-1)^2 \big)
\]</div>
<p>in two dimensions. Its graph looks like</p>
<a class="reference internal image-reference" href="../_images/objective-plot.png"><img alt="../_images/objective-plot.png" class="align-center" src="../_images/objective-plot.png" style="width: 75%;" /></a>
<p> </p>
<p>while its contour plot is</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
   <span class="n">theta1</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
   <span class="n">theta2</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
   <span class="k">return</span> <span class="p">(</span><span class="n">theta1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">theta2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">theta1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mf">0.50</span><span class="p">:</span><span class="mf">1.5</span><span class="p">:</span><span class="mf">0.01</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">:</span><span class="mf">1.3</span><span class="p">:</span><span class="mf">0.01</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/ee9b478bbe8c710b6995aaee5c50bf0a38e01609c937ac5f6c6929df23a1d0c4.svg" src="../_images/ee9b478bbe8c710b6995aaee5c50bf0a38e01609c937ac5f6c6929df23a1d0c4.svg" /></figure>
</div>
</div>
<p>The function has two minimizers at</p>
<div class="math notranslate nohighlight">
\[
\btheta^\star = (0, 0), (1,1),
\]</div>
<p>as well as a “saddle point” at <span class="math notranslate nohighlight">\((0.5, 0.5)\)</span> where the gradient <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> vanishes. Let’s run the gradient descent algorithm four times beginning with <em>no</em> learning rate decay, and track the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> plotted over the contours of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;first runs of gradient descent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/9b85ecb7413264f25e3d00e0c70a156c97e8afe731a5ee0688f6d4080ffd201c.svg" src="../_images/9b85ecb7413264f25e3d00e0c70a156c97e8afe731a5ee0688f6d4080ffd201c.svg" /></figure>
</div>
</div>
<p>The large magenta dots in the plots indicate the initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span>, while the smaller dots indicate the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> for <span class="math notranslate nohighlight">\(t&gt;0\)</span>. The algorithm appears to be converging nicely to the minimizer <span class="math notranslate nohighlight">\(\btheta^\star = (1,1)\)</span> in the upper-left plot, while in the other three plots, the algorithm finds a neighborhood of a minimizer, but then oscillates back and forth and never appears to settle down. This is due jointly to the elliptical (non-circular) shape of the contours, the choice of initial guesses, and poorly chosen learning rates. Notice that the initial guesses in the top two plots are nearly identical, but they lead to quite different convergence behavior.</p>
<p>In particular, since the gradient is orthogonal to contours (see <a class="reference internal" href="#grad-uphill-thm">Theorem 11.2</a>), in all the plots except the top-left one, we see that the negative gradients (which the algorithm is following) do <em>not</em> point directly toward the minimizers. The elliptical nature of the contours creates local curvatures at the minimizers that are quite different depending on which direction you look. From the previous section, we know that the local curvatures are encoded in the Hessian matrix, and the “variance” or “range” of the local curvatures is scored by its condition number. This suggests that studying the Hessian matrix might lead to insights into the convergence properties of gradient descent.</p>
<p>To begin this study, let’s start more generally with a function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star\)</span> a point. We then take a degree-<span class="math notranslate nohighlight">\(2\)</span> Taylor polynomial approximation centered at <span class="math notranslate nohighlight">\(\btheta^\star\)</span>:</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) \approx J(\btheta^\star) + (\btheta - \btheta^\star)^\intercal \nabla J(\btheta^\star) + \frac{1}{2} (\btheta - \btheta^\star)^\intercal \big(\nabla^2 J(\btheta^\star) \big) (\btheta - \btheta^\star).
\]</div>
<p>An approximation of the local geometry of the graph of <span class="math notranslate nohighlight">\(J\)</span> near <span class="math notranslate nohighlight">\(\btheta^\star\)</span> may be obtained by replacing <span class="math notranslate nohighlight">\(J\)</span> with its Taylor polynomial on the right-hand side; thus, for our purposes, we may as well assume that <span class="math notranslate nohighlight">\(J\)</span> is given by a degree-<span class="math notranslate nohighlight">\(2\)</span> (inhomogeneous) polynomial:</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{2}\btheta^\intercal H \btheta + \bb^\intercal \btheta + c,
\]</div>
<p>where <span class="math notranslate nohighlight">\(H \in \bbr^{n\times n}\)</span> is a symmetric matrix, <span class="math notranslate nohighlight">\(\bb\in \bbr^n\)</span> is a vector, and <span class="math notranslate nohighlight">\(c\in \bbr\)</span> is a scalar. As you may easily compute, the gradient and Hessian matrices are given by</p>
<div class="math notranslate nohighlight">
\[
\nabla J(\btheta) = H \btheta + \bb \quad \text{and} \quad  \nabla^2 J(\btheta) = H.
\]</div>
<p>Assuming that the decay rate is <span class="math notranslate nohighlight">\(\beta=0\)</span>, the update rule in the algorithm is given by</p>
<div class="math notranslate nohighlight">
\[
\btheta_{t+1} = \btheta_t - \alpha(H\btheta_t + \bb).
\]</div>
<p>Then, if <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is any stationary point (like a local minimizer), we may rewrite this update rule as</p>
<div class="math notranslate nohighlight">
\[
\btheta_{t+1} - \btheta^\star = (I - \alpha H)(\btheta_t - \btheta^\star)
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the <span class="math notranslate nohighlight">\(n\times n\)</span> identity matrix. This leads us to the update rule given in closed form by</p>
<div class="math notranslate nohighlight" id="equation-gd-closed-eqn">
<span class="eqno">(11.11)<a class="headerlink" href="#equation-gd-closed-eqn" title="Permalink to this equation">#</a></span>\[
\btheta_t - \btheta^\star = (I - \alpha H)^t (\btheta_0 - \btheta^\star)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\geq 0\)</span>.</p>
<p>Choosing the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is a balancing act: We want it large enough to obtain quick convergence, but small enough to avoid oscillations like in the plots above. To find the optimal <span class="math notranslate nohighlight">\(\alpha\)</span> in our current situation, let’s suppose that <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is indeed a local minimizer with positive definite Hessian matrix <span class="math notranslate nohighlight">\(H\)</span>. Suppose we linearly order the eigenvalues of <span class="math notranslate nohighlight">\(H\)</span> as</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \lambda_1 \leq \cdots \leq \lambda_n.
\]</div>
<p>The eigenvalues of the matrix <span class="math notranslate nohighlight">\(I - \alpha H\)</span> are <span class="math notranslate nohighlight">\(1 - \alpha \lambda_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. As long as we choose the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-lr-eqn">
<span class="eqno">(11.12)<a class="headerlink" href="#equation-lr-eqn" title="Permalink to this equation">#</a></span>\[
0 &lt; \alpha \leq 1 / \lambda_n,
\]</div>
<p>these latter eigenvalues are all nonnegative with</p>
<div class="math notranslate nohighlight" id="equation-new-order-eqn">
<span class="eqno">(11.13)<a class="headerlink" href="#equation-new-order-eqn" title="Permalink to this equation">#</a></span>\[
0 \leq 1 - \alpha \lambda_n \leq \cdots \leq 1 - \alpha \lambda_1 &lt; 1.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(I-\alpha H\)</span> is symmetric, its operator norm is equal to its spectral radius, <span class="math notranslate nohighlight">\(1-\alpha \lambda_1\)</span>. In particular, from <a class="reference internal" href="#equation-gd-closed-eqn">(11.11)</a> we obtain the upper bound</p>
<div class="math notranslate nohighlight">
\[
|\btheta_t - \btheta^\star| \leq |I - \alpha H|^t |\btheta_0 - \btheta^\star | = (1-\alpha \lambda_1)^t |\btheta_0 - \btheta^\star |.
\]</div>
<p>Our choice of learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> according to <a class="reference internal" href="#equation-lr-eqn">(11.12)</a> implies <span class="math notranslate nohighlight">\(1-\alpha \lambda_1&lt;1\)</span>, and therefore this last displayed inequality shows that we have exponentially fast convergence as <span class="math notranslate nohighlight">\(t\to \infty\)</span>. However, we may speed up the convergence by choosing <span class="math notranslate nohighlight">\(\alpha\)</span> to be the maximum value in the range allowed by <a class="reference internal" href="#equation-lr-eqn">(11.12)</a>, i.e., choose it to be the reciprocal spectral radius <span class="math notranslate nohighlight">\(\alpha = 1/\lambda_n = 1 / \rho(H)\)</span>. In this case, we have</p>
<div class="math notranslate nohighlight">
\[
|\btheta_t - \btheta^\star| \leq ( 1- 1/\kappa(H))^t |\btheta_0 - \btheta^\star |
\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa(H)\)</span> is the condition number of <span class="math notranslate nohighlight">\(H\)</span>. This shows that the fastest rates of convergence guaranteed by our arguments are those for which the condition number of the Hessian matrix is near <span class="math notranslate nohighlight">\(1\)</span>. If the Hessian matrix is ill-conditioned (i.e., if the condition number is large), then the speed of convergence guaranteed by these arguments is inflated. This does <em>not</em> say that the algorithm is <em>guaranteed</em> to converge slowly—for example, we might be very lucky with our initial guess and still obtain quick convergence, even in the case of an ill-conditioned Hessian matrix.</p>
<p>Let’s summarize our discussion in a theorem:</p>
<div class="proof theorem admonition" id="quadratic-conv-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.5 </span> (Quadratic approximations of convergence rates)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> be a function of class <span class="math notranslate nohighlight">\(C^2\)</span> and <span class="math notranslate nohighlight">\(\btheta^\star\)</span> a local minimizer with positive definite Hessian matrix <span class="math notranslate nohighlight">\(H = \nabla^2 J(\btheta^\star)\)</span>. For initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span> sufficiently near <span class="math notranslate nohighlight">\(\btheta^\star\)</span> to allow a degree-<span class="math notranslate nohighlight">\(2\)</span> Taylor polynomial approximation, the gradient descent algorithm with <span class="math notranslate nohighlight">\(\alpha = 1/\rho(H)\)</span> and <span class="math notranslate nohighlight">\(\beta=0\)</span> converges to <span class="math notranslate nohighlight">\(\btheta^\star\)</span> exponentially fast, with</p>
<div class="math notranslate nohighlight">
\[
|\btheta_t - \btheta^\star| \leq ( 1- 1/\kappa(H))^t |\btheta_0 - \btheta^\star |
\]</div>
<p>for each <span class="math notranslate nohighlight">\(t\geq 0\)</span>. Here, <span class="math notranslate nohighlight">\(\rho(H)\)</span> and <span class="math notranslate nohighlight">\(\kappa(H)\)</span> are the spectral radius and condition number of <span class="math notranslate nohighlight">\(H\)</span>, respectively.</p>
</section>
</div><p>Of course, in order to obtain the exponentially quick convergence guaranteed by the theorem, one needs to place their initial guess <span class="math notranslate nohighlight">\(\btheta_0\)</span> “sufficiently close” to the minimizer. But this would require the analyst to already have some sense of where the minimizer is likely to be located! This restricts its usefulness in practice.</p>
<p>For our polynomial objective <span class="math notranslate nohighlight">\(J\)</span> given in <a class="reference internal" href="#equation-two-dim-poly-eq">(11.10)</a> above, we compute the spectral radius of the Hessian matrices at the minimizers <span class="math notranslate nohighlight">\((0,0)\)</span> and <span class="math notranslate nohighlight">\((1,1)\)</span> to be <span class="math notranslate nohighlight">\(220\)</span> in both cases. Thus, if we choose learning rate <span class="math notranslate nohighlight">\(\alpha = 1/220 \approx 0.004\)</span> and re-run the gradient descent algorithm with the same initial guesses to test <a class="reference internal" href="#quadratic-conv-thm">Theorem 11.5</a>, we get the new plots:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">4e-3</span><span class="p">,</span> <span class="mf">4e-3</span><span class="p">,</span> <span class="mf">4e-3</span><span class="p">,</span> <span class="mf">4e-3</span><span class="p">]}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;second runs of gradient descent with smaller learning rates&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c16ba564f394b725c9030412141c552b7bce2a0fb426a17aefb0ee652697f6ae.svg" src="../_images/c16ba564f394b725c9030412141c552b7bce2a0fb426a17aefb0ee652697f6ae.svg" /></figure>
</div>
</div>
<p>Just like magic, the undesirable oscillations have vanished. But we had to pay a price: Because of the smaller learning rate, we had to double the number of gradient steps from <span class="math notranslate nohighlight">\(20\)</span> to <span class="math notranslate nohighlight">\(40\)</span>.</p>
<p>Alternatively, we may dampen the oscillations and keep the original (relatively large) learning rates by adding a slight learning rate decay at <span class="math notranslate nohighlight">\(\beta = 0.05\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>

    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0.05$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;third runs of gradient descent with learning rate decay&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/959888dabf9db7eaa68fea6b4126616f3fe568ccf2a57ad81a851a7d44bc5b86.svg" src="../_images/959888dabf9db7eaa68fea6b4126616f3fe568ccf2a57ad81a851a7d44bc5b86.svg" /></figure>
</div>
</div>
<p>Here are the values of the objective function for these last runs with learning rate decay, plotted against the number of gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0.05$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/99deecd671b82f6126c3921f919484ba4d9a62ddbeb896778e70356ca1cae62d.svg" src="../_images/99deecd671b82f6126c3921f919484ba4d9a62ddbeb896778e70356ca1cae62d.svg" /></figure>
</div>
</div>
<p>Notice the initial “overshoot” in the plot in the bottom left, causing the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> to <em>increase</em> after the first gradient step. Recall also that the initial value <span class="math notranslate nohighlight">\(\btheta_0\)</span> in the bottom right plot is near the saddle point <span class="math notranslate nohighlight">\((0.5,0.5)\)</span>, causing <span class="math notranslate nohighlight">\(\nabla J(\btheta_0) \approx 0\)</span>. This accounts for the small initial changes in the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> indicated by the (nearly) horizontal stretch early in the run of the algorithm.</p>
<p>Of course, an objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}^2 \to \mathbb{R}\)</span> defined on a <span class="math notranslate nohighlight">\(2\)</span>-dimensional input space is still not a realistic example of the objective functions encountered in the real world. In two dimensions, we have the ability to plot the algorithm’s progress through <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> on contour plots, as we did multiple times above. In higher dimensions we lose this valuable visual aid. But no matter the input dimension, we may always plot the objective values against the number of gradient steps as a diagnostic plot for convergence.</p>
</section>
<section id="stochastic-gradient-descent">
<span id="sgd-sec"></span><h2><span class="section-number">11.4. </span>Stochastic gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this heading">#</a></h2>
<p>The special types of objective functions that we will see in <a class="reference internal" href="13-learning.html#learning"><span class="std std-numref">Chapter 13</span></a> are so-called <em>stochastic objective functions</em> of the form</p>
<div class="math notranslate nohighlight" id="equation-stoch-obj-eqn">
<span class="eqno">(11.14)<a class="headerlink" href="#equation-stoch-obj-eqn" title="Permalink to this equation">#</a></span>\[J(\btheta) = E_{\bx \sim p(\bx)}\big[ g(\bx;\btheta) \big] = \sum_{\mathbf{x}\in \mathbb{R}^n} g(\mathbf{x};\btheta)p(\mathbf{x}),\]</div>
<p>where <span class="math notranslate nohighlight">\(\btheta \in \mathbb{R}^k\)</span> is a <em>parameter vector</em> and <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n+k} \to \mathbb{R}\)</span> is a function. Very often, the mass function <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> will be an empirical mass function of an observed multivariate dataset</p>
<div class="math notranslate nohighlight">
\[
\bx_1,\bx_2,\ldots,\bx_m \in \mathbb{R}^n,
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m g \big(\bx_i; \btheta \big).
\]</div>
<p>Provided that the function <span class="math notranslate nohighlight">\(g\)</span> is differentiable with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-batch-eqn">
<span class="eqno">(11.15)<a class="headerlink" href="#equation-batch-eqn" title="Permalink to this equation">#</a></span>\[
\nabla_\btheta J(\btheta) = \frac{1}{m} \sum_{i=1}^m \nabla_\btheta g\big(\bx_i; \btheta \big)
\]</div>
<p>where we write <span class="math notranslate nohighlight">\(\nabla_\btheta\)</span> to emphasize that the gradient is computed with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. In this context, the gradient descent algorithm applied to the objective function <a class="reference internal" href="#equation-batch-eqn">(11.15)</a> is given a new name:</p>
<div class="proof definition admonition" id="batch-gd-def">
<p class="admonition-title"><span class="caption-number">Definition 11.5 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>batch gradient descent algorithm</em> is the gradient descent algorithm applied to a stochastic objective function of the form <a class="reference internal" href="#equation-batch-eqn">(11.15)</a>.</p>
</section>
</div><p>Let’s take a look at a simple example. Suppose that we define</p>
<div class="math notranslate nohighlight" id="equation-quadratic-eqn">
<span class="eqno">(11.16)<a class="headerlink" href="#equation-quadratic-eqn" title="Permalink to this equation">#</a></span>\[g: \bbr^4 \to \bbr, \quad g(\bx;\btheta) = |\bx - \btheta|^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\bx,\btheta\in \bbr^2\)</span>. We create a bivariate dataset by drawing a random sample of size <span class="math notranslate nohighlight">\(1{,}024\)</span> from a <span class="math notranslate nohighlight">\(\mathcal{N}_2(\boldsymbol0,I)\)</span> distribution. A scatter plot of the dataset looks like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/91f5e05d74fc91415ba70b32e63edab335a5e6c34c877f81af3bf73ace9b52e6.svg" src="../_images/91f5e05d74fc91415ba70b32e63edab335a5e6c34c877f81af3bf73ace9b52e6.svg" /></figure>
</div>
</div>
<p>Then, two runs of the batch gradient descent algorithm produce the following plots of the objective function versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">g</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># batch gradient descent parameters</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">]}</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">objectives</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c214703ed93a172da2cbc226f165a5e54de1a9e5fd2024b092c050ecbdd9df71.svg" src="../_images/c214703ed93a172da2cbc226f165a5e54de1a9e5fd2024b092c050ecbdd9df71.svg" /></figure>
</div>
</div>
<p>If we track the parameters <span class="math notranslate nohighlight">\(\btheta = (\theta_1,\theta_2)\)</span> during the runs, we get the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mf">0.05</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">:</span><span class="mf">0.05</span><span class="p">]</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">arr</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">gd_output</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">gd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, gradient steps$=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/108cdb55a144a3ce442a3cc63500504fe7854d7ac62668c8ad85d83dd45d64a6.svg" src="../_images/108cdb55a144a3ce442a3cc63500504fe7854d7ac62668c8ad85d83dd45d64a6.svg" /></figure>
</div>
</div>
<p>In both cases, notice that the algorithm is nicely converging toward the minimizer at <span class="math notranslate nohighlight">\(\btheta^\star \approx (0,0)\)</span>.</p>
<p>One of the drawbacks of the batch algorithm is that it needs the <em>entire</em> dataset in order to take just a single gradient step. This isn’t an issue for our small toy dataset of size <span class="math notranslate nohighlight">\(m=1{,}024\)</span>, but for the large datasets that you may encounter in the real world, this can be a serious hindrance to fast convergence.</p>
<p>One method for dealing with this bottleneck is to use <em>mini-batches</em> of the data to compute gradient steps. To do so, we begin by randomly partitioning the dataset into subsets <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_\ell\)</span> called <em>mini-batches</em>:</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-eqn">
<span class="eqno">(11.17)<a class="headerlink" href="#equation-mini-batch-eqn" title="Permalink to this equation">#</a></span>\[B_1 \cup B_2 \cup \cdots \cup B_p = \{\bx_1,\bx_2,\ldots,\bx_m\}.\]</div>
<p>Supposing that the <span class="math notranslate nohighlight">\(j\)</span>-th mini-batch <span class="math notranslate nohighlight">\(B_j\)</span> has size <span class="math notranslate nohighlight">\(\ell_j\)</span>, we would then expect from <a class="reference internal" href="#equation-batch-eqn">(11.15)</a> that</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-grad-eqn">
<span class="eqno">(11.18)<a class="headerlink" href="#equation-mini-batch-grad-eqn" title="Permalink to this equation">#</a></span>\[
\nabla J(\btheta) \approx \frac{1}{\ell_j} \sum_{\bx \in B_j} \nabla_\btheta g\big(\bx; \btheta\big),
\]</div>
<p>for each <span class="math notranslate nohighlight">\(j=1,2,\ldots,p\)</span>. Very often, the mini-batch sizes <span class="math notranslate nohighlight">\(\ell_1,\ell_2,\ldots,\ell_p\)</span> are chosen to be equal to a common value <span class="math notranslate nohighlight">\(\ell\)</span>, except (possibly) for one to compensate for the fact that <span class="math notranslate nohighlight">\(m\)</span> may not be evenly divisible by <span class="math notranslate nohighlight">\(\ell\)</span>. For example, if <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\ell=30\)</span>, then we would have four mini-batches, three of size <span class="math notranslate nohighlight">\(\ell=30\)</span> and the fourth of size <span class="math notranslate nohighlight">\(10\)</span>.</p>
<p>As you are about to see, the mini-batch version of the gradient descent algorithm loops over the mini-batches <a class="reference internal" href="#equation-mini-batch-eqn">(11.17)</a> and computes gradient steps as in <a class="reference internal" href="#equation-mini-batch-grad-eqn">(11.18)</a>. A single loop through <em>all</em> the mini-batches, covering the <em>entire</em> dataset, is called an <em>epoch</em>. As the vanilla version of the gradient descent algorithm takes the number of gradient steps as a parameter, the new version of the algorithm takes the number of epochs as a parameter. This new version is called the <em>stochastic gradient descent (SGD) algorithm</em>:</p>
<div class="proof algorithm admonition" id="sgd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11.4 </span> (Stochastic gradient descent with learning rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A dataset <span class="math notranslate nohighlight">\(\bx_1,\bx_2\ldots,\bx_m\in \mathbb{R}^n\)</span>, a stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m g \big(\bx_i;\btheta \big), \quad \btheta \in \mathbb{R}^k,
\]</div>
<p>where <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n+k}\to \mathbb{R}\)</span> is a differentiable function, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^k\)</span> for a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span> of <span class="math notranslate nohighlight">\(J\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\beta \in [0, 1)\)</span>, a mini-batch size <span class="math notranslate nohighlight">\(\ell\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of epochs.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<hr class="docutils" />
<p>   1. <span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span> <br>
   2. For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(N-1\)</span>, do: <br>
           3. Randomly partition the dataset into mini-batches <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_p\)</span> of size <span class="math notranslate nohighlight">\(\ell\)</span>. <br>
           4. For each mini-batch <span class="math notranslate nohighlight">\(B_j\)</span>, do: <br>
                   5. <span class="math notranslate nohighlight">\(\btheta := \btheta - \displaystyle \alpha(1-\beta)^{t+1} \frac{1}{\ell} \sum_{\bx \in B_j} \nabla_\btheta g\big(\bx; \btheta\big)\)</span> <br>
   6. Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
</section>
</div><p>Notice that the dataset is randomly partitioned into mini-batches inside each iteration of the per-epoch <code class="docutils literal notranslate"><span class="pre">for</span></code> loop; and remember that there may be one mini-batch of size <span class="math notranslate nohighlight">\(\neq \ell\)</span> if the size of the dataset <span class="math notranslate nohighlight">\(m\)</span> is not divisible by <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>It is possible to select a mini-batch size of <span class="math notranslate nohighlight">\(\ell=1\)</span>, so that the algorithm computes a gradient step per data point. Some references refer to this algorithm as just <em>stochastic gradient descent</em>. In our example <a class="reference internal" href="#equation-quadratic-eqn">(11.16)</a> from above, a step size of <span class="math notranslate nohighlight">\(\ell=1\)</span> yields the following plots of objective values versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># SGD parameters</span>
<span class="n">sgd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">],</span>
                  <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">160</span><span class="p">]}</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">sgd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">sgd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">sgd_parameters_slice</span><span class="p">,</span>
                    <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/cf40777c8fb8a7e6dc9a5399ca176746b26ca670ac71ecbb0c4aa2cb90faa0c5.svg" src="../_images/cf40777c8fb8a7e6dc9a5399ca176746b26ca670ac71ecbb0c4aa2cb90faa0c5.svg" /></figure>
</div>
</div>
<p>The plots are very noisy, especially for large numbers of gradient steps. However, a slight downward trend in objective values is detectable, indicating that the algorithm is locating the minimizer. The trace of the algorithm through parameter space is shown in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">sgd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">sgd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">sgd_parameters_slice</span><span class="p">,</span>
                    <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/3bb6cb0e9830cd294655eb29a6e457321b6b3663bda3f1083fdffb5ba69bdfef.svg" src="../_images/3bb6cb0e9830cd294655eb29a6e457321b6b3663bda3f1083fdffb5ba69bdfef.svg" /></figure>
</div>
</div>
<p>The traces are very noisy, especially in the first row with the large learning rate <span class="math notranslate nohighlight">\(\alpha=0.1\)</span>. Nevertheless, it is clear that the algorithm has found the neighborhood of the minimizer at <span class="math notranslate nohighlight">\((0,0)\)</span>. We might try to tame the noise in these plots by increasing the decay rate, but according to our implementation, that would be equivalent to simply decreasing the learning rate since none of these four runs of the algorithm completes a full epoch. Indeed, notice that the power <span class="math notranslate nohighlight">\(t\)</span> in the expression <span class="math notranslate nohighlight">\((1-\gamma)^t\)</span> in the <a class="reference internal" href="#sgd-alg">statement</a> of the algorithm counts the number of epochs.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># mini-batch gradient descent parameters</span>
<span class="n">sgd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">],</span>
                 <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
                 <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">]}</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">sgd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">sgd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">sgd_parameters_slice</span><span class="p">,</span>
                    <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)),</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">per_step_objectives</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">ell=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/d48d98ca802b31b89a4e6dd15a9ae6518e9c26c44773365e863d27e47fdb98cc.svg" src="../_images/d48d98ca802b31b89a4e6dd15a9ae6518e9c26c44773365e863d27e47fdb98cc.svg" /></figure>
</div>
</div>
<p>Mini-batch gradient descent parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()):</span>
    <span class="n">sgd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">sgd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">sgd_output</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">sgd_parameters_slice</span><span class="p">,</span>
                    <span class="n">g</span><span class="o">=</span><span class="n">g</span><span class="p">,</span>
                    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
                    <span class="n">theta</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">sgd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sgd_output</span><span class="o">.</span><span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">axis</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">ell=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $</span><span class="se">\\</span><span class="s1">beta=0$, gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axis</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/18768f639b83813669761f228c137ae96254c1f04592b2908ae254eb1e41b499.svg" src="../_images/18768f639b83813669761f228c137ae96254c1f04592b2908ae254eb1e41b499.svg" /></figure>
</div>
</div>
</section>
<section id="appendix-convex-functions">
<span id="app-conv-sec"></span><h2><span class="section-number">11.5. </span>Appendix: Convex functions<a class="headerlink" href="#appendix-convex-functions" title="Permalink to this heading">#</a></h2>
<p>To be written!!!</p>
<div class="proof definition admonition" id="convex-concave-def">
<p class="admonition-title"><span class="caption-number">Definition 11.6 </span></p>
<section class="definition-content" id="proof-content">
<p>We shall say a function <span class="math notranslate nohighlight">\(J:\bbr^n \to \bbr\)</span> is <em>convex</em> provided that</p>
<div class="math notranslate nohighlight">
\[
J\big((1-t) \ba + t\bb\big) \leq (1-t) J(\ba) + t J(\bb)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\ba,\bb\in \bbr^n\)</span> and all <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>. If “<span class="math notranslate nohighlight">\(\leq\)</span>” is replaced with “<span class="math notranslate nohighlight">\(&lt;\)</span>”, then the function is called <em>strictly convex</em>; if the inequalities are reversed, we obtain the definitions of <em>concave</em> and <em>strictly concave</em>.</p>
</section>
</div><div class="proof theorem admonition" id="main-convex-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.6 </span> (Main theorem on convex functions (single-variable version))</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(J: \bbr \to \bbr\)</span> be a twice-differentiable function. The following statements are equivalent:</p>
<ol class="arabic">
<li><p>The function <span class="math notranslate nohighlight">\(J\)</span> is convex.</p></li>
<li><p>For all numbers <span class="math notranslate nohighlight">\(a,b,c\)</span> with <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    \frac{J(b)-J(a)}{b-a} \leq \frac{J(c)-J(b)}{c-b}.
    \]</div>
</li>
<li><p>The graph of <span class="math notranslate nohighlight">\(J\)</span> lies above its tangent lines, i.e., for all <span class="math notranslate nohighlight">\(\theta,a\in \bbr\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
    J(\theta) \geq J'(a)(\theta-a) + J(a).
    \]</div>
</li>
<li><p>The second derivative is nonnegative everywhere, i.e., <span class="math notranslate nohighlight">\(J''(\theta)\geq 0\)</span> for all <span class="math notranslate nohighlight">\(\theta\in \bbr\)</span>.</p></li>
</ol>
<p>Moreover, if <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a stationary point of <span class="math notranslate nohighlight">\(J\)</span> and <span class="math notranslate nohighlight">\(J\)</span> is convex, then <span class="math notranslate nohighlight">\(\theta^\star\)</span> is a global minimizer.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We shall prove the statements as (1) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (1), and then (3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (4) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2).</p>
<p>(1) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (3): Suppose that <span class="math notranslate nohighlight">\(\theta,a\in \bbr\)</span> and note that</p>
<div class="math notranslate nohighlight" id="equation-der-almost-eq">
<span class="eqno">(11.19)<a class="headerlink" href="#equation-der-almost-eq" title="Permalink to this equation">#</a></span>\[
J'(a)(\theta-a) = \lim_{t\to 0} \frac{J\big( a+t(\theta-a) \big) - J(a)}{t}.
\]</div>
<p>Provided <span class="math notranslate nohighlight">\(t\in [0,1]\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
J\big( a+t(\theta-a) \big) = J \big( (1-t)a + t\theta\big) \leq (1-t)J(a) + tJ(\theta) = J(a) +t\big(J(\theta) - J(a)\big)
\]</div>
<p>and so</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \geq \frac{J\big( a+t(\theta-a) \big) - J(a)}{t} + J(a).
\]</div>
<p>Taking <span class="math notranslate nohighlight">\(t\to 0^+\)</span> and using <a class="reference internal" href="#equation-der-almost-eq">(11.19)</a> yields</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \geq J'(a) (\theta-a) + J(a),
\]</div>
<p>as desired.</p>
<p>(3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2): Letting <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>, by hypothesis we have</p>
<div class="math notranslate nohighlight">
\[
J(a) \geq J'(b)(a-b) + J(b) \quad \text{and} \quad J(c) \geq J'(b)(c-b) + J(b).
\]</div>
<p>Then the inequalities</p>
<div class="math notranslate nohighlight">
\[
\frac{J(b)-J(a)}{b-a} \leq J'(b) \leq \frac{J(c)-J(b)}{c-b}
\]</div>
<p>follow immediately, establishing (2).</p>
<p>(2) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (1): Suppose that the inequality</p>
<div class="math notranslate nohighlight" id="equation-inc-sec-eq">
<span class="eqno">(11.20)<a class="headerlink" href="#equation-inc-sec-eq" title="Permalink to this equation">#</a></span>\[
\frac{J(b)-J(a)}{b-a} \leq \frac{J(c)-J(b)}{c-b}
\]</div>
<p>holds for all <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>. Fixing <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(c\)</span> with <span class="math notranslate nohighlight">\(a&lt;c\)</span>, let <span class="math notranslate nohighlight">\(t\in (0,1)\)</span> and set</p>
<div class="math notranslate nohighlight">
\[
b = (1-t)a + tc.
\]</div>
<p>Then we have both</p>
<div class="math notranslate nohighlight" id="equation-ratio-t-eq">
<span class="eqno">(11.21)<a class="headerlink" href="#equation-ratio-t-eq" title="Permalink to this equation">#</a></span>\[
t = \frac{b-a}{c-a},
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-second-choice-eq">
<span class="eqno">(11.22)<a class="headerlink" href="#equation-second-choice-eq" title="Permalink to this equation">#</a></span>\[
1-t = \frac{c-b}{c-a}
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\frac{J(b)-J(a)}{t} \leq \frac{J(c)-J(b)}{1-t}
\]</div>
<p>by <a class="reference internal" href="#equation-second-choice-eq">(11.22)</a> and <a class="reference internal" href="#equation-inc-sec-eq">(11.20)</a>. We may rearrange this inequality to obtain</p>
<div class="math notranslate nohighlight">
\[
J\big((1-t)a+tc \big) =J(b) \leq (1-t)J(a) + tJ(c),
\]</div>
<p>which is what we wanted to show.</p>
<p>(3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (4): By the Mean Value Theorem, to prove that <span class="math notranslate nohighlight">\(J''(\theta)\geq 0\)</span> for all <span class="math notranslate nohighlight">\(\theta\)</span>, it will suffice to show that <span class="math notranslate nohighlight">\(J'\)</span> is an increasing function. For this, suppose given <span class="math notranslate nohighlight">\(a,b\in \bbr\)</span> with <span class="math notranslate nohighlight">\(a&lt; b\)</span>. By hypothesis, we have both</p>
<div class="math notranslate nohighlight">
\[
J(b) \geq J'(a)(b-a) + J(a) \quad \text{and} \quad J(a) \geq J'(b)(a-b) + J(b).
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
J'(a)(b-a) + J(a) \leq J(b) \leq -J'(b)(a-b) + J(a)
\]</div>
<p>and so <span class="math notranslate nohighlight">\(J'(a) \leq J'(b)\)</span>.</p>
<p>(4) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2): Let <span class="math notranslate nohighlight">\(a&lt;b&lt;c\)</span>. The Mean Value Theorem is then used twice: Once to conclude that <span class="math notranslate nohighlight">\(J'\)</span> is an increasing function, and then to show that there are numbers <span class="math notranslate nohighlight">\(u\in (a,b)\)</span> and <span class="math notranslate nohighlight">\(v\in (b,c)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
J'(u) = \frac{J(b)-J(a)}{b-a} \quad \text{and} \quad J'(v) = \frac{J(c)-J(b)}{b-c}.
\]</div>
<p>But then</p>
<div class="math notranslate nohighlight">
\[
\frac{J(b)-J(a)}{b-a} = J'(u) \leq J'(v) = \frac{J(c)-J(b)}{b-c}
\]</div>
<p>since <span class="math notranslate nohighlight">\(u&lt;v\)</span> and <span class="math notranslate nohighlight">\(J'\)</span> is increasing. Q.E.D.</p>
</div>
<p>Suppose <span class="math notranslate nohighlight">\(g:\bbr \to \bbr\)</span> is a twice-differentiable function, <span class="math notranslate nohighlight">\(s,t\)</span> are fixed distinct real numbers, and we define</p>
<div class="math notranslate nohighlight" id="equation-aux-func-eq">
<span class="eqno">(11.23)<a class="headerlink" href="#equation-aux-func-eq" title="Permalink to this equation">#</a></span>\[
h:\bbr \to \bbr, \quad h(r) = g\big(r(s-t)+t\big).
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight" id="equation-aux-tangent-eq">
<span class="eqno">(11.24)<a class="headerlink" href="#equation-aux-tangent-eq" title="Permalink to this equation">#</a></span>\[
g(s) \geq g'(t) (s-t) + g(t) \quad \Leftrightarrow \quad h(1) \geq h'(0) + h(0)
\]</div>
<p>while</p>
<div class="math notranslate nohighlight" id="equation-aux-curv-eq">
<span class="eqno">(11.25)<a class="headerlink" href="#equation-aux-curv-eq" title="Permalink to this equation">#</a></span>\[
g''(t) \geq 0 \quad \Leftrightarrow \quad h''(0) \geq 0.
\]</div>
<div class="proof theorem admonition" id="main-convex-multi-thm">
<p class="admonition-title"><span class="caption-number">Theorem 11.7 </span> (Main theorem on convex functions (multi-variable version))</p>
<section class="theorem-content" id="proof-content">
<p>Blah blah blah, to be written later.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Throughout the proof we fix <span class="math notranslate nohighlight">\(\btheta,\bv\in \bbr^n\)</span> and we consider the function</p>
<div class="math notranslate nohighlight" id="equation-aux-2-eq">
<span class="eqno">(11.26)<a class="headerlink" href="#equation-aux-2-eq" title="Permalink to this equation">#</a></span>\[
g: \bbr \to \bbr, \quad g(t) \def J\big(t \bv + \btheta \big).
\]</div>
<p>(1) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (2): Supposing that <span class="math notranslate nohighlight">\(J\)</span> is convex, the function <span class="math notranslate nohighlight">\(g\)</span> is easily seen to be convex. Thus,</p>
<div class="math notranslate nohighlight">
\[
J(\bv + \btheta) = g(1) \geq g'(0) + g(0) = \bv^\intercal \nabla J(\btheta) + J(\btheta),
\]</div>
<p>where the second equality follows from <a class="reference internal" href="#directional-der-def">Definition 11.2</a> and <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>.</p>
<p>(2) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (3): For real numbers <span class="math notranslate nohighlight">\(s,t\in \bbr\)</span>, we shall prove</p>
<div class="math notranslate nohighlight" id="equation-goal-tan-line-eq">
<span class="eqno">(11.27)<a class="headerlink" href="#equation-goal-tan-line-eq" title="Permalink to this equation">#</a></span>\[
g(s) \geq g'(t)(s-t) + g(t)
\]</div>
<p>through <a class="reference internal" href="#equation-aux-tangent-eq">(11.24)</a> using the auxiliary function <span class="math notranslate nohighlight">\(h\)</span> defined in terms of <span class="math notranslate nohighlight">\(g\)</span> via <a class="reference internal" href="#equation-aux-func-eq">(11.23)</a>. But note that</p>
<div class="math notranslate nohighlight">
\[
h(1) = J(s\bv + \btheta) \quad \text{and} \quad h(0) = J(t\bv + \btheta)
\]</div>
<p>while</p>
<div class="math notranslate nohighlight">
\[
h'(0) = (s-t) \bv^\intercal \nabla J(t\bv + \btheta)
\]</div>
<p>by <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a>. But by hypothesis, we have</p>
<div class="math notranslate nohighlight">
\[
J(s\bv + \btheta) \geq (s-t) \bv^\intercal \nabla J(t\bv + \btheta) + J(t\bv + \btheta),
\]</div>
<p>from which <a class="reference internal" href="#equation-goal-tan-line-eq">(11.27)</a> follows. So, since the graph of <span class="math notranslate nohighlight">\(g\)</span> lies above its tangent lines, we know what <span class="math notranslate nohighlight">\(g''(t)\geq 0\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and, in particular, that <span class="math notranslate nohighlight">\(g''(0)\geq 0\)</span>. However, from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a> we get the equality in</p>
<div class="math notranslate nohighlight">
\[
\bv^\intercal \nabla^2 J(\btheta) \bv = g''(0)   \geq 0,
\]</div>
<p>which shows that the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(\btheta)\)</span> is positive semidefinite since <span class="math notranslate nohighlight">\(\bv\)</span> was chosen arbitrarily.</p>
<p>(3) <span class="math notranslate nohighlight">\(\Rightarrow\)</span> (1): We need to prove that</p>
<div class="math notranslate nohighlight" id="equation-new-goal-eq">
<span class="eqno">(11.28)<a class="headerlink" href="#equation-new-goal-eq" title="Permalink to this equation">#</a></span>\[
J(t\bv + \btheta) \leq (1-t) J(\btheta) + t J (\bv + \btheta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t\in [0,1]\)</span>. But note that this is the same inequality as</p>
<div class="math notranslate nohighlight">
\[
g(t) \leq (1-t) g(0) + tg(1),
\]</div>
<p>so it will suffice to show that <span class="math notranslate nohighlight">\(g\)</span> is convex. But to do this, we shall show <span class="math notranslate nohighlight">\(g''(t)\geq 0\)</span> through <a class="reference internal" href="#equation-aux-curv-eq">(11.25)</a> and the auxiliary function <span class="math notranslate nohighlight">\(h\)</span>. However, we have</p>
<div class="math notranslate nohighlight">
\[
h''(0) = (s-t)^2\bv^\intercal \nabla^2 J(t\bv + \btheta)\bv \geq 0
\]</div>
<p>from <a class="reference internal" href="#directional-der-grad-thm">Theorem 11.1</a> and positive semidefiniteness of the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 J(t\bv + \btheta)\)</span>.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10-info-theory.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Information theory</p>
      </div>
    </a>
    <a class="right-next"
       href="12-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Probabilistic graphical models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">11.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curvature-and-derivatives-in-higher-dimensions">11.2. Curvature and derivatives in higher dimensions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">11.3. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">11.4. Stochastic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-convex-functions">11.5. Appendix: Convex functions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>