

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>9. Optimization &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"Ber": "\\mathcal{B}er", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "btheta": "\\boldsymbol\\theta", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/09-optim';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. Probabilistic models" href="10-models.html" />
    <link rel="prev" title="8. More probability theory" href="08-more-prob.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-models.html">10. Probabilistic models</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-learning.html">11. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-stats-estimators.html">12. Statistics and estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-asymptotic.html">13. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-more-samp-dist.html">14. More sampling distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/09-optim.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">9.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">9.2. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">9.3. Stochastic gradient descent</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization">
<span id="optim"></span><h1><span class="section-number">9. </span>Optimization<a class="headerlink" href="#optimization" title="Permalink to this heading">#</a></h1>
<p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section id="gradient-descent-in-one-variable">
<h2><span class="section-number">9.1. </span>Gradient descent in one variable<a class="headerlink" href="#gradient-descent-in-one-variable" title="Permalink to this heading">#</a></h2>
<p>Consider the optimization problem of locating the minimum values of the polynomial function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \theta^4 - 6\theta^3 + 11\theta^2 - 7\theta + 4.
\]</div>
<p>This function is called the <em>objective function</em> of the optimization problem. Its graph is displayed in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.distributions.multivariate_normal</span> <span class="kn">import</span> <span class="n">MultivariateNormal</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">11</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">7</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">+</span> <span class="mi">4</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/a864994db24362eefd1577fee10246ee56b1cf94afc02869b8712d981821ea9a.svg" src="../_images/a864994db24362eefd1577fee10246ee56b1cf94afc02869b8712d981821ea9a.svg" /></figure>
</div>
</div>
<p>From the graph, we see that the objective function is minimized near <span class="math notranslate nohighlight">\(\theta^\star\approx 0.5\)</span> and <span class="math notranslate nohighlight">\(2.7\)</span>; these values are called <em>(local) minimizers</em> of the objective function.</p>
<p>We shall use a simple single-variable version of the <em>gradient descent (GD) algorithm</em> to solve this optimization problem. In this context, the GD algorithm is called the <em>optimizer</em>. This algorithm depends on an initial guess for a minimizer, as well as two parameters called the <em>learning rate</em> and the <em>number of gradient steps</em>. We will state the algorithm first, and then walk through some intuition for why it works:</p>
<div class="proof algorithm admonition" id="algorithm-0">
<p class="admonition-title"><span class="caption-number">Algorithm 9.1 </span> (Single-variable gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta - \alpha J'(\theta)\)</span></p></li>
</ol>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</section>
</div><p>Beginning from an initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span> for a minimizer, the GD algorithm outputs a sequence of approximations <span class="math notranslate nohighlight">\(\theta_1,\theta_2,\ldots,\theta_N\)</span> for a minimizer. The last value <span class="math notranslate nohighlight">\(\theta_N\)</span> in the sequence is taken as the output of the algorithm; if the algorithm converges to a minimizer, then we should have <span class="math notranslate nohighlight">\(\theta_N \approx \theta^\star\)</span>.</p>
<p>The equation</p>
<div class="math notranslate nohighlight" id="equation-update-rule-eqn">
<span class="eqno">(9.1)<a class="headerlink" href="#equation-update-rule-eqn" title="Permalink to this equation">#</a></span>\[\theta_t := \theta_{t-1} - \alpha J'(\theta_{t-1})\]</div>
<p>in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop is called the <em>update rule</em>; we say that the new parameter <span class="math notranslate nohighlight">\(\theta_t\)</span> is obtained by taking a <em>gradient step</em> from <span class="math notranslate nohighlight">\(\theta_{t-1}\)</span>. The first update occurs when <span class="math notranslate nohighlight">\(t=1\)</span>, yielding</p>
<div class="math notranslate nohighlight">
\[
\theta_1 := \theta_{0} - \alpha J'(\theta_{0}).
\]</div>
<p>To understand the intuition for this rule, consider the two cases that the derivative <span class="math notranslate nohighlight">\(J'(\theta_0)\)</span> is positive or negative:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">J_prime</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-</span> <span class="mi">18</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">22</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="mi">7</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">+</span> <span class="mf">0.4</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">12.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="o">-</span><span class="mf">0.4</span><span class="p">),</span> <span class="sa">r</span><span class="s2">&quot;$J&#39;(\theta_0)&lt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J_prime</span><span class="p">(</span><span class="mf">3.3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grid</span> <span class="o">-</span> <span class="mf">3.3</span><span class="p">)</span> <span class="o">+</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">3.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">3.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">],</span> <span class="p">[</span><span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\theta_0$&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.9</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="mf">3.3</span><span class="p">),</span> <span class="sa">r</span><span class="s2">&quot;$J&#39;(\theta_0)&gt;0$&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/941c3e8fb8d351c439c9be1ea80ced48b3bb626e2bf692e36ca942fa2b9dfc7e.svg" src="../_images/941c3e8fb8d351c439c9be1ea80ced48b3bb626e2bf692e36ca942fa2b9dfc7e.svg" /></figure>
</div>
</div>
<p>In this plot, we’ve drawn the tangent lines to the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> at two initial values <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and <span class="math notranslate nohighlight">\(\theta_0=3.3\)</span>. Since the derivatives are the slopes of these tangent lines, the sign of the derivative is negative when <span class="math notranslate nohighlight">\(\theta_0=-0.4\)</span> and positive when <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span>. In the first case, we have</p>
<div class="math notranslate nohighlight" id="equation-first-update-eqn">
<span class="eqno">(9.2)<a class="headerlink" href="#equation-first-update-eqn" title="Permalink to this equation">#</a></span>\[\theta_1 = -0.4 - \alpha J'(-0.4) &gt; -0.4\]</div>
<p>since <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, while in the second case we have</p>
<div class="math notranslate nohighlight" id="equation-second-update-eqn">
<span class="eqno">(9.3)<a class="headerlink" href="#equation-second-update-eqn" title="Permalink to this equation">#</a></span>\[\theta_1 = 3.3 - \alpha J'(3.3) &lt; 3.3.\]</div>
<p>But notice that the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = -0.4\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 0.5\)</span>, and so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-first-update-eqn">(9.2)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than the initial guess <span class="math notranslate nohighlight">\(\theta_0\)</span>, provided that the (scaled) negative derivative</p>
<div class="math notranslate nohighlight" id="equation-neg-derivative-eqn">
<span class="eqno">(9.4)<a class="headerlink" href="#equation-neg-derivative-eqn" title="Permalink to this equation">#</a></span>\[-\alpha J'(\theta_0)\]</div>
<p>is not too large (in magnitude) causing the new <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” the minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>. Similarly, the nearest minimizer to <span class="math notranslate nohighlight">\(\theta_0 = 3.3\)</span> is <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>, so the new <span class="math notranslate nohighlight">\(\theta_1\)</span> computed according to <a class="reference internal" href="#equation-second-update-eqn">(9.3)</a> should be closer to <span class="math notranslate nohighlight">\(\theta^\star\)</span> than <span class="math notranslate nohighlight">\(\theta_0\)</span>, again provided that the (scaled) negative derivative <a class="reference internal" href="#equation-neg-derivative-eqn">(9.4)</a> is not too large in magnitude.</p>
<p>From these considerations, we conclude the following:</p>
<div class="proof observation admonition" id="gd-obs">
<p class="admonition-title"><span class="caption-number">Observation 9.1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>The negative derivative <span class="math notranslate nohighlight">\(-J(\theta)\)</span> always “points downhill.”</p></li>
<li><p>When the gradient descent algorithm works, it locates a minimizer by following the negative derivative “downhill.”</p></li>
</ul>
</section>
</div><p>The sense in which the negative derivative “points downhill” is made precise by our observation that it is positive if the point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> sits on a decreasing portion of the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>, and it is negative if <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span> is on an increasing portion of the graph. The role of the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is to scale down the magnitude of the negative derivative so that the gradient step in the update rule does not cause <span class="math notranslate nohighlight">\(\theta_1\)</span> to “overshoot” a nearby minimizer.</p>
<p>Let’s run the GD algorithm four times, with various settings of the parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the gradient descent function</span>
<span class="k">def</span> <span class="nf">GD</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    
    <span class="c1"># initialize lists to track objective values and thetas</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_thetas</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># begin gradient descent loop</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>

        <span class="c1"># compute objective with current theta</span>
        <span class="n">objective</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        
        <span class="c1"># compute gradients</span>
        <span class="n">objective</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="c1"># append current objective and theta to running lists</span>
        <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">running_thetas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        
        <span class="c1"># take a step and update the theta</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">g</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">theta</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">theta</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>

        <span class="c1"># zero out the gradient to prepare for the next iteration</span>
        <span class="n">theta</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="c1"># output tensors instead of lists</span>
    <span class="n">running_thetas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">running_thetas</span><span class="p">)</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">running_thetas</span><span class="p">,</span> <span class="n">running_objectives</span>

<span class="c1"># plot objective function</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>

<span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.45</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">2e-1</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/9068d1c180fa6c30c6852b35473fbf5690c86d3ec14265e10758547911ee040a.svg" src="../_images/9068d1c180fa6c30c6852b35473fbf5690c86d3ec14265e10758547911ee040a.svg" /></figure>
</div>
</div>
<p>In all four plots, the large magenta dot represents the initial point <span class="math notranslate nohighlight">\((\theta_0,J(\theta_0))\)</span>, while the smaller dots represent the points</p>
<div class="math notranslate nohighlight">
\[
(\theta_1,J(\theta_1)), (\theta_2, J(\theta_2)),\ldots, (\theta_N,J(\theta_N)),
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of gradient steps in the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop in the GD algorithm. In the first row, the algorithm appears to be converging in both cases to the nearest minimizer to the initial guesses. In the second row, the learning rate is (relatively) large, causing the first gradient steps to “overshoot” the nearest minimizers to the initial guesses. However, the algorithm still appears to converge in both cases.</p>
<p>It is possible for the GD algorithm to diverge, especially if the learning rate is too large. For example, suppose that we set the learning rate to <span class="math notranslate nohighlight">\(\alpha = 0.2\)</span> and use <span class="math notranslate nohighlight">\(\theta_0 = 3.5\)</span> as our initial guess. Then three steps of gradient descent produce the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-1</span><span class="p">,}</span>

<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">55</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/c608f4ca06b974ad5cd732225c7cf71a668fe0b44bf87e5c3a4c4e1d19e738ca.svg" src="../_images/c608f4ca06b974ad5cd732225c7cf71a668fe0b44bf87e5c3a4c4e1d19e738ca.svg" /></figure>
</div>
</div>
<p>We see already that <span class="math notranslate nohighlight">\(J(\theta_3) \approx 10^7\)</span>; in fact, we have <span class="math notranslate nohighlight">\(J(\theta_t) \to \infty\)</span> as <span class="math notranslate nohighlight">\(t\to\infty\)</span> for these particular parameters. Of course, one can often prevent divergence by simply using a smaller learning rate, but sometimes a large <em>initial</em> learning rate is desirable to help the algorithm quickly find the neighborhood of a minimizer. So, what we desire is a scheme to shrink the learning rate from (relatively) large values to (relatively) smaller ones as the algorithm runs. This scheme is called <em>learning rate decay</em> or <em>rate decay</em>.</p>
<div class="proof algorithm admonition" id="algorithm-2">
<p class="admonition-title"><span class="caption-number">Algorithm 9.2 </span> (Single-variable gradient descent with rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\theta_0\in \mathbb{R}\)</span> for a minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\theta^\star\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta := \theta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\theta := \theta - \alpha (1 - \gamma)^t J'(\theta)\)</span></p></li>
</ul>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</section>
</div><p>While there are many ways to shrink the learning rate during gradient descent, the simple version in this implementation multiplies the derivative <span class="math notranslate nohighlight">\(J'(\theta_{t-1})\)</span> by a factor <span class="math notranslate nohighlight">\((1-\gamma)^t \in (0,1]\)</span>. Setting <span class="math notranslate nohighlight">\(\gamma=0\)</span> results in <em>no</em> rate decay. In our diverging example above, setting <span class="math notranslate nohighlight">\(\gamma=0.2\)</span> results in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">2e-1</span><span class="p">,}</span>

<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_objectives</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$J(\theta)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/52d57a3cb03c9d77d0262236927749aae08f2a7bb6b0869d42fee5779bb618f4.svg" src="../_images/52d57a3cb03c9d77d0262236927749aae08f2a7bb6b0869d42fee5779bb618f4.svg" /></figure>
</div>
</div>
<p>We have carried out <span class="math notranslate nohighlight">\(N=8\)</span> gradient steps, and it appears that the algorithm has successfully located the minimizer <span class="math notranslate nohighlight">\(\theta^\star \approx 2.7\)</span>.</p>
<p>The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> and the decay rate <span class="math notranslate nohighlight">\(\gamma\)</span> are often chosen by experimentation.</p>
<div class="admonition-tip admonition">
<p class="admonition-title">Tip</p>
<p>When using the gradient descent algorithm to solve an optimization problem, try beginning with a learning and decay rate around <span class="math notranslate nohighlight">\(\alpha \approx 0.01\)</span> and <span class="math notranslate nohighlight">\(\gamma \approx 0.1\)</span>, respectively.</p>
</div>
<p>These values may be tuned by the analyst by closely monitoring the values of the objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> as the algorithm runs. This is easy in the single-variable case, since one can plot the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span>. In the multi-variable case, however, the graph of <span class="math notranslate nohighlight">\(J(\theta)\)</span> may live in many more dimensions than we can visualize, so the analyst might track the values of the objective function against the number of gradient steps. For example, with our polynomial objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> from above and</p>
<div class="math notranslate nohighlight">
\[
\theta_0 = -0.5, \quad \alpha = 0.01, \quad \gamma = 0.1,
\]</div>
<p>we would plot the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,}</span>

<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)),</span> <span class="n">running_objectives</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/7ad3e0038acaaf45d4bde65665bed908a6edf7f2dd003c4460ba86730c7109d9.svg" src="../_images/7ad3e0038acaaf45d4bde65665bed908a6edf7f2dd003c4460ba86730c7109d9.svg" /></figure>
</div>
</div>
<p>One may use this plot to decide on the total number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps; simply choose <span class="math notranslate nohighlight">\(N\)</span> large enough to reach a point where the plot “plateaus” or “levels out,” indicating that the algorithm is converging on a minimizer. Alternatively, the analyst may build an automatic stopping condition into the algorithm that halts when the magnitude between successive objective values is less than some chosen threshold, say</p>
<div class="math notranslate nohighlight">
\[
|J(\theta_t) - J(\theta_{t-1}) | &lt; \epsilon,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span> is a small number.</p>
</section>
<section id="gradient-descent-in-multiple-variables">
<h2><span class="section-number">9.2. </span>Gradient descent in multiple variables<a class="headerlink" href="#gradient-descent-in-multiple-variables" title="Permalink to this heading">#</a></h2>
<p>We began this chapter with a discussion of the single-variable gradient descent algorithm only to help build intuition. But in reality the objective functions encountered in practice are functions of multiple variables—often many, <em>many</em> variables! So, in this section, we adapt the single-variable gradient descent algorithm to apply to functions of the form</p>
<div class="math notranslate nohighlight">
\[
J:\mathbb{R}^n \to \mathbb{R},
\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is a positive integer. We will write a bold face <span class="math notranslate nohighlight">\(\btheta \in \mathbb{R}^n\)</span> to stand for the (column!) vector of input variables to the objective function.</p>
<p>The goal here, as it was in the single-variable case, is to find (local) minimizers <span class="math notranslate nohighlight">\(\btheta^\star\)</span> of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>. By definition, a vector <span class="math notranslate nohighlight">\(\btheta^\star\)</span> is a local minimizer if</p>
<div class="math notranslate nohighlight">
\[
J(\btheta^\star) \leq J(\btheta)
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\btheta\)</span> in an open neighborhood of <span class="math notranslate nohighlight">\(\btheta^\star\)</span>. The underlying intuition for the multi-variable gradient descent algorithm is the natural generalization of the intuition from the single-variable case in <a class="reference internal" href="#gd-obs">Observation 9.1</a>. Indeed, provided that the objective function <span class="math notranslate nohighlight">\(J\)</span> is differentiable, we have:</p>
<div class="proof observation admonition" id="observation-3">
<p class="admonition-title"><span class="caption-number">Observation 9.2 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>The negative gradient <span class="math notranslate nohighlight">\(-\nabla J(\btheta)\)</span> always “points downhill.”</p></li>
<li><p>When the gradient descent algorithm works, it locates a minimizer by following the negative gradient “downhill.”</p></li>
</ul>
</section>
</div><p>The claim in the first bullet point may be made precise by recalling the relationship between the gradient and directional derivatives that you learned in multi-variable calculus: Given a unit vector <span class="math notranslate nohighlight">\(\mathbf{u}\in \mathbb{R}^n\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-grad-directional-eqn">
<span class="eqno">(9.5)<a class="headerlink" href="#equation-grad-directional-eqn" title="Permalink to this equation">#</a></span>\[\nabla J(\btheta) \mathbf{u} = \lim_{h\to \infty} \frac{J(\btheta + h\mathbf{u}) - J(\btheta)}{h},\]</div>
<p>where the right-hand side is the directional derivative of <span class="math notranslate nohighlight">\(J\)</span> in the direction of <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>. The matrix product on the left-hand side (gradients are row vectors!) is equal to the product</p>
<div class="math notranslate nohighlight" id="equation-grad-dot-eqn">
<span class="eqno">(9.6)<a class="headerlink" href="#equation-grad-dot-eqn" title="Permalink to this equation">#</a></span>\[|\nabla J(\theta) |\cos{\phi},\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\in [0,\pi]\)</span> is the angle between the gradient <span class="math notranslate nohighlight">\(\nabla J(\theta)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>. This shows the directional derivative is minimized when <span class="math notranslate nohighlight">\(\phi = \pi\)</span>, and so the direction of fastest descent points in the direction of the negative gradient <span class="math notranslate nohighlight">\(-\nabla J(\theta)\)</span>.</p>
<p>Another important consequence of the equations <a class="reference internal" href="#equation-grad-directional-eqn">(9.5)</a> and <a class="reference internal" href="#equation-grad-dot-eqn">(9.6)</a> is that the gradient vector (and its negative) are normal to the level (hyper-)surfaces of <span class="math notranslate nohighlight">\(J(\btheta)\)</span> (which are also called contours). Indeed, if <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is a unit tangent vector to a level surface of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>, then the directional derivative of <span class="math notranslate nohighlight">\(J\)</span> in the direction of <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> must be <span class="math notranslate nohighlight">\(0\)</span> since <span class="math notranslate nohighlight">\(J\)</span> is (by definition) constant along its level surfaces. But then we must have <span class="math notranslate nohighlight">\(\phi = \pi/2\)</span>.</p>
<p>After these preparations, we state the algorithm:</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span class="caption-number">Algorithm 9.3 </span> (Multi-variable gradient descent with rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A differentiable objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}^n\to \mathbb{R}\)</span>, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^n\)</span> for a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of gradient steps.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta - \alpha(1-\gamma)^t \nabla J(\btheta)^T\)</span></p></li>
</ul>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ul>
</section>
</div><p>Remember, we are viewing the input vector <span class="math notranslate nohighlight">\(\btheta\)</span> as column vector and the gradient <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> as a row vector; this accounts for the transpose in the update rule. Notice that this algorithm specializes to its single-variable cousin in the case <span class="math notranslate nohighlight">\(n=1\)</span> since <span class="math notranslate nohighlight">\(J'(\theta) = \nabla J(\theta)\)</span> when <span class="math notranslate nohighlight">\(\theta \in \mathbb{R}\)</span>.</p>
<p>For an example, let’s consider the polynomial objective function</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = J(\theta_1,\theta_2) = (\theta_1^2 + 10 \theta_2^2)\big((\theta_1-1)^2 + 10(\theta_2-1)^2 \big)
\]</div>
<p>in two dimensions. This function has two minimizers</p>
<div class="math notranslate nohighlight">
\[
\btheta^\star = (0, 0), (1,1),
\]</div>
<p>as well as a “saddle point” at <span class="math notranslate nohighlight">\((0.5, 0.5)\)</span> where the gradient <span class="math notranslate nohighlight">\(\nabla J(\btheta)\)</span> vanishes. A contour plot of its level curves looks like:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the objective function</span>
<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">theta_1</span><span class="p">,</span> <span class="n">theta_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">theta</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta_1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">theta_2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">theta_1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># plot contours of objective function</span>
<span class="n">linspace_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">linspace_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.25</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">linspace_x</span><span class="p">,</span> <span class="n">linspace_y</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/9857ef3c94b2cdd10a880e4c86157e065e652eefd7d199d41b45e1fe8cae73df.svg" src="../_images/9857ef3c94b2cdd10a880e4c86157e065e652eefd7d199d41b45e1fe8cae73df.svg" /></figure>
</div>
</div>
<p>Let’s run the GD algorithm four times beginning with <em>no</em> rate decay, and track the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> plotted over the contours of <span class="math notranslate nohighlight">\(J(\btheta)\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the objective function</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">21</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">5e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/090931bd76729cc50668dfba396675e3c9b9cc59086e6a29b3778c78e18c6618.svg" src="../_images/090931bd76729cc50668dfba396675e3c9b9cc59086e6a29b3778c78e18c6618.svg" /></figure>
</div>
</div>
<p>The large magenta dots in the plots indicate the initial guesses <span class="math notranslate nohighlight">\(\btheta_0\)</span>, while the smaller dots indicate the approximations <span class="math notranslate nohighlight">\(\btheta_t\)</span> for <span class="math notranslate nohighlight">\(t&gt;0\)</span>. The algorithm appears to be converging nicely to the minimizer <span class="math notranslate nohighlight">\(\btheta^\star = (1,1)\)</span> in the upper-left plot; notice that the learning rate <span class="math notranslate nohighlight">\(\alpha = 0.005\)</span> is quite small. In the other three plots, the algorithm finds a neighborhood of a minimizer, but then oscillates back and forth and never appears to settle on the minimizer; this is due to the larger learning rate at <span class="math notranslate nohighlight">\(\alpha=0.01\)</span>.</p>
<p>We may dampen these oscillations and encourage the algorithm to converge by adding learning rate decay. Here are four plots with the same initial values and learning rates, but with <span class="math notranslate nohighlight">\(\gamma = 0.05\)</span> and <span class="math notranslate nohighlight">\(N\)</span> increased to <span class="math notranslate nohighlight">\(40\)</span> to account for the learning rate decay:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot the objective function</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">5e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0.05$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f019ec84a3905982a6e03ac8a16a86e47d70e77af9029521548a02b4e783e58a.svg" src="../_images/f019ec84a3905982a6e03ac8a16a86e47d70e77af9029521548a02b4e783e58a.svg" /></figure>
</div>
</div>
<p>Now, the learning rate <span class="math notranslate nohighlight">\(\alpha = 0.005\)</span> in the first plot appears to be much too small causing the gradient steps to shrink too fast before the algorithm converges. On the other hand, the algorithm in the other three plots appears to be nicely converging to minimizers. We have effectively “dampened out” the wild oscillations in the first four plots above.</p>
<p>Here are the values of the objective function in all four runs, plotted against the number of gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># parameters for gradient descent</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.49</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)],</span>
                 <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">41</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">5e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">]}</span>

<span class="c1"># run gradient descent and plot</span>
<span class="n">axes_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">GD</span><span class="p">(</span><span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">,</span> <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_steps&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)),</span> <span class="n">running_objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0.05$, $N=</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/f02833032a56feffe9194ea3b02db058508085f18a900021cef74b38fc8703e5.svg" src="../_images/f02833032a56feffe9194ea3b02db058508085f18a900021cef74b38fc8703e5.svg" /></figure>
</div>
</div>
<p>Notice the initial “overshoot” in the plot in the bottom left, causing the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> to <em>increase</em> after the first gradient step. Recall also that the initial value <span class="math notranslate nohighlight">\(\btheta_0\)</span> in the bottom right plot is near the saddle point <span class="math notranslate nohighlight">\((0.5,0.5)\)</span>, causing <span class="math notranslate nohighlight">\(\nabla J(\btheta_0) \approx 0\)</span>. This accounts for the small initial changes in the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> indicated by the (nearly) horizontal stretch early in the run of the algorithm.</p>
<p>Of course, an objective function <span class="math notranslate nohighlight">\(J:\mathbb{R}^2 \to \mathbb{R}\)</span> defined on a <span class="math notranslate nohighlight">\(2\)</span>-dimensional input space is still not a realistic example of the objective functions encountered in the real world. In two dimensions, we have the ability to plot the algorithm’s progress through <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> on a contour plot, as we did above. In dimensions <span class="math notranslate nohighlight">\(n\geq 4\)</span> we lose this visual aid, though one may plot input variables two at a time in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. But no matter the input dimension, we may always plot the objective values against the number of gradient steps as a diagnostic plot for convergence.</p>
</section>
<section id="stochastic-gradient-descent">
<h2><span class="section-number">9.3. </span>Stochastic gradient descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this heading">#</a></h2>
<p>The special types of objective functions that we will see in <a class="reference internal" href="11-learning.html#learning"><span class="std std-numref">Chapter 11</span></a> are so-called <em>stochastic objective functions</em> of the form</p>
<div class="math notranslate nohighlight" id="equation-stoch-obj-eqn">
<span class="eqno">(9.7)<a class="headerlink" href="#equation-stoch-obj-eqn" title="Permalink to this equation">#</a></span>\[J(\btheta) = E\big( g(\mathbf{X};\btheta) \big)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional random vector, <span class="math notranslate nohighlight">\(\btheta \in \mathbb{R}^k\)</span> is a <span class="math notranslate nohighlight">\(k\)</span>-dimensional <em>parameter vector</em>, and <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n+k} \to \mathbb{R}\)</span> is a function. In many important cases, the probability distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is discrete and does not depend on <span class="math notranslate nohighlight">\(\btheta\)</span>, so that</p>
<div class="math notranslate nohighlight" id="equation-object-expect-eqn">
<span class="eqno">(9.8)<a class="headerlink" href="#equation-object-expect-eqn" title="Permalink to this equation">#</a></span>\[J(\btheta) = \sum_{\mathbf{x}\in \mathbb{R}^n} g(\mathbf{x};\btheta)p(\mathbf{x})\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> is the mass function of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact, the mass function <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> will often be an <em>empirical</em> mass function of an observed multivariate dataset</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(m)}\in \mathbb{R}^n,
\]</div>
<p>where we use a superscript with parentheses to index the data vectors rather than a subscript. Then, from <a class="reference internal" href="#equation-object-expect-eqn">(9.8)</a> and the definition of the mass function, we get</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m g\big(\mathbf{x}^{(i)}; \btheta \big).
\]</div>
<p>Provided that the function is differentiable with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-batch-eqn">
<span class="eqno">(9.9)<a class="headerlink" href="#equation-batch-eqn" title="Permalink to this equation">#</a></span>\[\nabla_\btheta J(\btheta) = \frac{1}{m} \sum_{i=1}^m \nabla_\btheta g\big(\mathbf{x}^{(i)}; \btheta \big)\]</div>
<p>where we write <span class="math notranslate nohighlight">\(\nabla_\btheta\)</span> to emphasize that the gradient is computed with respect to the parameter vector <span class="math notranslate nohighlight">\(\btheta\)</span>. In this context, the gradient descent algorithm applied to <a class="reference internal" href="#equation-batch-eqn">(9.9)</a> is given a new name:</p>
<div class="proof definition admonition" id="definition-5">
<p class="admonition-title"><span class="caption-number">Definition 9.1 </span></p>
<section class="definition-content" id="proof-content">
<p>The <em>batch gradient descent algorithm</em> is the gradient descent algorithm applied to a stochastic objective function of the form <a class="reference internal" href="#equation-batch-eqn">(9.9)</a>.</p>
</section>
</div><p>Let’s take a look at a simple example. Suppose that we define</p>
<div class="math notranslate nohighlight" id="equation-quadratic-eqn">
<span class="eqno">(9.10)<a class="headerlink" href="#equation-quadratic-eqn" title="Permalink to this equation">#</a></span>\[g: \bbr^4 \to \bbr, \quad g(\bx,\btheta) = |\bx - \btheta|^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\bx,\btheta\in \bbr^2\)</span> and the vertical bars represent the usual Euclidean norm. We create a bivariate dataset by drawing a random sample of size <span class="math notranslate nohighlight">\(1{,}024\)</span> drawn from a <span class="math notranslate nohighlight">\(\mathcal{N}_2(\boldsymbol0,I)\)</span> distribution. A scatter plot of the dataset looks like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">covariance_matrix</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">dataset</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/09260b856d58502ebcdb5d5afdc34ef9273e2047a31eaf5258e155cd0a82859c.svg" src="../_images/09260b856d58502ebcdb5d5afdc34ef9273e2047a31eaf5258e155cd0a82859c.svg" /></figure>
</div>
</div>
<p>Then, two runs of the batch gradient descent algorithm produce the following plots of the objective function versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">tracking</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># define data loader</span>
    <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>
    
    <span class="c1"># initialize lists and a dictionary to track objectives and parameters</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># begin looping through epochs</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        
        <span class="c1"># initialize a list to track per-step objectives. this will only be used if</span>
        <span class="c1"># tracking is set to &#39;epoch&#39;</span>
        <span class="n">per_step_objectives</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># begin gradient descent loop</span>
        <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            
            <span class="c1"># compute objective with current parameters</span>
            <span class="n">objective</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

            <span class="c1"># if we are tracking per gradient step, then add objective value and parameters to the </span>
            <span class="c1"># running lists. otherwise, we are tracking per epoch, so add the objective value to</span>
            <span class="c1"># the list of per-step objectives</span>
            <span class="k">if</span> <span class="n">tracking</span> <span class="o">==</span> <span class="s1">&#39;gd_step&#39;</span><span class="p">:</span>
                <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">running_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">per_step_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        
            <span class="c1"># compute gradients    </span>
            <span class="n">objective</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># take a gradient step and update the parameters</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">g</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">parameter</span><span class="o">.</span><span class="n">grad</span>
                    <span class="n">parameter</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>
            
            <span class="c1"># zero out the gradients to prepare for the next iteration</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">parameter</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># if we hit the maximum number of gradient steps, break out of the inner `for`</span>
            <span class="c1"># loop</span>
            <span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">step_count</span> <span class="o">==</span> <span class="n">max_steps</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="c1"># if we are tracking per epoch, then add the average per-step objective to the</span>
        <span class="c1"># list of running objectives. also, add the current parameters to the list of running</span>
        <span class="c1"># parameters</span>
        <span class="k">if</span> <span class="n">tracking</span> <span class="o">==</span> <span class="s1">&#39;epoch&#39;</span><span class="p">:</span>
            <span class="n">per_step_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">per_step_objectives</span><span class="p">)</span>
            <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">per_step_objectives</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">running_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        
        <span class="c1"># if we hit the maximum number of gradient steps, break out of the outer `for`</span>
        <span class="c1"># loop</span>
        <span class="k">if</span> <span class="n">step_count</span> <span class="o">==</span> <span class="n">max_steps</span><span class="p">:</span>
            <span class="k">break</span>
            
    <span class="c1"># output tensors instead of lists</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">running_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span>

<span class="c1"># define the objective function</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Parameters must be a dictionary of tensors.&#39;</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">parameters</span><span class="p">))</span>

<span class="c1"># get grids for contour plots</span>
<span class="n">linspace</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">linspace</span><span class="p">,</span> <span class="n">linspace</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">grid_1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">grid_2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">z_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">grid</span><span class="p">:</span>
    <span class="n">z_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">}))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="n">z_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">grid_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># gradient descent parameters</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
                 <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">]}</span>
<span class="n">parameters_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">objectives_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># run gradient descent</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                                                 <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                                                 <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                                                 <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;gd_step&#39;</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                 <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                                 <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">parameters_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">)</span>
    <span class="n">objectives_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

<span class="c1"># plot the objective function</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">objectives</span> <span class="o">=</span> <span class="n">objectives_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/a88a1a300a06a16003d4a81385370519f386413c899fce9eb63b8cbb54b185b8.svg" src="../_images/a88a1a300a06a16003d4a81385370519f386413c899fce9eb63b8cbb54b185b8.svg" /></figure>
</div>
</div>
<p>If we track the parameters <span class="math notranslate nohighlight">\(\btheta = (\theta_1,\theta_2)\)</span> during the runs, we get the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">parameters_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, gradient steps$=</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/788950e2cb2f3926077c811b5737673c05c2cf343f2baf405c846f1ca1412e71.svg" src="../_images/788950e2cb2f3926077c811b5737673c05c2cf343f2baf405c846f1ca1412e71.svg" /></figure>
</div>
</div>
<p>In both cases, notice that the algorithm is nicely converging toward the minimizer at <span class="math notranslate nohighlight">\(\btheta^\star \approx (0,0)\)</span>.</p>
<p>One of the drawbacks of the batch algorithm is that it needs the <em>entire</em> dataset in order to take just a single gradient step. This isn’t an issue for our small toy dataset of size <span class="math notranslate nohighlight">\(m=1{,}024\)</span>, but for the large datasets that you may encounter in the real world, this can be a serious hindrance to fast convergence.</p>
<p>One method for dealing with this bottleneck is to use <em>mini-batches</em> of the data to compute gradient steps. To do so, we begin by randomly partitioning the dataset into subsets <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_\ell\)</span> called <em>mini-batches</em>:</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-eqn">
<span class="eqno">(9.11)<a class="headerlink" href="#equation-mini-batch-eqn" title="Permalink to this equation">#</a></span>\[B_1 \cup B_2 \cup \cdots \cup B_p = \{\bx^{(1)},\bx^{(2)},\ldots,\bx^{(m)}\}.\]</div>
<p>Supposing that the <span class="math notranslate nohighlight">\(j\)</span>-th mini-batch <span class="math notranslate nohighlight">\(B_j\)</span> has size <span class="math notranslate nohighlight">\(\ell_j\)</span>, we would then expect from <a class="reference internal" href="#equation-batch-eqn">(9.9)</a> that</p>
<div class="math notranslate nohighlight" id="equation-mini-batch-grad-eqn">
<span class="eqno">(9.12)<a class="headerlink" href="#equation-mini-batch-grad-eqn" title="Permalink to this equation">#</a></span>\[\nabla J(\btheta) \approx \frac{1}{\ell_j} \sum_{x^{(i)} \in B_j} \nabla_\btheta g\big(\bx^{(i)}; \btheta\big).\]</div>
<p>Very often, the mini-batch sizes <span class="math notranslate nohighlight">\(\ell_1,\ell_2,\ldots,\ell_p\)</span> are chosen to be equal to a common value <span class="math notranslate nohighlight">\(\ell\)</span>, except (possibly) for one to compensate for the fact that <span class="math notranslate nohighlight">\(m\)</span> may not be evenly divisible by <span class="math notranslate nohighlight">\(\ell\)</span>. For example, if <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\ell=30\)</span>, then we would have four mini-batches, three of size <span class="math notranslate nohighlight">\(\ell=30\)</span> and the fourth of size <span class="math notranslate nohighlight">\(10\)</span>.</p>
<p>As you are about to see, the mini-batch version of the GD algorithm loops over the mini-batches <a class="reference internal" href="#equation-mini-batch-eqn">(9.11)</a> and computes gradient steps as in <a class="reference internal" href="#equation-mini-batch-grad-eqn">(9.12)</a>. A single loop through <em>all</em> the mini-batches, covering the <em>entire</em> dataset, is called an <em>epoch</em>. As the vanilla version of the GD algorithm takes the number of gradient steps as a parameter, the new version of the algorithm takes the number of epochs as a parameter. This new version is called the <em>stochastic gradient descent (SGD) algorithm</em>:</p>
<div class="proof algorithm admonition" id="sgd-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 9.4 </span> (Stochastic gradient descent with rate decay)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> A dataset <span class="math notranslate nohighlight">\(\bx^{(1)},\ldots,\bx^{(m)}\in \mathbb{R}^n\)</span>, a stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = \frac{1}{m} \sum_{i=1}^m g \big(\bx^{(i)};\btheta \big), \quad \btheta \in \mathbb{R}^n,
\]</div>
<p>where <span class="math notranslate nohighlight">\(g:\mathbb{R}^{n+k}\to \mathbb{R}\)</span> is a differentiable function, an initial guess <span class="math notranslate nohighlight">\(\btheta_0\in \mathbb{R}^k\)</span> for a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span> of <span class="math notranslate nohighlight">\(J\)</span>, a learning rate <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, a decay rate <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span>, a mini-batch size <span class="math notranslate nohighlight">\(\ell\)</span>, and the number <span class="math notranslate nohighlight">\(N\)</span> of epochs.</p>
<p><strong>Output:</strong> An approximation to a minimizer <span class="math notranslate nohighlight">\(\btheta^\star\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta_0\)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\(t\)</span> from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(N\)</span>, do:</p>
<ul>
<li><p>Randomly partition the dataset into mini-batches <a class="reference internal" href="#equation-mini-batch-eqn">(9.11)</a> of size <span class="math notranslate nohighlight">\(\ell\)</span>.</p></li>
<li><p>For each mini-batch <span class="math notranslate nohighlight">\(B_j\)</span>, do:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\btheta := \btheta - \frac{\alpha(1-\gamma)^t}{\ell} \sum_{x^{(i)} \in B_j} \nabla_\btheta g\big(\bx^{(i)}; \btheta\big)^T\)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\btheta\)</span>.</p></li>
</ul>
</section>
</div><p>Notice that the dataset is randomly partitioned into mini-batches inside each iteration of the per-epoch <code class="docutils literal notranslate"><span class="pre">for</span></code> loop; and remember that there may be one mini-batch of size <span class="math notranslate nohighlight">\(\neq \ell\)</span> if the size of the dataset <span class="math notranslate nohighlight">\(m\)</span> is not divisible by <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>It is possible to select a mini-batch size of <span class="math notranslate nohighlight">\(\ell=1\)</span>, so that the algorithm computes a gradient step per data point. Some references refer to this algorithm as just <em>stochastic gradient descent</em>. In our example <a class="reference internal" href="#equation-quadratic-eqn">(9.10)</a> from above, a step size of <span class="math notranslate nohighlight">\(\ell=1\)</span> yields the following plots of objective values versus gradient steps:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># SGD parameters</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">,</span> <span class="mf">3e-2</span><span class="p">],</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">160</span><span class="p">]}</span>
<span class="n">parameters_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">objectives_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># run SGD</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                                                 <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                                                 <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                                                 <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;gd_step&#39;</span><span class="p">,</span>
                                                 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                 <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                                 <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">parameters_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">)</span>
    <span class="n">objectives_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

<span class="c1"># plot the objective function</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">objectives</span> <span class="o">=</span> <span class="n">objectives_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">fr</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/4d3a1545c959af089d1ded74c323b1a7c50226ee397312b8a363173b98f9050b.svg" src="../_images/4d3a1545c959af089d1ded74c323b1a7c50226ee397312b8a363173b98f9050b.svg" /></figure>
</div>
</div>
<p>The plots are very noisy, especially for large numbers of gradient steps. However, a slight downward trend in objective values is detectable, indicating that the algorithm is locating the minimizer. The trace of the algorithm through parameter space is shown in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">parameters_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/4cdb6d9be33dcc7e31af92471be6ef79b788e2f421b38cd2d9da53190a2003e8.svg" src="../_images/4cdb6d9be33dcc7e31af92471be6ef79b788e2f421b38cd2d9da53190a2003e8.svg" /></figure>
</div>
</div>
<p>The traces are very noisy, especially in the first row with the large learning rate <span class="math notranslate nohighlight">\(\alpha=0.1\)</span>. Nevertheless, it is clear that the algorithm has found the neighborhood of the minimizer at <span class="math notranslate nohighlight">\((0,0)\)</span>. We might try to tame the noise in these plots by increasing the decay rate, but according to our implementation, that would be equivalent to simply decreasing the learning rate since none of these four runs of the algorithm completes a full epoch. Indeed, notice that the power <span class="math notranslate nohighlight">\(t\)</span> in the expression <span class="math notranslate nohighlight">\((1-\gamma)^t\)</span> in the <a class="reference internal" href="#sgd-alg">statement</a> of the algorithm counts the number of epochs.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># mini-batch gradient descent parameters</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">gd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
               <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">],</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">]}</span>
<span class="n">parameters_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">objectives_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># run mini-batch gradient descent</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)}</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                             <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                             <span class="n">J</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                             <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;gd_step&#39;</span><span class="p">,</span>
                             <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                             <span class="o">**</span><span class="n">gd_parameters_slice</span><span class="p">)</span>
    <span class="n">parameters_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">)</span>
    <span class="n">objectives_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)</span>

<span class="c1"># plot the objective function</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">objectives</span> <span class="o">=</span> <span class="n">objectives_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objectives</span><span class="p">)),</span> <span class="n">objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$\ell=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">$, $\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/236ae557f62f1abe276e8de19d338062c9a61e16d742338949d57e238cc2ed88.svg" src="../_images/236ae557f62f1abe276e8de19d338062c9a61e16d742338949d57e238cc2ed88.svg" /></figure>
</div>
</div>
<p>Mini-batch GD parameters:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">axes_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">parameters_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span>
    <span class="n">gd_parameters_slice</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gd_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">max_steps</span> <span class="o">=</span> <span class="n">gd_parameters_slice</span><span class="p">[</span><span class="s1">&#39;max_steps&#39;</span><span class="p">]</span>
    <span class="n">total_data_points</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">max_steps</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">grid_1</span><span class="p">,</span> <span class="n">grid_2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">running_parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$\ell=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">$, $\alpha=</span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">$, $\gamma=0$, gradient steps$=</span><span class="si">{</span><span class="n">max_steps</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta_2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/ddf8eb0f6c408a6b3a72ae70cd23ebd5383c1f0b96bc1766197cf706e7e11f34.svg" src="../_images/ddf8eb0f6c408a6b3a72ae70cd23ebd5383c1f0b96bc1766197cf706e7e11f34.svg" /></figure>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="08-more-prob.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>More probability theory</p>
      </div>
    </a>
    <a class="right-next"
       href="10-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Probabilistic models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-one-variable">9.1. Gradient descent in one variable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-multiple-variables">9.2. Gradient descent in multiple variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent">9.3. Stochastic gradient descent</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>