

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>6. Connecting theory to practice: data and samples &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/theory-to-practice';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Random vectors; joint, marginal and conditional distributions; independence" href="random-vectors.html" />
    <link rel="prev" title="5. Examples of random variables" href="examples-of-rvs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="random-variables.html">4. Random variables, expected values, variances</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Connecting theory to practice: data and samples</a></li>
<li class="toctree-l1"><a class="reference internal" href="random-vectors.html">7. Random vectors; joint, marginal and conditional distributions; independence</a></li>
<li class="toctree-l1"><a class="reference internal" href="covar-correl.html">8. Covariance and correlation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples-of-random-vecs.html">9. Examples of random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="more-prob.html">10. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">11. Probabilistic models</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats-estimators.html">12. Statistics and estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="asymptotic.html">13. The asymptotic theory: the LLN and CLT</a></li>
<li class="toctree-l1"><a class="reference internal" href="more-samp-dist.html">14. More sampling distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/theory-to-practice.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Connecting theory to practice: data and samples</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-and-random-samples">6.1. Data and random samples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-models-and-empirical-distributions">6.2. Probabilistic models and empirical distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">6.3. Histograms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-density-estimation">6.4. Kernel density estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-statistics">6.5. Empirical statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qq-plots">6.6. QQ-plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#box-plots-and-violin-plots">6.7. Box plots and violin plots</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="connecting-theory-to-practice-data-and-samples">
<span id="theory-to-practice"></span><h1><span class="section-number">6. </span>Connecting theory to practice: data and samples<a class="headerlink" href="#connecting-theory-to-practice-data-and-samples" title="Permalink to this heading">#</a></h1>
<section id="data-and-random-samples">
<h2><span class="section-number">6.1. </span>Data and random samples<a class="headerlink" href="#data-and-random-samples" title="Permalink to this heading">#</a></h2>
<p>This chapter is a continuation of the ideas presented in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a> where we began exploring how the concepts that we have been studying over the past few chapters apply to <em>real-world</em> datasets. In that assignment, we learned about <em>empirical distributions</em> of datasets and associated empirical quantities like means, variances, and quantiles. We will continue making the connection between theory and practice by exploring some further methods and techniques for real-world data analysis, as well as catch a first glimpse of <em>probabilistic models</em>, which we will study in much more depth in <a class="reference internal" href="models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>.</p>
<p>Our exploration in the third programming assignment centered on the <em>Ames housing dataset</em>. In this chapter, we will explore a dataset related to Airbnbs. In particular, we have at hand a sample of listing prices (in USD) for Airbnbs in Austin, Texas, over the last 12 months:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span> 
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">600</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="n">srs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../aux-files/austin_sample.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="n">srs</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0        44.01
1        32.83
2       112.50
3        31.76
4       104.05
         ...  
3967    212.17
3968    200.25
3969    382.34
3970    199.20
3971    149.58
Name: price, Length: 3972, dtype: float64
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>This is a subset of a larger dataset obtained <a class="reference external" href="http://insideairbnb.com">here</a>. The dataset originally consisted of integer prices, but for pedagogical reasons I want the prices to  more closely resemble a continuous variable. Thus, I added random fractions of a dollar onto each price.</p>
</aside>
<p>Though the sample consists of quite a few prices (<span class="math notranslate nohighlight">\(m=3{,}972\)</span> of them, in fact), it does not represent <em>all</em> prices. In fact, there were a little over 14,500 total listings during the time period from which our sample was drawn.</p>
<p>To fit this into our theoretical framework, we conceptualize the collection of <em>all</em> 14,500 listings as a probability space <span class="math notranslate nohighlight">\(S\)</span>. (I will talk about the associated probability measure below.) Then, we conceptualize the price of a listing as a random variable</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X: S \to \mathbb{R}
\end{equation*}\]</div>
<p>which takes a listing from the population <span class="math notranslate nohighlight">\(S\)</span> as input and spits out its price:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X(\text{listing}) = \text{price}.
\end{equation*}\]</div>
<p>The actual prices in our sample are called <em>observed values</em> of the random variable <span class="math notranslate nohighlight">\(X\)</span> and, as in previous chapters, they are represented with a lowercase <span class="math notranslate nohighlight">\(x\)</span>. We would list the elements in our sample as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_1,x_2,\ldots,x_m \in \mathbb{R}
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(m=3{,}972\)</span>.</p>
<p>So far, this discussion is not all that different from ones that we have had before; in fact, I can even use the same cartoon from previous chapters to visualize the action of the random variable <span class="math notranslate nohighlight">\(X\)</span>:</p>
<a class="reference internal image-reference" href="../_images/airbnb0.svg"><img alt="../_images/airbnb0.svg" class="align-center" src="../_images/airbnb0.svg" width="90%" /></a>
<p> </p>
<p>However, what makes our current scenario different from those considered in past chapters is that we have an entire sample of prices at hand, not just a <em>single</em> price. How are we to fit <em>samples</em> and <em>datasets</em> into our theoretical framework?</p>
<p>The answer is pretty obvious, actually. After all, a collection of <span class="math notranslate nohighlight">\(m\)</span> prices like those in our sample must come from a collection of <span class="math notranslate nohighlight">\(m\)</span> listings, right? This suggests that we should simply duplicate the random variable <span class="math notranslate nohighlight">\(X\)</span> to obtain a <em>sequence</em> of random variables</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m,
\end{equation*}\]</div>
<p>where the random variable <span class="math notranslate nohighlight">\(X_i\)</span> (for <span class="math notranslate nohighlight">\(1\leq i \leq m\)</span>) spits out the price of the <span class="math notranslate nohighlight">\(i\)</span>-th listing. Something like:</p>
<a class="reference internal image-reference" href="../_images/cartesian.svg"><img alt="../_images/cartesian.svg" class="align-center" src="../_images/cartesian.svg" width="90%" /></a>
<p> </p>
<p>To make this work, technically we need to replace the sample space <span class="math notranslate nohighlight">\(S\)</span> with its <span class="math notranslate nohighlight">\(m\)</span>-fold <a href="https://en.wikipedia.org/wiki/Cartesian_product#n-ary_Cartesian_power">cartesian power</a>, but we won’t worry about these details. For us, it’s enough to understand this process at the intuitive level.</p>
<p>Now, what about probability? Remember, I called the population <span class="math notranslate nohighlight">\(S\)</span> of all listings a <em>probability</em> space, so evidently it must come equipped with a probability measure <span class="math notranslate nohighlight">\(P\)</span>. But here’s the truth:</p>
<blockquote>
<div><p>We don’t actually <em>care</em> about <span class="math notranslate nohighlight">\(P\)</span>.</p>
</div></blockquote>
<p>Indeed, <span class="math notranslate nohighlight">\(P\)</span> is a purely academic object whose only role in this business is to make the theory under the hood tick along. It’s mostly pure mathematicians like me that spend time worrying about <span class="math notranslate nohighlight">\(P\)</span>, but it is <em>never</em>, <em>ever</em> mentioned or acknowledged in real-world scenarios.</p>
<p>On the other hand, we do very much(!) care about the probability distributions of the random variables <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span>. We will be devoting a huge amount of time and effort over the rest of this course trying to figure out the distribution of this or that random variable. In the context of our Airbnb prices, the distributions of the <span class="math notranslate nohighlight">\(X_i\)</span>’s tell us the distribution of prices:</p>
<a class="reference internal image-reference" href="../_images/airbnb.svg"><img alt="../_images/airbnb.svg" class="align-center" src="../_images/airbnb.svg" width="90%" /></a>
<p> </p>
<p>But because each of the random variables <span class="math notranslate nohighlight">\(X_i\)</span> is essentially a “duplicate” of the single ‘price’ random variable <span class="math notranslate nohighlight">\(X\)</span>, they all have the <em>same</em> distribution, in the sense that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
P(X_1\in A) = P(X_2\in A) = \cdots = P(X_m\in A)
\end{equation*}\]</div>
<p>for all events <span class="math notranslate nohighlight">\(A\subset \mathbb{R}\)</span>. If we draw each of the random variables along with their distributions, we would get:</p>
<a class="reference internal image-reference" href="../_images/iid.svg"><img alt="../_images/iid.svg" class="align-center" src="../_images/iid.svg" width="100%" /></a>
<p> </p>
<p>Notice that all the distributions are the same! This leads us to one of the main definitions in this entire course:</p>
<div class="proof definition admonition" id="random-sample-defn">
<p class="admonition-title"><span class="caption-number">Definition 6.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> be a sequence of random variables, all defined on the same probability space.</p>
<ul class="simple">
<li><p>The random variables are called a <em>random sample</em> if they are <em>independent</em> and <em>identically distributed</em> (IID).</p></li>
</ul>
<p>Provided that the sequence is a random sample, an <em>observed random sample</em>, or a <em>dataset</em>, is a sequence of real numbers <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> where <span class="math notranslate nohighlight">\(x_i\)</span> is an observed value of <span class="math notranslate nohighlight">\(X_i\)</span>.</p>
</section>
</div><p>Two random variables are said to be <em>independent</em> if the probability of one of the random variables taking a particular value is not influenced or affected by the other random variable taking a particular value. This isn’t a precise definition, and it must be adapted to apply to an entire <em>sequence</em> of random variables, but it is good enough for now. (The precise definition will come in <a class="reference internal" href="random-vectors.html#independence-defn">Definition 7.11</a>.)</p>
<p>Take care to notice the difference between a <em>random sample</em> (without the modifier) and an <em>observed random sample</em>—the former is an IID sequence of random variables, while the latter is a sequence of real numbers!</p>
<p>Why have two different types of random samples? Answers:</p>
<div class="admonition-the-roles-of-random-samples admonition">
<p class="admonition-title">The roles of random samples</p>
<ul class="simple">
<li><p>Observed random samples <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> are the datasets that we work with in the real world. It is therefore obvious why we care about these.</p></li>
<li><p>We use random samples <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> when we want to reason theoretically about the observed random samples that we encounter in the real world. For example, suppose that you want to prove that some type of statistical estimator or machine learning algorithm works well for <em>any</em> dataset. Then you <em>must</em> argue using random samples consisting of IID random variables!</p></li>
</ul>
</div>
</section>
<section id="probabilistic-models-and-empirical-distributions">
<h2><span class="section-number">6.2. </span>Probabilistic models and empirical distributions<a class="headerlink" href="#probabilistic-models-and-empirical-distributions" title="Permalink to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m\)</span> is a random sample, then by definition the probability distributions of the <span class="math notranslate nohighlight">\(X_i\)</span> are all identical. This will often be written as either</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m \sim f \quad \text{or} \quad X_1,X_2,\ldots,X_m \sim F,
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the density function of the <span class="math notranslate nohighlight">\(X_i\)</span>’s and <span class="math notranslate nohighlight">\(F\)</span> is their distribution function. A particular choice of <span class="math notranslate nohighlight">\(f\)</span> or <span class="math notranslate nohighlight">\(F\)</span> is called a <em>probabilistic model</em>. Often, an analyst doesn’t choose a <em>specific</em> distribution, but rather chooses a <em>family</em> from which the model distribution is drawn. For example, an analyst might choose the model from the Gaussian family of distributions, so that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m;\mu,\sigma^2 \sim \mathcal{N}(\mu,\sigma^2).
\end{equation*}\]</div>
<p>Then, the goal is to estimate specific values of the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> that yield a best-fit model for an observed random sample <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span>. In this scenario the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are called <em>model parameters</em> (or sometimes <em>population parameters</em>).</p>
<p>As we will learn in <a class="reference internal" href="models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>, the random variables and parameters in a probabilistic model may be depicted graphically; for our normal model, we would draw the following picture in the case that <span class="math notranslate nohighlight">\(m=3\)</span>:</p>
<a class="reference internal image-reference" href="../_images/norm-model.svg"><img alt="../_images/norm-model.svg" class="align-center" src="../_images/norm-model.svg" width="50%" /></a>
<p> </p>
<p>The probabilistic models we will study in <a class="reference internal" href="models.html#prob-models"><span class="std std-numref">Chapter 11</span></a> will be more complex than this, however, with arrows running between the random variables themselves (representing “probabilistic influence”) and both <em>observed</em> and <em>latent</em> (or <em>hidden</em>) random variables.</p>
<p>What do we mean when we say that a model “fits” the data well? Essentially, we mean that the model distribution and the empirical distribution of the dataset—the latter of which you learned about in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>—are reasonably “close.” While <em>closeness</em> may actually be <em>quantified</em> in certain ways (e.g., using <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>), often times an analyst judges <em>closeness</em> by checking that various probabilistic objects of the model and empirical distributions are nearly equal, and also by checking several diagnostic plots, many of which we will study in this chapter.</p>
<p>The first method we will use to compare a model and empirical distribution is by comparing their CDFs. For this, we need to define the CDF of a dataset. Here’s the definition:</p>
<div class="proof definition admonition" id="emp-dist-defn">
<p class="admonition-title"><span class="caption-number">Definition 6.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_m \sim F\)</span> be a random sample from an unknown distribution function <span class="math notranslate nohighlight">\(F\)</span>, and let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be an <em>observed</em> random sample. Then the <em>empirical distribution</em> of the dataset is the discrete probability measure on <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> with probability mass function</p>
<div class="math notranslate nohighlight" id="equation-ecdf-eqn">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-ecdf-eqn" title="Permalink to this equation">#</a></span>\[p(x) = \frac{\text{number of data points $x_i$ that match $x$}}{m}.\]</div>
<p>The <em>empirical cumulative distribution function</em> (ECDF) of the dataset is the CDF of the empiricical disribution. It is often denoted <span class="math notranslate nohighlight">\(\widehat{F}(x)\)</span>, and it is given by the usual formula</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\widehat{F}(x) = \sum_{y\leq x} p(y) = \frac{\text{number of data points $x_i$ with $x_i \leq x$}}{m}.
\end{equation*}\]</div>
</section>
</div><p>You can think of empirical probabilities as relative frequencies. Indeed, this is essentially the <em>definition</em> described by the formula <a class="reference internal" href="#equation-ecdf-eqn">(6.1)</a>: It says that <span class="math notranslate nohighlight">\(p(x)\)</span> is the relative frequency of the number <span class="math notranslate nohighlight">\(x\)</span> in the dataset.</p>
<p>To make this concrete, let’s bring back the Airbnb prices. I have asked the computer to generate the ECDF of this dataset:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/66db9bd2ae0ade5ecd5296f0b3e423e4874e4ee2061fa4b1386d2bf5c2d6bccd.png"><img alt="../_images/66db9bd2ae0ade5ecd5296f0b3e423e4874e4ee2061fa4b1386d2bf5c2d6bccd.png" src="../_images/66db9bd2ae0ade5ecd5296f0b3e423e4874e4ee2061fa4b1386d2bf5c2d6bccd.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>Remember, the CDFs of discrete distributions are step functions. So why doesn’t <em>this</em> look like a step function? Essentially, the data points are so numerous and packed together so tightly along the <span class="math notranslate nohighlight">\(x\)</span>-axis that we can’t see the steps. But rest assured, if we were to zoom in, we’d see them.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Actually, it’s not quite right to say that the empirical variance <span class="math notranslate nohighlight">\(s^2\)</span> is the parameter estimate derived from the method of moments or maximum likelihood estimation. In fact, the latter estimates have the sample size <span class="math notranslate nohighlight">\(m\)</span> in the denominator, whereas the empirical variance <span class="math notranslate nohighlight">\(s^2\)</span> is usually defined with <span class="math notranslate nohighlight">\(m-1\)</span> in the denominator.</p>
</aside>
<p>Now, what if we thought that our Airbnb prices were well modeled by a normal distribution, so that</p>
<div class="math notranslate nohighlight">
\[
X_1,X_2,\ldots,X_{3{,}972} ; \mu,\sigma^2 \sim \mathcal{N}(\mu,\sigma^2)
\]</div>
<p>in the notation introduced above. How might we find good settings for the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>? In this case, we might choose <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> to be the <em>empirical mean</em> <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <em>empirical variance</em> <span class="math notranslate nohighlight">\(s^2\)</span>, which you learned about back in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>—and which we will review in a section below. (This is the <em>method of moments</em> for estimating model parameters which, in this special case, is also the method of <em>maximum likelihood estimation</em>.) Let’s then plot the ECDF of the dataset with the CDF of the normal model distribution superimposed:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xbar</span> <span class="o">=</span> <span class="n">srs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">srs</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">xbar</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">srs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">srs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

<span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;normal CDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xbar</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;empirical mean&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/f628657b2a5d99e41ab941c6d17a891a137f52c7060007ac50a72606135fb0c3.png"><img alt="../_images/f628657b2a5d99e41ab941c6d17a891a137f52c7060007ac50a72606135fb0c3.png" src="../_images/f628657b2a5d99e41ab941c6d17a891a137f52c7060007ac50a72606135fb0c3.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>Yikes. Those CDFs are quite different from each other, suggesting that the normal model is a bad fit.</p>
<p>Closely related to CDFs are density functions, of course. Now, <em>if</em> we assume that our Airbnb dataset is an observed random sample drawn from <em>some</em> unknown “true” model distribution, then what might the density function of that model distribution look like? As I will show you in a section below, computers are capable of estimating these types of PDFs from data using something called <em>kernel density estimation</em> (KDE). If I feed the Airbnb dataset into my computer, it returns the following sketch of the data density curve using KDE:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;estimated data PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;normal PDF&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">510</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/264a125ccb9542e64c6b08a39ed5ed27cab11245bab81ddd257ff8823c7b8f3f.png"><img alt="../_images/264a125ccb9542e64c6b08a39ed5ed27cab11245bab81ddd257ff8823c7b8f3f.png" src="../_images/264a125ccb9542e64c6b08a39ed5ed27cab11245bab81ddd257ff8823c7b8f3f.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>I have superimposed the density curve of the normal model from above for comparison. Again, this plot shows that this model is a bad fit.</p>
<p>However, the KDE estimate for the data density curve shows that the data is right-skewed (long tail to the right) and unimodal (one peak); this suggests that a log transform (which you learned about in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>) might “un-skew” the data by removing the tail. Let’s try performing a log transform, setting up a <em>new</em> normal model based on the empirical mean and variance of the transformed dataset, and then print out CDFs and a KDE:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">srs</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">srs_log</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="n">srs_log</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">srs_log</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">srs_log</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs_log</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ECDF&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;normal CDF&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;log price&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs_log</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;estimated data PDF&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;normal PDF&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;log price&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/e6125ff84a2618181bf7c3fcf03bae3be53125666dd5ed0051705edfad7aee59.png"><img alt="../_images/e6125ff84a2618181bf7c3fcf03bae3be53125666dd5ed0051705edfad7aee59.png" src="../_images/e6125ff84a2618181bf7c3fcf03bae3be53125666dd5ed0051705edfad7aee59.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>By the way, datasets whose log transforms are well modeled by normal distributions occur quite frequently. So frequently, in fact, that we refer to these datasets as <a class="reference external" href="https://en.wikipedia.org/wiki/Log-normal_distribution#">log-normally distributed</a>.</p>
</aside>
<p>That looks pretty good! Based just on these two diagnostic plots, I would feel confident in using a normal model to model (the logarithm of the) data. We just need to remember that any information we learn about the data through this model, we need to “back-transform” using the exponential function to translate to the original dataset.</p>
</section>
<section id="histograms">
<h2><span class="section-number">6.3. </span>Histograms<a class="headerlink" href="#histograms" title="Permalink to this heading">#</a></h2>
<p>In the previous section, we used a plot of the ECDF of the Airbnb prices in order to <em>visualize</em> the empirical distribution. From this plot, we were able to get a sense of the <em>shape</em> of the dataset.</p>
<p>There are a few more ways that we might attempt to visualize the empirical distribution. Indeed, since the empirical distribution of the Airbnb prices is technically discrete, we might attempt to visualize it using the probability histograms that we saw in <a class="reference internal" href="prob-spaces.html#prob-histo"><span class="std std-numref">Section 2.7</span></a> and implemented in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">thrid programming assignment</a>. However, as we saw in the previous section, the data points in our sample are so numerous and packed so closely together that such a probability histogram would be essentially useless. We would have <span class="math notranslate nohighlight">\(3{,}548\)</span> bars in our histogram, one for each unique price in our dataset!</p>
<p>But here’s an idea: Suppose we “bin together” nearby prices along the <span class="math notranslate nohighlight">\(x\)</span>-axis <em>before</em> we draw the histogram. In other words, imagine we chop up the <span class="math notranslate nohighlight">\(x\)</span>-axis into smaller subintervals called <em>bins</em>, and then group together nearby data points in these bins:</p>
<a class="reference internal image-reference" href="../_images/bins1.svg"><img alt="../_images/bins1.svg" class="align-center" src="../_images/bins1.svg" width="100%" /></a>
<p> </p>
<p>In the figure on the right, there are seven bins, labeled <span class="math notranslate nohighlight">\(B_1,B_2,\ldots,B_7\)</span>. The number of bins can either be chosen explicitly by us, or we can let the computer choose the “optimal” number of bins based on some rule of thumb. Either way, the number <span class="math notranslate nohighlight">\(k\)</span> of bins need not be <span class="math notranslate nohighlight">\(7\)</span>.</p>
<p>Now, to draw the “binned” histogram, we put a rectangle on top of each bin:</p>
<a class="reference internal image-reference" href="../_images/bins.svg"><img alt="../_images/bins.svg" class="align-center" src="../_images/bins.svg" width="70%" /></a>
<p> </p>
<p>The heights of the rectangles must satisfy <em>two</em> properties: (1) Their areas must sum to <span class="math notranslate nohighlight">\(1\)</span>, and (2) their heights must be proportional to the number of data points that fall within the bins. For example, you can see that the taller rectangles in the figure contain <em>more</em> data points in their base bins. If <span class="math notranslate nohighlight">\(k\)</span> is the total number of bins and <span class="math notranslate nohighlight">\(m\)</span> is the size of the data set, then to satisfy both these properties, we can choose the <span class="math notranslate nohighlight">\(j\)</span>-th rectangle (for <span class="math notranslate nohighlight">\(j=1,2,\ldots,k\)</span>) to have height</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\frac{\text{number of data points in $j$-th bin}}{m \times (\text{width of $j$-th bin})}.
\end{equation*}\]</div>
<p>Of course, computers are capable of plotting these types of histograms. Here is one for our Airbnb prices:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/cc621649aca6184f5f205d00b89a6715c7a6413bb2b6a787acb8ac290fb470e1.png"><img alt="../_images/cc621649aca6184f5f205d00b89a6715c7a6413bb2b6a787acb8ac290fb470e1.png" src="../_images/cc621649aca6184f5f205d00b89a6715c7a6413bb2b6a787acb8ac290fb470e1.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>Taking a moment to look at this histogram, if you then scroll up and look at the estimated data density curve in the previous section, you’ll notice that they match up pretty well. Thus, both ways of getting a sense of the shape of the data (estimated density curves and histograms) do their jobs adequately.</p>
<p>Be warned, however, that the shapes of these types of histograms are quite sensitive to the number of bins (hence also the bin width), and making the <em>wrong</em> choice for this number can sometimes hide important features of the data. Here’s a histogram of the Airbnb data with 100 bins:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7aa5cb112bedb977acd4404da737832a54330165ca62d1e6c753bcf135dfa901.png"><img alt="../_images/7aa5cb112bedb977acd4404da737832a54330165ca62d1e6c753bcf135dfa901.png" src="../_images/7aa5cb112bedb977acd4404da737832a54330165ca62d1e6c753bcf135dfa901.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>Though there are lots more bins here than our first histogram, I can still get a pretty good sense of the shape of the data. When you increase the number of bins, you tend to lose “smoothness” of the histogram.</p>
<p>At the other extreme, here’s a histogram with three bins:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/47fbaae5826b8741c408e70932fbbc4565f1a7d50b9450c04b85e49982b01ac8.png"><img alt="../_images/47fbaae5826b8741c408e70932fbbc4565f1a7d50b9450c04b85e49982b01ac8.png" src="../_images/47fbaae5826b8741c408e70932fbbc4565f1a7d50b9450c04b85e49982b01ac8.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>This last histogram is not particularly informative. I get a sense that the sample data is skewed toward the right, but that’s about it.</p>
<p>So, how do you choose the number of bins? First, you might let the computer decide for you, using the default setting. This default setting is often based off of rules of thumb that have been demonstrated to work well for some reason or another. Then, you can play with the number of bins manually, fine-tuning the number until you get a figure that you believe most accurately represents the shape of the data.</p>
</section>
<section id="kernel-density-estimation">
<h2><span class="section-number">6.4. </span>Kernel density estimation<a class="headerlink" href="#kernel-density-estimation" title="Permalink to this heading">#</a></h2>
<p>Let’s return now to an idea that we met above: Estimating a empirical distribution’s <em>density</em> curve. I mentioned that one way to accomplish this task is to use something called a <em>kernel density estimator</em> (KDE). I will now give a very brief introduction to these estimators.</p>
<p>Naturally, a kernel density estimation begins by choosing the <em>kernel</em>, which is a function that has a “bell shape,” not unlike the normal density curve. In fact, one can even <em>choose</em> the normal density curve as the kernel, and then one obtains <em>Gaussian KDE</em>. These will be the only types of kernels that we will consider.</p>
<p>Imagine for simplicity that we have three data points along the <span class="math notranslate nohighlight">\(x\)</span>-axis. The idea is then to place three kernels (i.e., normal density curves) directly over top of the data points. We then <em>sum</em> the kernels and divide by <span class="math notranslate nohighlight">\(3\)</span> (to normalize the area under the curve to <span class="math notranslate nohighlight">\(1\)</span>), obtaining a <em>kernel density estimate</em>. The width of the kernel is controlled by a parameter called <em>bandwidth</em>, denoted <span class="math notranslate nohighlight">\(h\)</span>, which coincides with the standard deviation of the normal distribution in the case of Gaussian KDE. So, large values of <span class="math notranslate nohighlight">\(h\)</span> correspond to wide kernels, and smaller values correspond to narrow kernels. Here are three examples of Gaussian KDE with different values of <span class="math notranslate nohighlight">\(h\)</span>:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">bandwidths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>

<span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">bandwidths</span><span class="p">:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bandwidths</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
    <span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

    <span class="n">y1</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">y3</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;bandwidth $h=</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y1</span> <span class="o">+</span> <span class="n">y2</span> <span class="o">+</span> <span class="n">y3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sum of kernels&#39;</span><span class="p">)</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">y1</span> <span class="o">+</span> <span class="n">y2</span> <span class="o">+</span> <span class="n">y3</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sum of kernels&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/5797553b9e495e150c18d987ece1a5018ce6d190dc617e0417e7160e7056828a.png"><img alt="../_images/5797553b9e495e150c18d987ece1a5018ce6d190dc617e0417e7160e7056828a.png" src="../_images/5797553b9e495e150c18d987ece1a5018ce6d190dc617e0417e7160e7056828a.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<p>Reading from left to right, here’s what’s going on:</p>
<ul class="simple">
<li><p>The plots in the first column display the original three data points.</p></li>
<li><p>In the second column, you see the normal density curves over top of the data points for three different values of the bandwidth <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p>In the third column, I have plotted the <em>sum</em> of the three normal density curves (divided by <span class="math notranslate nohighlight">\(3\)</span>), which are, by definition, the KDEs generated from the data set.</p></li>
<li><p>In the last column, I have plotted the KDEs on their own.</p></li>
</ul>
<p>Now, cover up the middle two columns in the figure. Which of the KDEs do you believe does the best job of conveying the <em>shape</em> of the data? I would say it’s a split decision between <span class="math notranslate nohighlight">\(h=1\)</span> and <span class="math notranslate nohighlight">\(h=0.5\)</span>.</p>
<p>You’ve seen how to generate a KDE for a small three-sample dataset. The KDE that I generated above for the Airbnb prices follows the exactly same procedure, but instead of three normal density curves, there are now 3,972 of them, one for each data point in the sample! Here are three KDEs for the Airbnb prices, with different bandwidths:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bandwidths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">bandwidths</span><span class="p">:</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">bandwidths</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">bw_method</span><span class="o">=</span><span class="n">h</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bandwidth $h=</span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/907d281a52a85c8ed1d8b9f6eb150e71ce50b7ffd0d8b31ddcbee4548bb4ded0.png"><img alt="../_images/907d281a52a85c8ed1d8b9f6eb150e71ce50b7ffd0d8b31ddcbee4548bb4ded0.png" src="../_images/907d281a52a85c8ed1d8b9f6eb150e71ce50b7ffd0d8b31ddcbee4548bb4ded0.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<p>So, we’ve seen that KDEs are supposed to serve as estimates for the density curves of datasets, but they are highly sensitive to the choice of bandwidth <span class="math notranslate nohighlight">\(h\)</span>. How do you choose <span class="math notranslate nohighlight">\(h\)</span>? Just like the ‘number of bins’ parameter for histograms, you are best off first letting the computer decide the bandwidth for you, and then manually fine-tune it (if needed) until you get a KDE that you believe best represents the data.</p>
</section>
<section id="empirical-statistics">
<h2><span class="section-number">6.5. </span>Empirical statistics<a class="headerlink" href="#empirical-statistics" title="Permalink to this heading">#</a></h2>
<p>Before we continue discussing more ways to <em>visualize</em> datasets, we need to discuss numerical summaries of datasets. This section is essentially a recapitulation of what you learned in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a> with a few new things thrown in at the end.</p>
<p>Let’s begin our discussion by returning to a general IID random sample</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
X_1,X_2,\ldots,X_m \sim F,
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(F\)</span> represents the (unknown) distribution function corresponding to a probabilistic model. Suppose that the model distribution has a mean <span class="math notranslate nohighlight">\(\mu\)</span> and a variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, so that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
E(X_i) = \mu \quad \text{and} \quad V(X_i) = \sigma^2,
\end{equation*}\]</div>
<p>for each <span class="math notranslate nohighlight">\(i=1,2,\ldots,n\)</span>. Based on an observed random sample</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_1,x_2,\ldots,x_m,
\end{equation*}\]</div>
<p>how might we estimate the unknown model parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>?</p>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 6.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be an observed random sample (i.e., a dataset). The <em>empirical mean</em> is defined to be the number</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\bar{x} = \frac{1}{m} \sum_{i=1}^m x_i,
\end{equation*}\]</div>
<p>while the <em>empirical variance</em> is defined to be the number</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
s^2 = \frac{1}{m-1} \sum_{i=1}^m (x_i - \bar{x})^2.
\end{equation*}\]</div>
<p>The <em>empirical standard deviation</em> <span class="math notranslate nohighlight">\(s\)</span> is defined, as usual, as the positive square root of the empirical variance, <span class="math notranslate nohighlight">\(s = \sqrt{s^2}\)</span>.</p>
</section>
</div><p>The empirical mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and standard deviation <span class="math notranslate nohighlight">\(s\)</span> are supposed to serve as data-based estimates for the model mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>I should mention that the empirical quantities we just defined are often called the <em>sample mean</em>, <em>sample variance</em>, and <em>sample standard deviation</em>. However, as we will see later, our empirical quantities turn out to be observed values of certain <em>estimators</em> that are also called the <em>sample mean</em>, <em>sample variance</em>, and <em>sample standard deviation</em>. Since I believe that it is important—at least at first—to distinguish between an <strong>estimate</strong> and an <strong>estimator</strong>, I have decided to refer to <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(s^2\)</span>, and <span class="math notranslate nohighlight">\(s\)</span> as <em>empirical quantities</em> rather than <em>sample quantities</em>. In later chapters, however, I will not be so careful, and will refer to <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(s^2\)</span>, and <span class="math notranslate nohighlight">\(s\)</span> as <em>sample quantities</em>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>By the way, the replacement of <span class="math notranslate nohighlight">\(1/m\)</span> with <span class="math notranslate nohighlight">\(1/(m-1)\)</span> in the empirical variance is sometimes called <a class="reference external" href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>.</p>
</aside>
<p>The definitions of <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(s^2\)</span> are surely quite natural, <em>except</em> that the empirical variance involves division by <span class="math notranslate nohighlight">\(m-1\)</span> instead of the sample size <span class="math notranslate nohighlight">\(m\)</span> like you might have expected. The reason for this is that, if we had a factor of <span class="math notranslate nohighlight">\(1/m\)</span> in <span class="math notranslate nohighlight">\(s^2\)</span> instead of <span class="math notranslate nohighlight">\(1/(m-1)\)</span>, then the value of <span class="math notranslate nohighlight">\(s^2\)</span> would <em>systematically</em> underestimate the true value <span class="math notranslate nohighlight">\(\sigma^2\)</span> over repeated sampling. This can be demonstrated empirically through computer simulation, and it can also be <em>proved</em> theoretically as we will see later when we study bias of estimators. So for now, we will just take the above definition of <span class="math notranslate nohighlight">\(s^2\)</span> on faith, postponing till a later discussion the explanation regarding <em>why</em> it’s a good estimator.</p>
<p>Though the empirical quantities <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(s^2\)</span>, and <span class="math notranslate nohighlight">\(s\)</span> all have definitions that closely mimic their counterparts <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma^2\)</span>, and <span class="math notranslate nohighlight">\(\sigma\)</span>, the definition of the <em>empirical quantiles</em> of a dataset is a bit further removed from the definition of quantiles that we learned back in <a class="reference internal" href="random-variables.html#random-variables"><span class="std std-numref">Chapter 4</span></a>. Here is the definition:</p>
<div class="proof definition admonition" id="emp-quantile-defn">
<p class="admonition-title"><span class="caption-number">Definition 6.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be an observed random sample, written in non-decreasing order:</p>
<div class="math notranslate nohighlight" id="equation-listing-eqn">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-listing-eqn" title="Permalink to this equation">#</a></span>\[x_1 \leq x_2 \leq \cdots \leq x_m.\]</div>
<p>For each <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>, the datapoint <span class="math notranslate nohighlight">\(x_i\)</span> is called the <em>empirical <span class="math notranslate nohighlight">\(q\)</span>-quantile</em> where</p>
<div class="math notranslate nohighlight" id="equation-quantile-eqn">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-quantile-eqn" title="Permalink to this equation">#</a></span>\[q  = \frac{i-1}{m-1}.\]</div>
</section>
</div><p>This definition appeared in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>, where I explained that the intuition for the formula <a class="reference internal" href="#equation-quantile-eqn">(6.3)</a> for <span class="math notranslate nohighlight">\(q\)</span> is that it is precisely the proportion of data points (excluding) <span class="math notranslate nohighlight">\(x_i\)</span> that fall to the <em>left</em> of <span class="math notranslate nohighlight">\(x_i\)</span> in the listing <a class="reference internal" href="#equation-listing-eqn">(6.2)</a>. I also explained in that assignment how one goes about computing the empirical <span class="math notranslate nohighlight">\(q\)</span>-quantile where <span class="math notranslate nohighlight">\(q\)</span> is a number (between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, inclusive) that is <em>not</em> of the form <a class="reference internal" href="#equation-quantile-eqn">(6.3)</a>: The default method in the Pandas library is linear interpolation.</p>
<p>The empirical 0.25-, 0.5-, and 0.75-quantiles are called the <em>first</em>, <em>second</em>, and <em>third quartiles</em>. For the Airbnb dataset, these are listed in the following printout on the lines labeled <span class="math notranslate nohighlight">\(25\%\)</span>, <span class="math notranslate nohighlight">\(50\%\)</span> and <span class="math notranslate nohighlight">\(75\%\)</span>:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    3972.000000
mean      154.114371
std       102.481302
min         0.820000
25%        83.240000
50%       121.555000
75%       199.460000
max       509.790000
Name: price, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Along with the empirical quartiles, you also see that this method from the Pandas library conveniently outputs the empirical mean and standard deviation, as well as the size of the dataset (the <em>count</em>) and the minimum and maximum sample values.</p>
<p>The range over which the middle 50% of a dataset sits is defined in:</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 6.5 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be an observed random sample. The <em>empirical interquartile range</em> (<em>empirical IQR</em>) is the difference</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(\text{empirical 0.75-quantile}) - (\text{empirical 0.25-quantile}).
\end{equation*}\]</div>
</section>
</div><p>So, using the outputs above, we see that the empirical IQR of the Airbnb dataset is:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iqr_airbnb</span> <span class="o">=</span> <span class="n">srs</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">-</span> <span class="n">srs</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The IQR for the Airbnb dataset is </span><span class="si">{</span><span class="n">iqr_airbnb</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The IQR for the Airbnb dataset is 116.22.
</pre></div>
</div>
</div>
</div>
<p>With the definition of <em>empirical IQR</em> in hand, we may now define <em>outliers</em>:</p>
<div class="proof definition admonition" id="definition-5">
<p class="admonition-title"><span class="caption-number">Definition 6.6 </span></p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_m\)</span> be an observed random sample. Then a data point <span class="math notranslate nohighlight">\(x_i\)</span> is called an <em>outlier</em> if it is above an upper threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_i &gt; (\text{empirical 0.75-quantile}) + 1.5\times (\text{empirical IQR}),
\end{equation*}\]</div>
<p>or if it is below a lower threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_i &lt; (\text{empirical 0.25-quantile}) - 1.5\times (\text{empirical IQR}).
\end{equation*}\]</div>
</section>
</div><p>There’s a very convenient way to <em>visually</em> summarize all these empirical statistics (along with outliers) which we will discuss in the last section of this chapter.</p>
</section>
<section id="qq-plots">
<h2><span class="section-number">6.6. </span>QQ-plots<a class="headerlink" href="#qq-plots" title="Permalink to this heading">#</a></h2>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>I should mention that there are other types of plots closely related to QQ-plots, called <em>probability plots</em> and <em>PP-plots</em>. In fact, there seems to be some disagreement as to whether what I am describing in this section actually <em>is</em> a QQ-plot. But this all seems to me to be uninteresting academic pedantry.</p>
</aside>
<p>We learned in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a> how to produce a plot of the empirical quantiles of a dataset. In this section, we will learn how to produce a plot that compares these empirical quantiles to the (theoretical) quantiles of a proposed model distribution. These new types of plots are called <em>quantile-quantile plots</em> or <em>QQ-plots</em>.</p>
<p>Though the basic idea behind a QQ-plot is quite simple, it demands that we slightly alter the definition of <em>empirical quantiles</em> given in the previous section and the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/tree/main/programming-assignments">third programming assignment</a>. Indeed, according to that definition, the minimum and maximum values in a dataset are the <span class="math notranslate nohighlight">\(0\)</span>- and <span class="math notranslate nohighlight">\(1\)</span>-quantiles, respectively. But we will run into trouble if we are going to compare these to the quantiles of theoretical model distributions which might not have <span class="math notranslate nohighlight">\(0\)</span>- and <span class="math notranslate nohighlight">\(1\)</span>-quantiles.</p>
<p>To help motivate the new definition, for convenience, let’s suppose that the points in our dataset are labeled with <span class="math notranslate nohighlight">\(y\)</span>’s instead of <span class="math notranslate nohighlight">\(x\)</span>’s. (You’ll see why this is convenient, in just a moment.) Suppose that we put our dataset in non-decreasing order,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
y_1 \leq y_2 \leq \cdots \leq y_m,
\end{equation*}\]</div>
<p>where (as usual) <span class="math notranslate nohighlight">\(m\)</span> is the size of the dataset. Then, instead of identifying quantiles through the association</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
y_i \leftrightarrow \frac{i-1}{m-1}
\end{equation*}\]</div>
<p>as we did in <a class="reference internal" href="#emp-quantile-defn">Definition 6.4</a>, we instead make the association</p>
<div class="math notranslate nohighlight" id="equation-quant-eqn">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-quant-eqn" title="Permalink to this equation">#</a></span>\[y_i \leftrightarrow \frac{i-1/2}{m},\]</div>
<p>for <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>. For a specific example, suppose that <span class="math notranslate nohighlight">\(m=5\)</span> and that all the data points are distinct. Then, if we plot our dataset along an axis along with the labels <a class="reference internal" href="#equation-quant-eqn">(6.4)</a>, we get the following picture:</p>
<a class="reference internal image-reference" href="../_images/quant.svg"><img alt="../_images/quant.svg" class="align-center" src="../_images/quant.svg" width="80%" /></a>
<p> </p>
<p>Notice that the minimum and maximum values are no longer the <span class="math notranslate nohighlight">\(0\)</span>- and <span class="math notranslate nohighlight">\(1\)</span>-quantiles, but instead the <span class="math notranslate nohighlight">\(0.1\)</span>- and <span class="math notranslate nohighlight">\(0.9\)</span>-quantiles.</p>
<p>Now, suppose that we thought that our data was well modeled by a probability distribution with continuous distribution function <span class="math notranslate nohighlight">\(F\)</span> and quantile function <span class="math notranslate nohighlight">\(Q = F^{-1}\)</span>. Then, to construct the <em>QQ-plot</em> that compares the empirical quantiles to the model quantiles, we define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x_i = Q\left( \frac{i-1/2}{m} \right)
\end{equation*}\]</div>
<p>for each <span class="math notranslate nohighlight">\(i=1,2,\ldots,m\)</span>. In particular, note that <span class="math notranslate nohighlight">\(x_i\)</span> really <em>is</em> the <span class="math notranslate nohighlight">\((i-1/2)/m\)</span>-quantile of the model distribution, according to our earlier definition of <em>quantile</em> in <a class="reference internal" href="random-variables.html#random-variables"><span class="std std-numref">Chapter 4</span></a>. The QQ-plot then consists of those points</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(x_i,y_i), \quad i=1,2,\ldots,m.
\end{equation*}\]</div>
<p>As I mentioned, QQ-plots serve as another type of diagnostic plot that allow us to compare an empirical distribution to a proposed model distribution. Let’s see how this might work with our dataset of Airbnb prices. Remember, we originally thought that the Airbnb dataset <em>itself</em> might be well modeled by a normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the empirical mean and variance. But we saw through comparison of CDFs and PDFs that this model did <em>not</em> fit the data well. The QQ-plot suggests the same:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.graphics.gofplots</span> <span class="kn">import</span> <span class="n">qqplot</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">srs</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="n">srs</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>

<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;(normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/c0f5fdaf22594c056af9dafd9a1663c4ca71dc2085d9e6f144db6e10b672615f.png"><img alt="../_images/c0f5fdaf22594c056af9dafd9a1663c4ca71dc2085d9e6f144db6e10b672615f.png" src="../_images/c0f5fdaf22594c056af9dafd9a1663c4ca71dc2085d9e6f144db6e10b672615f.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>How do we interpret this plot? The idea is that, if the model distribution fit the dataset well, then the empirical quantiles should be reasonably close to the model quantiles. One can judge this “reasonable closeness” in the QQ-plot by checking how well the scattered points fit the diagonal red line (which has a slope of <span class="math notranslate nohighlight">\(1\)</span>, or 45 degrees). So, in our Airbnb example, it is clear that the scattered points are a poor fit for the diagonal line, which suggests our dataset is <em>not</em> accurately modeled by the proposed normal distribution.</p>
<p>But what if we just chose our parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> poorly, and the dataset is accurately modeled by <em>another</em> normal distribution with different parameters? In particular, what if we thought that the dataset was accurately modeled by a <em>standard</em> normal distribution? Here’s the relevant QQ-plot, to test our hypothesis:</p>
<div class="cell tag_hide-input tag_full-width docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;(standard normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;with diagonal line&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;(standard normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;without diagonal line&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/fa9a0fd1322cc313ce3ccf46c0e4242d55cdb4606173a086f1ee27e081296828.png"><img alt="../_images/fa9a0fd1322cc313ce3ccf46c0e4242d55cdb4606173a086f1ee27e081296828.png" src="../_images/fa9a0fd1322cc313ce3ccf46c0e4242d55cdb4606173a086f1ee27e081296828.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<p>The QQ-plot on the left (with the diagonal line) shows us that we picked an <em>even worse</em> model. The horizontal axis on the QQ-plot on the right has been re-scaled so that the scattered points do not appear to fall along a (nearly) vertical line, as they do in the left-hand plot.</p>
<p>The point I want to illustrate now is that the QQ-plot on the right—without the diagonal line, and with axes on different scales—may be used to judge whether our data is fit well by <em>some</em> normal distribution. Indeed, my goal is to justify the following:</p>
<blockquote>
<div><p><strong>Observation</strong>: What we are looking for in the (standard normal) QQ-plot on the right is whether the scattered points fall along <em>some</em> straight line <span class="math notranslate nohighlight">\(y = ax + b\)</span> (with <span class="math notranslate nohighlight">\(a&gt;0\)</span>). If they do, then the data is fit well by the normal distribution <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>.</p>
</div></blockquote>
<p>This observation rests upon the fact that affine transformations of normal variables are still normal (which we saw back in <a class="reference internal" href="examples-of-rvs.html#examples"><span class="std std-numref">Chapter 5</span></a>).</p>
<p>To explain, suppose that the points in the QQ-plot fell <em>exactly</em> on a straight line, so that</p>
<div class="math notranslate nohighlight" id="equation-norm-eqn">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-norm-eqn" title="Permalink to this equation">#</a></span>\[y_i = ax_i + b, \quad i=1,2,\ldots,m,\]</div>
<p>for some <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> with <span class="math notranslate nohighlight">\(a &gt;0\)</span>. Then, let</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\phi(x) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{x^2}{2}\right)
\end{equation*}\]</div>
<p>be the density of the standard normal distribution, with associated distribution function</p>
<div class="math notranslate nohighlight">
\[
\Phi(x) = \int_{-\infty}^x \phi(t) \ \text{d} t.
\]</div>
<p>Now, if <a class="reference internal" href="#equation-norm-eqn">(6.5)</a> were true, then</p>
<div class="math notranslate nohighlight" id="equation-trans-eqn">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-trans-eqn" title="Permalink to this equation">#</a></span>\[\frac{i-1/2}{m} = \Phi\left( \frac{y_i-b}{a} \right) = \int_{-\infty}^{(y_i-b)/a} \phi(t) \ \text{d} t = \int_{-\infty}^{y_i} \frac{1}{a} \phi \left( \frac{s-b}{a}\right) \ \text{d} s,\]</div>
<p>where I made the substitution <span class="math notranslate nohighlight">\(t = (s-b)/a\)</span> in going from the first integral to the second. But notice that the transformed function</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\frac{1}{a} \phi \left( \frac{x-b}{a}\right) = \frac{1}{a\sqrt{2\pi}} \exp\left[ -\frac{1}{2} \left(\frac{x-b}{a}\right)^2\right]
\end{equation*}\]</div>
<p>is the density of <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>, and so <a class="reference internal" href="#equation-trans-eqn">(6.6)</a> shows that, provided <a class="reference internal" href="#equation-norm-eqn">(6.5)</a> is true, the data point <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\((i-1/2)/m\)</span>-quantile of <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>. Thus, the empirical quantiles match the (theoretical) model quantiles of <span class="math notranslate nohighlight">\(\mathcal{N}(b,a^2)\)</span>, which justifies the observation in the box above.</p>
<p>So, the standard normal model is a bad fit—even worse than the first normal model. But remember that the plots of the CDFs and PDFs suggest that the Airbnb dataset is <em>log-normal</em>, in the sense that its log transform is well modeled by a normal distribution. To confirm this, let’s check a QQ-plot of the log transform against standard normal quantiles:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qqplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">srs_log</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;(standard normal) model quantiles&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;empirical quantiles (w/log transform)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/70fd2e788ddca6b302e4addd187152f562d74a3a4f3dd3029410482fb9cd59e9.png"><img alt="../_images/70fd2e788ddca6b302e4addd187152f562d74a3a4f3dd3029410482fb9cd59e9.png" src="../_images/70fd2e788ddca6b302e4addd187152f562d74a3a4f3dd3029410482fb9cd59e9.png" style="width: 70%;" /></a>
</figure>
</div>
</div>
<p>Remember, we are looking for the data to fall along a straight line. In this plot, it appears that the scattered points fall along the line <span class="math notranslate nohighlight">\(y = 0.6x + 4.8\)</span>, which I estimated through visual inspection. This suggests that the best-fit normal model should be <span class="math notranslate nohighlight">\(\mathcal{N}(4.8, 0.6^2)\)</span>. Note how closely these parameter values match the empirical statistics of the log transformed data:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">srs_log</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;log price&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    3972.000000
mean        4.835900
std         0.644223
min        -0.198451
25%         4.421728
50%         4.800367
75%         5.295614
max         6.233999
Name: log price, dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="box-plots-and-violin-plots">
<h2><span class="section-number">6.7. </span>Box plots and violin plots<a class="headerlink" href="#box-plots-and-violin-plots" title="Permalink to this heading">#</a></h2>
<p>We finish the chapter with a discussion of two more methods to visualize datasets and empirical distributions. To begin, let’s consider our Airbnb data and all the empirical statistics that we described and computed in the previous sections. We may combine all this information in something called a <em>box plot</em> (or <em>box and whisker plot</em>):</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/5ebb2d2dd261b830bd16cb20671eb28c23d418818e7a7e607107f83587f781df.png"><img alt="../_images/5ebb2d2dd261b830bd16cb20671eb28c23d418818e7a7e607107f83587f781df.png" src="../_images/5ebb2d2dd261b830bd16cb20671eb28c23d418818e7a7e607107f83587f781df.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<p>The left edge of the blue box is at <span class="math notranslate nohighlight">\(x=83.24\)</span>, which is the empirical <span class="math notranslate nohighlight">\(0.25\)</span>-quantile or first quartile; its right edge is at <span class="math notranslate nohighlight">\(x=199.46\)</span>, which is the empirical 0.75-quantile or third quartile. Therefore, the <em>width</em> of the box is exactly the empirical IQR. The box thus represents where the middle 50% of the dataset lives. The vertical line through the box is at <span class="math notranslate nohighlight">\(x=121.56\)</span>, which is the empirical 0.5-quantile or the empirical median.</p>
<p>You notice that the box has “whiskers.” In general, the left whisker in a box plot either extends out to the minimum value in the dataset <em>or</em> to the threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(\text{empirical 0.25-quantile}) - 1.5\times (\text{empirical IQR})
\end{equation*}\]</div>
<p>for determining outliers, whichever is greater. In the case of our Airbnb data, the whisker extends to the minimum value at <span class="math notranslate nohighlight">\(x=0.82\)</span> (82 cents—really?).</p>
<p>Likewise, the right whisker in general either extends out to the maximum value in the dataset <em>or</em> to the upper threshold value</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
(\text{empirical 0.75-quantile}) + 1.5\times (\text{empirical IQR})
\end{equation*}\]</div>
<p>for determining outliers, whichever is smaller. In the case of our Airbnb data, the dataset <em>does</em> contain outliers in its upper tail, so the whisker extends to the threshold value, and all the dots to the right are outliers.</p>
<p>Now, what were to happen if we combined a box plot and a KDE? We’d get something like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;paper&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">bw</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;quantity&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/b0c844308b8539570b72cc9676035eb7e4e656319afa10ea75be0d3a687710f7.png"><img alt="../_images/b0c844308b8539570b72cc9676035eb7e4e656319afa10ea75be0d3a687710f7.png" src="../_images/b0c844308b8539570b72cc9676035eb7e4e656319afa10ea75be0d3a687710f7.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<p>This is a <em>violin plot</em>—the reason for the name is evident. Along the central horizontal line is a box plot—can you see it? The white dot in the box represents the empirical median, while you can see the upper and lower whiskers as well. Then, above the central horizontal line is displayed a KDE of the dataset, and its mirror image is displayed below. For comparison, here’s a picture of a KDE of the dataset all on its own:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">srs</span><span class="p">,</span> <span class="n">bw_method</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;quantity&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;probability density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/c1f6ec2c3b662046949a5902beb4d1fb7bd90d81b3ee4b10003d090268663722.png"><img alt="../_images/c1f6ec2c3b662046949a5902beb4d1fb7bd90d81b3ee4b10003d090268663722.png" src="../_images/c1f6ec2c3b662046949a5902beb4d1fb7bd90d81b3ee4b10003d090268663722.png" style="width: 100%;" /></a>
</figure>
</div>
</div>
<p>So, violin plots are tools belonging to both graphical and numerical exploratory data analysis since they combine KDEs with box plots. They have advantages over just plain box plots because they are better able to convey a sense of the <em>shape</em> of a dataset. For example, box plots cannot display multiple data modes (multiple peaks in the distribution), whereas KDEs <em>can</em>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="examples-of-rvs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Examples of random variables</p>
      </div>
    </a>
    <a class="right-next"
       href="random-vectors.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Random vectors; joint, marginal and conditional distributions; independence</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-and-random-samples">6.1. Data and random samples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-models-and-empirical-distributions">6.2. Probabilistic models and empirical distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">6.3. Histograms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-density-estimation">6.4. Kernel density estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-statistics">6.5. Empirical statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#qq-plots">6.6. QQ-plots</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#box-plots-and-violin-plots">6.7. Box plots and violin plots</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>