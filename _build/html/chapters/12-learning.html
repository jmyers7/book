

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>12. Learning &#8212; Mathematical Statistics with a View Toward Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "\\operatorname*{argmax}", "argmin": "\\operatorname*{argmin}", "MSE": "\\operatorname*{MSE}", "MAE": "\\operatorname*{MAE}", "Ber": "\\mathcal{B}er", "Beta": "\\mathcal{B}eta", "Bin": "\\mathcal{B}in", "def": "\\stackrel{\\text{def}}{=}", "balpha": "\\boldsymbol\\alpha", "bbeta": "\\boldsymbol\\beta", "bdelta": "\\boldsymbol\\delta", "bmu": "\\boldsymbol\\mu", "bfeta": "\\boldsymbol\\eta", "btheta": "\\boldsymbol\\theta", "bTheta": "\\boldsymbol\\Theta", "bSigma": "\\boldsymbol\\Sigma", "dev": "\\varepsilon", "bbr": "\\mathbb{R}", "ba": "\\mathbf{a}", "bA": "\\mathbf{A}", "bb": "\\mathbf{b}", "bc": "\\mathbf{c}", "bd": "\\mathbf{d}", "be": "\\mathbf{e}", "bg": "\\mathbf{g}", "bu": "\\mathbf{u}", "bv": "\\mathbf{v}", "bw": "\\mathbf{w}", "bx": "\\mathbf{x}", "by": "\\mathbf{y}", "bz": "\\mathbf{z}", "bS": "\\mathbf{S}", "bX": "\\mathbf{X}", "bY": "\\mathbf{Y}", "bZ": "\\mathbf{Z}", "calN": "\\mathcal{N}", "calP": "\\mathcal{P}", "Jac": "\\operatorname{Jac}", "thetaMLE": "\\widehat{\\theta}_{\\text{MLE}}", "bthetaMLE": "\\widehat{\\btheta}_{\\text{MLE}}", "thetaMAP": "\\widehat{\\theta}_{\\text{MAP}}", "bthetaMAP": "\\widehat{\\btheta}_{\\text{MAP}}", "hattheta": "\\widehat{\\theta}", "hatbtheta": "\\widehat{\\btheta}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/12-learning';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13. Statistics and general parameter estimation" href="13-stats-estimators.html" />
    <link rel="prev" title="11. Probabilistic graphical models" href="11-models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Mathematical Statistics with a View Toward Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Mathematical Statistics with a View Toward Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-preview.html">1. Preview</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-prob-spaces.html">2. Probability spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-rules-of-prob.html">3. Rules of probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-random-variables.html">4. Random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-examples-of-rvs.html">5. Examples of random variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-theory-to-practice.html">6. Connecting theory to practice: a first look at model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-random-vectors.html">7. Random vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-more-prob.html">8. More probability theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-info-theory.html">9. Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-optim.html">10. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-models.html">11. Probabilistic graphical models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-stats-estimators.html">13. Statistics and general parameter estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-asymptotic.html">14. Large sample theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-CIs.html">15. Confidence intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="16-hyp-test.html">16. Hypothesis testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-lin-reg.html">17. Linear regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="bib.html">18. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jmyers7/stats-book-materials" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/12-learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-based-learning-objectives">12.1. Likelihood-based learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-linear-regression">12.2. MLE for linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-logistic-regression">12.3. MLE for logistic regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-neural-networks">12.4. MLE for neural networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-for-gaussian-mixture-models">12.5. Expectation maximization for Gaussian mixture models</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><strong>THIS CHAPTER IS CURRENTLY UNDER CONSTRUCTION!!!</strong></p>
<section class="tex2jax_ignore mathjax_ignore" id="learning">
<span id="id1"></span><h1><span class="section-number">12. </span>Learning<a class="headerlink" href="#learning" title="Permalink to this heading">#</a></h1>
<p>In the <a class="reference internal" href="11-models.html#prob-models"><span class="std std-ref">last chapter</span></a>, we studied general probabilistic models and described several specific and important examples. These descriptions included careful identifications of the parameters of the models, but the question was left open concerning exactly <em>how</em> these parameters are chosen in practice. To cut straight to the chase:</p>
<blockquote>
<div><p>The goal is to <em>learn</em> the parameters of a model based on an observed dataset.</p>
</div></blockquote>
<p>The actual implementation of a concrete learning procedure is called a <em>learning algorithm</em> by machine learning researchers and engineers, and they will refer to <em>training</em> or <em>fitting</em> a model. Statisticians refer to learning as <em>parameter estimation</em>. But no matter what you call them, the values of the parameters that these learning procedures seek are very often solutions to some sort of optimization problem. Intuitively, we want to choose parameters to minimize the “distance” between the model probability distribution and the empirical probability distribution of the dataset:</p>
<a class="reference internal image-reference" href="../_images/prob-distance.svg"><img alt="../_images/prob-distance.svg" class="align-center" src="../_images/prob-distance.svg" width="75%" /></a>
<p> </p>
<p>How one precisely defines and measures “distance” (or “discrepancy”) is essentially a matter of choosing an objective function to minimize. Some learning algorithms we will study below are actually posed as maximization problems, but these may be reframed as minimization problems via the usual trick of replacing the objective function with its negative.</p>
<p>So, our first goal in this chapter is to describe objective functions for parameter learning. In some form or fashion, all these objectives will involve the data and model probability functions described in <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>, though these functions will be called <em>likelihood functions</em> in this chapter. Thus, all the learning algorithms in this book are <em>likelihood based</em>. For some simple models, the solutions to these optimization problems may be obtained in closed form; for others, the gradient-based optimization algorithms that we studied in <a class="reference internal" href="10-optim.html#optim"><span class="std std-numref">Chapter 10</span></a> are required to obtain approximate solutions.</p>
<p>Our focus in this chapter is using likelihood-based learning algorithms in a framework inspired by machine learning practice; in the chapters that follow, we will turn toward theoretical and statistical properties of likelihood-based parameter estimators in a more traditional statistics-based context.</p>
<section id="likelihood-based-learning-objectives">
<span id="likelihood-learning-sec"></span><h2><span class="section-number">12.1. </span>Likelihood-based learning objectives<a class="headerlink" href="#likelihood-based-learning-objectives" title="Permalink to this heading">#</a></h2>
<p>To help motivate likelihood-based learning objectives, let’s begin with a simple example. Suppose that we flip a coin <span class="math notranslate nohighlight">\(m\geq 1\)</span> times and let <span class="math notranslate nohighlight">\(x^{(i)}\)</span> be the number of heads obtained on the <span class="math notranslate nohighlight">\(i\)</span>-th toss; thus, <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is an observed value of a random variable</p>
<div class="math notranslate nohighlight">
\[
X \sim \Ber(\theta).
\]</div>
<p>This is a very simple example of a probabilistic graphical model whose underlying graph consists of only two nodes, one for the parameter <span class="math notranslate nohighlight">\(\theta\)</span> and one for the (observed) random variable <span class="math notranslate nohighlight">\(X\)</span>:</p>
<a class="reference internal image-reference" href="../_images/bern-pgm.svg"><img alt="../_images/bern-pgm.svg" class="align-center" src="../_images/bern-pgm.svg" width="25%" /></a>
<p> </p>
<p>Our observations together form a dataset of size <span class="math notranslate nohighlight">\(m\)</span>:</p>
<div class="math notranslate nohighlight">
\[
x^{(1)},\ldots,x^{(m)} \in \{0,1\}.
\]</div>
<p>Based on this dataset, our goal is to <em>learn</em> an optimal value for <span class="math notranslate nohighlight">\(\theta\)</span> that minimizes the discrepancy between the model distribution and the empirical distribution of the dataset. To do this, it will be convenient to introduce the sum</p>
<div class="math notranslate nohighlight" id="equation-sum-dep-eqn">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-sum-dep-eqn" title="Permalink to this equation">#</a></span>\[
\Sigma x \def x^{(1)} + \cdots + x^{(m)}
\]</div>
<p>which counts the total number of heads seen during the <span class="math notranslate nohighlight">\(m\)</span> flips of the coin. To make this concrete, suppose that <span class="math notranslate nohighlight">\(m=10\)</span> and <span class="math notranslate nohighlight">\(\Sigma x=7\)</span>, so that we see seven heads over ten flips. Then, intuition suggests that <span class="math notranslate nohighlight">\(\theta=0.7\)</span> would be a “more optimal” estimate for the parameter then, say, <span class="math notranslate nohighlight">\(\theta=0.1\)</span>. Indeed, if <span class="math notranslate nohighlight">\(\theta=0.1\)</span>, we would expect it highly unlikely to observe seven heads over ten flips when there is only a one-in-ten chance of seeing a head on a single flip.</p>
<p>We may confirm our hunch by actually computing probabilities. Assuming, as always, that the observations in the dataset are independent, we have</p>
<div class="math notranslate nohighlight" id="equation-likelihood-bern-eqn">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-likelihood-bern-eqn" title="Permalink to this equation">#</a></span>\[
p\big(x^{(1)},\ldots,x^{(m)};\theta\big) = \prod_{i=1}^m \theta^{x^{(i)}}(1-\theta)^{1-x^{(i)}} = \theta^x (1-\theta)^{m-\Sigma x}.
\]</div>
<p>Notice that the value of the joint mass function depends only on the sum <a class="reference internal" href="#equation-sum-dep-eqn">(12.1)</a>. If this sum is <span class="math notranslate nohighlight">\(\Sigma x=7\)</span> and we have <span class="math notranslate nohighlight">\(m=10\)</span> and <span class="math notranslate nohighlight">\(\theta=0.1\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
p\big(x^{(1)},\ldots,x^{(m)};\theta=0.1\big) = 0.1^{7} (1-0.1)^{10-7} = 7.29 \times 10^{-8}.
\]</div>
<p>On the other hand, when <span class="math notranslate nohighlight">\(\theta=0.7\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
p\big(x^{(1)},\ldots,x^{(m)};\theta=0.7\big) = 0.7^{7} (1-0.7)^{10-7} \approx 2.22 \times 10^{-3}.
\]</div>
<p>Thus, it is five orders of magnitude more likely to observe our dataset when <span class="math notranslate nohighlight">\(\theta=0.7\)</span> compared to <span class="math notranslate nohighlight">\(\theta=0.1\)</span>. In fact, the value <span class="math notranslate nohighlight">\(\theta = 0.7\)</span> is a global maximizer of <a class="reference internal" href="#equation-likelihood-bern-eqn">(12.2)</a> as a function of <span class="math notranslate nohighlight">\(\theta\)</span>, which may be verified by inspecting the graph:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.distributions.normal</span> <span class="kn">import</span> <span class="n">Normal</span>
<span class="kn">from</span> <span class="nn">torch.distributions.bernoulli</span> <span class="kn">import</span> <span class="n">Bernoulli</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline.backend_inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="k">as</span> <span class="nn">clr</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../aux-files/custom_style_light.mplstyle&#39;</span><span class="p">)</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">blue</span> <span class="o">=</span> <span class="s1">&#39;#486AFB&#39;</span>
<span class="n">magenta</span> <span class="o">=</span> <span class="s1">&#39;#FD46FC&#39;</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">7</span>

<span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="n">predict_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/ef630d6c97179bfddcd4e0b68d9b6683b8ac233c6cfcb53b102a7e6b75c420f9.svg" src="../_images/ef630d6c97179bfddcd4e0b68d9b6683b8ac233c6cfcb53b102a7e6b75c420f9.svg" /></figure>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><strong>Warning</strong>: Note that the likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> is <strong>not</strong> a probability density function over <span class="math notranslate nohighlight">\(\theta\)</span>!</p>
</aside>
<p>Note the label along the vertical axis; when the dataset is held fixed, the values of the joint mass function <a class="reference internal" href="#equation-likelihood-bern-eqn">(12.2)</a> as a function of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> are referred to as <em>likelihoods</em>. This function is called the <em>data likelihood function</em> and is denoted</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\big(\theta;x^{(1)},\ldots,x^{(m)}\big) = p\big(x^{(1)},\ldots,x^{(m)};\theta\big).
\]</div>
<p>When the dependence of the likelihood function on the dataset does not need to be explicitly indicated, we shall often simply write <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>.</p>
<p>Thus, we see that the parameter <span class="math notranslate nohighlight">\(\theta = 0.7\)</span> is a solution to the optimization problem that consists of maximizing the likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>. This is a simple example of <em>maximum likelihood estimation</em>, or <em>MLE</em>.</p>
<p>We see from <a class="reference internal" href="#equation-likelihood-bern-eqn">(12.2)</a> that the data likelihood function is a product of probabilities. Thus, if <span class="math notranslate nohighlight">\(m\)</span> is very large, the values of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> will be very small. For example, in the case that <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\Sigma x=70\)</span> (which are still quite small values), we get the following plot:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">70</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/a49b5108f3668553e0337339bf1d636e906bf9b28518bef918787b295baf8f2a.svg" src="../_images/a49b5108f3668553e0337339bf1d636e906bf9b28518bef918787b295baf8f2a.svg" /></figure>
</div>
</div>
<p>This often leads to difficulties when implementing MLE in computer algorithms due to numerical round-off. The machine is liable to round very small numbers to <span class="math notranslate nohighlight">\(0\)</span>. For this reason (and others), we often work with the (base-<span class="math notranslate nohighlight">\(e\)</span>) logarithm of the data likelihood function, denoted by</p>
<div class="math notranslate nohighlight">
\[
\ell\big(\theta; x^{(1)},\ldots,x^{(m)}\big) \def \log{\mathcal{L}\big(\theta; x^{(1)},\ldots,x^{(m)}\big)}.
\]</div>
<p>This is called the <em>data log-likelihood function</em>. As with the data likelihood function, if the dataset does not need to be explicitly mentioned, we will often write <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>.</p>
<p>MLE is the optimization problem with the data likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> as the objective function. But it is not hard to prove (see the suggested problems for this section) that the maximizers of the data likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> are the <em>same</em> as the maximizers of the data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>. For our Bernoulli model with <span class="math notranslate nohighlight">\(m=100\)</span> and <span class="math notranslate nohighlight">\(\Sigma x=70\)</span>, a visual comparison of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> is given in:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>

<span class="n">predict_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">predict_grid</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">theta$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;log-likelihood&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/e52ed9f1b6abf6db6611f6d42ee428c71e2c5b6bebb88aed874e84ac6562a13f.svg" src="../_images/e52ed9f1b6abf6db6611f6d42ee428c71e2c5b6bebb88aed874e84ac6562a13f.svg" /></figure>
</div>
</div>
<p>Notice that the values of <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> are on a much more manageable scale compared to the values of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, and that the two functions have the same global maximizer at <span class="math notranslate nohighlight">\(\theta=0.7\)</span>.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>The acronym <em>MLE</em> often serves double duty: It stands for the procedure of <em>maximum likelihood estimation</em>, but it also sometimes stands for the results of this procedure, called <em>maximum likelihood estimates</em>.</p>
</aside>
<p>Using the data log-likelihood function as the objective, we may easily compute the MLE in closed form for our Bernoulli model:</p>
<div class="proof theorem admonition" id="bern-mle-1-thm">
<p class="admonition-title"><span class="caption-number">Theorem 12.1 </span> (MLE for the Bernoulli model, part 1)</p>
<section class="theorem-content" id="proof-content">
<p>Consider the Bernoulli model described above and suppose that <span class="math notranslate nohighlight">\(0 &lt; \Sigma x &lt; m\)</span>. The (unique) global maximizer <span class="math notranslate nohighlight">\(\theta^\star\)</span> of the data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> over <span class="math notranslate nohighlight">\(\theta \in (0,1)\)</span> is given by <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span>. Thus, <span class="math notranslate nohighlight">\(\theta^\star=\Sigma x/m\)</span> is the maximum likelihood estimate.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. First note that</p>
<div class="math notranslate nohighlight" id="equation-data-log-like-bern-eqn">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-data-log-like-bern-eqn" title="Permalink to this equation">#</a></span>\[
\ell(\theta) = \Sigma x \log{\theta} + (m-\Sigma x) \log{(1-\theta)}
\]</div>
<p>from <a class="reference internal" href="#equation-likelihood-bern-eqn">(12.2)</a>.  As you well know, the maximizers of <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> over <span class="math notranslate nohighlight">\((0,1)\)</span> must occur at points where <span class="math notranslate nohighlight">\(\ell'(\theta)=0\)</span>. But</p>
<div class="math notranslate nohighlight">
\[
\ell'(\theta) = \frac{\Sigma x}{\theta} - \frac{m-\Sigma x}{1-\theta},
\]</div>
<p>and a little algebra yields the solution <span class="math notranslate nohighlight">\(\theta = \Sigma x/m\)</span> to the equation <span class="math notranslate nohighlight">\(\ell'(\theta)=0\)</span>. To confirm that <span class="math notranslate nohighlight">\(\theta = \Sigma x/m\)</span> is a global maximizer over <span class="math notranslate nohighlight">\((0,1)\)</span>, note that the second derivatives of both <span class="math notranslate nohighlight">\(\log{\theta}\)</span> and <span class="math notranslate nohighlight">\(\log{(1-\theta)}\)</span> are always negative, and hence <span class="math notranslate nohighlight">\(\ell''(\theta)&lt;0\)</span> as well since <span class="math notranslate nohighlight">\(\Sigma x\)</span> and <span class="math notranslate nohighlight">\(m-\Sigma x\)</span> are positive (this is a manifestation of <a class="reference external" href="https://en.wikipedia.org/wiki/Concave_function">concavity</a>). Thus, <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span> must be the (unique) global maximizer of <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>. Q.E.D.</p>
</div>
<p>Note that the data likelihood function</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\big(\theta; x^{(1)},\ldots,x^{(m)}\big) = p\big(x^{(1)},\ldots,x^{(m)};\theta\big)
\]</div>
<p>is exactly the <em>data probability function</em>, in the language of <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Chapter 11</span></a>. The latter is the product</p>
<div class="math notranslate nohighlight">
\[
p\big(x^{(1)},\ldots,x^{(m)};\theta\big) = \prod_{i=1}^m p\big(x^{(i)};\theta\big)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
p(x;\theta) = \theta^x (1-\theta)^{1-x}
\]</div>
<p>is the <em>model probability function</em>. As a function of <span class="math notranslate nohighlight">\(\theta\)</span> with <span class="math notranslate nohighlight">\(x\)</span> held fixed, we call</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta; x) \stackrel{\text{def}}{=} p(\theta; x)
\]</div>
<p>the <em>model likelihood function</em> and</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta;x) \stackrel{\text{def}}{=} \log{\mathcal{L}(\theta;x)}
\]</div>
<p>the <em>model log-likelihood function</em>. When the data point <span class="math notranslate nohighlight">\(x\)</span> does not need to be mentioned explicitly, we will write <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> in place of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta;x)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span>. Note that this clashes with our usage of <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> and <span class="math notranslate nohighlight">\(\ell(\theta)\)</span> to represent the <em>data</em> likelihood and log-likelihood functions when the dataset is not made explicit. You will need to rely on context to clarify which of the two types of likelihood functions (data or model) is meant when we write <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> or <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>.</p>
<p>It will be convenient to describe an optimization problem involving the <em>model</em> likelihood function that is equivalent to MLE. Here, <em>equivalence</em> means that the two optimization problems have the same solutions. This new (but equivalent!) optimization problem is appealing in part because it directly uses the empirical probability distribution of the dataset and thus more closely aligns with the intuitive scheme described in the introduction to this chapter, that the goal of parameter learning is to minimize the “distance” (or “discrepancy”) between the model distribution and the empirical distribution. This optimization problem is also useful because it opens the door for the <em>stochastic gradient descent algorithm</em> from <a class="reference internal" href="10-optim.html#optim"><span class="std std-numref">Chapter 10</span></a> when closed form solutions are not available.</p>
<p>To describe the new optimization problem, let’s consider again our Bernoulli model. Let <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> be the empirical mass function of the dataset</p>
<div class="math notranslate nohighlight">
\[
x^{(1)},\ldots,x^{(m)} \in \{0,1\}.
\]</div>
<p>Thus, in general we have</p>
<div class="math notranslate nohighlight">
\[
\hat{p}(x) = \frac{\text{number of data points $x^{(i)}$ that match $x$}}{m}
\]</div>
<p>for all <span class="math notranslate nohighlight">\(x\in \bbr\)</span>, but for our particular Bernoulli model, this simplifies to</p>
<div class="math notranslate nohighlight">
\[
\hat{p}(0) = \frac{m-\Sigma x}{m} \quad \text{and} \quad \hat{p}(1) = \frac{\Sigma x}{m},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma x=x^{(1)} + \cdots + x^{(m)}\)</span>. Letting <span class="math notranslate nohighlight">\(\widehat{X}\)</span> be a Bernoulli random variable with <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> as its mass function, we consider the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) \stackrel{\text{def}}{=} E \big( \ell\big(\theta; \widehat{X} \big) \big),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> is the model log-likelihood function. Note that</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = \ell(\theta;1) \hat{p}(1)+ \ell(\theta; 0) \hat{p}(0) = \frac{1}{m} \left[ \Sigma x \log{\theta}  + (m-\Sigma x)\log{(1-\theta)} \right],
\]</div>
<p>and so by comparison with <a class="reference internal" href="#equation-data-log-like-bern-eqn">(12.3)</a> we see that the stochastic objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span> differs from the data log-likelihood function <span class="math notranslate nohighlight">\(\ell\big( \theta; x^{(1)},\ldots,x^{(m)}\big)\)</span> only by a constant factor of <span class="math notranslate nohighlight">\(1/m\)</span>. Therefore, MLE is equivalent to the optimization problem with <span class="math notranslate nohighlight">\(J(\theta)\)</span> as an objective function, where <em>equivalence</em> means that the two problems have the same solutions. Thus:</p>
<div class="proof theorem admonition" id="bern-mle-2-thm">
<p class="admonition-title"><span class="caption-number">Theorem 12.2 </span> (MLE for the Bernoulli model, part 2)</p>
<section class="theorem-content" id="proof-content">
<p>Consider the Bernoulli model described above, suppose that <span class="math notranslate nohighlight">\(0 &lt; \Sigma x &lt; m\)</span>, and let <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> be the empirical mass function of a dataset. The (unique) maximum likelihood estimate <span class="math notranslate nohighlight">\(\theta^\star = \Sigma x/m\)</span> is the global maximizer for the optimization problem with the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
J(\theta) = E \big( \ell\big(\theta; \widehat{X} \big) \big),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell(\theta;x)\)</span> is the model log-likelihood function and <span class="math notranslate nohighlight">\(\widehat{X} \sim \hat{p}(x)\)</span>.</p>
</section>
</div><p>Then, by combining <a class="reference internal" href="#bern-mle-1-thm">Theorem 12.1</a> and <a class="reference internal" href="#bern-mle-2-thm">Theorem 12.2</a>, we conclude:</p>
<div class="proof theorem admonition" id="three-mle-thm">
<p class="admonition-title"><span class="caption-number">Theorem 12.3 </span> (Three ways to obtain an MLE)</p>
<section class="theorem-content" id="proof-content">
<p>The maximum likelihoood estimate for the Bernoulli model may be obtained by maximizing the data likelihood function <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span>, the data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\theta)\)</span>, or the stochastic objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span>.</p>
</section>
</div><p>Our description of likelihood-based learning methods thus far has focused on the simple Bernoulli model. However, from this specific and simple case I am hoping that you can see an outline of a general method able to fit <em>any</em> probabilistic model to data. In particular, <em>all</em> the models we studied in the <a class="reference internal" href="11-models.html#prob-models"><span class="std std-ref">previous chapter</span></a> have model and data likelihood functions, and thus these general methods apply to them.</p>
<p>To be more precise, we need to distinguish between two types of models. The <a class="reference internal" href="11-models.html#lin-reg-sec"><span class="std std-ref">linear regression</span></a>, <a class="reference internal" href="11-models.html#log-reg-sec"><span class="std std-ref">logistic regression</span></a>, and <a class="reference internal" href="11-models.html#nn-sec"><span class="std std-ref">neural network models</span></a> that we studied in the previous chapter will all be trained as <em>fully-observed discriminative models</em>. This means two things: (1) all stochastic nodes in the underlying graphs are observed, and (2) the likelihood functions are obtained from the <em>conditional</em> probability functions of the models. The <a class="reference internal" href="11-models.html#gmm-sec"><span class="std std-ref">Gaussian mixture models</span></a> will be trained as <em>partially-observed generative models</em>, which means that the straightfoward MLE algorithm will need to be replaced with the <em>expectation maximization</em> (or <em>EM</em>) algorithm. We will address the EM algorithm separately in <a class="reference internal" href="#em-gmm-sec"><span class="std std-numref">Section 12.5</span></a> below.</p>
<p>To describe the MLE algorithm for fully-observed discriminative models, we need to define the likelihood-based objective functions. These functions are all defined in terms of the data and model probability functions from <a class="reference internal" href="11-models.html#prob-models"><span class="std std-numref">Section 11</span></a>.</p>
<div class="proof definition admonition" id="mle-training-objectives-def">
<p class="admonition-title"><span class="caption-number">Definition 12.1 </span> (MLE training objectives for fully-observed discriminative models)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\btheta\in \bbr^{k}\)</span> be the parameter vector of a fully-observed discriminative model and let <span class="math notranslate nohighlight">\(\hat{p}(\bx,y)\)</span> be the empirical mass function of a dataset</p>
<div class="math notranslate nohighlight">
\[
(\bx^{(1)},y^{(1)}),\ldots,(\bx^{(m)},y^{(m)}) \in \bbr^{1\times n} \times \bbr.
\]</div>
<ol class="arabic">
<li><p>Define the <em>data likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) \def p \big(y^{(1)},\ldots,y^{(m)} \mid \bx^{(1)},\ldots,\bx^{(m)}; \ \btheta \big)
    \]</div>
<p>and the <em>model likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(\btheta; \ \bx, y) \def p ( y \mid \bx ; \ \btheta) 
    \]</div>
</li>
<li><p>Define the <em>data log-likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \ell\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) \def \log{\mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big)}.
    \]</div>
<p>and the <em>model log-likelihood function</em></p>
<div class="math notranslate nohighlight">
\[
    \ell(\btheta; \ \bx, y) \def \log{\mathcal{L}(\btheta; \ \bx, y)}.
    \]</div>
</li>
<li><p>Define the stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
    J\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) \def E \big( \ell\big(\btheta; \widehat{\bX}, \widehat{Y}\big) \big),
    \]</div>
<p>where <span class="math notranslate nohighlight">\((\widehat{\bX}, \widehat{Y}) \sim \hat{p}(\bx, y)\)</span>.</p>
</li>
</ol>
</section>
</div><p>From independence of the dataset, we obtain the following expressions for these training objectives:</p>
<div class="proof theorem admonition" id="mle-formulas-thm">
<p class="admonition-title"><span class="caption-number">Theorem 12.4 </span> (Formulas for MLE training objectives)</p>
<section class="theorem-content" id="proof-content">
<p>Let the notation be as in <a class="reference internal" href="#mle-training-objectives-def">Definition 12.1</a>. We have</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) = \prod_{i=1}^m \mathcal{L}\big( \btheta;\ \bx^{(i)}, y^{(i)} \big)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-log-like-simp-eqn">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-log-like-simp-eqn" title="Permalink to this equation">#</a></span>\[
\ell\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) = \sum_{i=1}^m \ell\big(\btheta; \ \bx^{(i)}, y^{(i)} \big)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-stochastic-simp-eqn">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-stochastic-simp-eqn" title="Permalink to this equation">#</a></span>\[
J\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big) = \frac{1}{m} \sum_{i=1}^m \ell\big(\btheta; \ \bx^{(i)}, y^{(i)} \big).
\]</div>
</section>
</div><p>We now state the MLE algorithm for fully-observed discriminative models; note the similarity to <a class="reference internal" href="#three-mle-thm">Theorem 12.3</a>.</p>
<div class="proof definition admonition" id="mle-fully-observed-def">
<p class="admonition-title"><span class="caption-number">Definition 12.2 </span> (Maximum likelihood estimation for fully-observed discriminative models)</p>
<section class="definition-content" id="proof-content">
<p>Let the notation be as in <a class="reference internal" href="#mle-training-objectives-def">Definition 12.1</a>. A <em>maximum likelihood estimate</em> is a parameter vector <span class="math notranslate nohighlight">\(\btheta^\star\)</span> that is a solution to one of the following three equivalent optimization problems:</p>
<ol class="arabic simple">
<li><p>Maximize the data likelihood function</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  \mathcal{L}\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big).
  \]</div>
<ol class="arabic simple" start="2">
<li><p>Maximize the data log-likelihood function</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  \ell\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big).
  \]</div>
<ol class="arabic simple" start="3">
<li><p>Maximize the stochastic objective function</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
  J\big(\btheta; \ \bx^{(1)},\ldots,\bx^{(m)}, y^{(1)},\ldots,y^{(m)}\big).
  \]</div>
</section>
</div><p>In practice, nobody ever maximizes the data likelihood function directly; instead, maximum likelihood estimates are obtained via the other two objective functions in the forms <a class="reference internal" href="#equation-log-like-simp-eqn">(12.4)</a> and <a class="reference internal" href="#equation-stochastic-simp-eqn">(12.5)</a>. As we mentiond above, the stochastic objective function has the advantage of allowing the use of stochastic gradient descent when closed form solutions are not available.</p>
</section>
<section id="mle-for-linear-regression">
<h2><span class="section-number">12.2. </span>MLE for linear regression<a class="headerlink" href="#mle-for-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Linear regression models have the special property that maximum likelihood estimates may be obtained in <em>closed form</em>. To derive them, we shall assume—as many books in statistics and machine learning do—that the variance parameter <span class="math notranslate nohighlight">\(\sigma^2\)</span> is a <em>fixed</em>, <em>known</em> number and does not need to be learned. You will address the case that <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/suggested-problems/11-2-suggested-problems.md#problem-3">suggested problems</a> for this section.</p>
<p>Therefore, the underlying graph of the linear regression model is of the form</p>
<a class="reference internal image-reference" href="../_images/log-reg-00.svg"><img alt="../_images/log-reg-00.svg" class="align-center" src="../_images/log-reg-00.svg" width="50%" /></a>
<p> </p>
<p>where <span class="math notranslate nohighlight">\(\beta_0 \in \bbr\)</span> and <span class="math notranslate nohighlight">\(\bbeta \in \mathbb{R}^{n\times 1}\)</span> are the only parameters. The link function at <span class="math notranslate nohighlight">\(Y\)</span> is still given by</p>
<div class="math notranslate nohighlight">
\[
Y \mid \bX ; \ \beta_0,\bbeta \sim \mathcal{N}(\mu, \sigma^2), \quad \text{where} \quad \mu = \beta_0 + \bx \bbeta.
\]</div>
<p>Then, given a dataset</p>
<div class="math notranslate nohighlight">
\[
(\bx^{(1)},y^{(1)}),\ldots,(\bx^{(m)},y^{(m)}) \in \bbr^{1\times n} \times \bbr,
\]</div>
<p>we may retrieve the data log-likelihood function from <a class="reference internal" href="11-models.html#lin-reg-sec"><span class="std std-numref">Section 11.2</span></a>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\beta_0, \bbeta) &amp;= \sum_{i=1}^m \log \left[ \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(- \frac{1}{2\sigma^2} \big( y^{(i)} - \mu^{(i)} \big)^2 \right) \right] \\
&amp;= - m\log{\sqrt{2\pi\sigma^2}} - \frac{1}{2\sigma^2} \sum_{i=1}^m \big( y^{(i)} - \mu^{(i)}\big)^2,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu^{(i)} = \beta_0 + \bx^{(i)} \bbeta \)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>.</p>
<p>The maximizers of <span class="math notranslate nohighlight">\(\ell(\beta_0,\bbeta)\)</span> will occur at those parameter values for which <span class="math notranslate nohighlight">\(\nabla \ell(\beta_0, \bbeta)=0\)</span>. Since <span class="math notranslate nohighlight">\(-m\log{\sqrt{2\pi\sigma^2}}\)</span> is constant with respect to the parameters, it may be dropped, leaving the equivalent objective function</p>
<div class="math notranslate nohighlight">
\[
(\beta_0,\bbeta) \mapsto - \frac{1}{2\sigma^2} \sum_{i=1}^m \big( y^{(i)} - \mu^{(i)}\big)^2.
\]</div>
<p>But the reciprocal variance <span class="math notranslate nohighlight">\(1/\sigma^2\)</span> (i.e., the <em>precision</em>) is fixed, and so it too may be dropped, leaving us with the equivalent objective function</p>
<div class="math notranslate nohighlight" id="equation-lin-reg-mle-objective-eqn">
<span class="eqno">(12.6)<a class="headerlink" href="#equation-lin-reg-mle-objective-eqn" title="Permalink to this equation">#</a></span>\[
J(\beta_0,\bbeta) \def - \frac{1}{2}\sum_{i=1}^m \big( y^{(i)} - \mu^{(i)}\big)^2.
\]</div>
<p>Using this latter objective function, we obtain:</p>
<div class="proof theorem admonition" id="mle-lin-reg-thm">
<p class="admonition-title"><span class="caption-number">Theorem 12.5 </span> (Maximum likelihood estimates for linear regression with known variance)</p>
<section class="theorem-content" id="proof-content">
<p>Let the notation be as above, and let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{X} = \begin{bmatrix}
1 &amp; x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_n \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x^{(m)}_1 &amp; \cdots &amp; x^{(m)}_n
\end{bmatrix}, \quad \by = \begin{bmatrix} y^{(1)} \\ \vdots \\ y^{(m)} \end{bmatrix}, \quad \btheta = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n \end{bmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bbeta^T = (\beta_1,\ldots,\beta_n)\)</span>. Provided that the <span class="math notranslate nohighlight">\((n+1) \times (n+1)\)</span> square matrix <span class="math notranslate nohighlight">\(\mathcal{X}^T \mathcal{X}\)</span> is invertible, the maximum likelihood estimates for the parameters <span class="math notranslate nohighlight">\(\bbeta\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> are given by</p>
<div class="math notranslate nohighlight">
\[
\btheta = \left(\mathcal{X}^T \mathcal{X}\right)^{-1}\mathcal{X}^T \by.
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. As we noted above, the MLEs may be obtained by maximizing the function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> given in <a class="reference internal" href="#equation-lin-reg-mle-objective-eqn">(12.6)</a>, which may be rewritten as</p>
<div class="math notranslate nohighlight">
\[
J(\btheta) = -\frac{1}{2} \left( \by - \mathcal{X}\btheta\right)^T\left( \by - \mathcal{X}\btheta\right).
\]</div>
<p>But as you will prove in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/suggested-problems/11-2-suggested-problems.md#problem-1-solution">suggested problems</a>, taking the gradient gives</p>
<div class="math notranslate nohighlight">
\[
\nabla J(\btheta) = - \left( \by - \mathcal{X}\btheta\right)^T \text{Jac}\left(\by - \mathcal{X}\btheta \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{Jac}\left(\by - \mathcal{X}\btheta \right)\)</span> is the Jacobian matrix of the function</p>
<div class="math notranslate nohighlight">
\[
\bbr^{n+1} \to \bbr^m, \quad \btheta \mapsto \by - \mathcal{X}\btheta.
\]</div>
<p>But it is easy to show that <span class="math notranslate nohighlight">\(\text{Jac}\left(\by - \mathcal{X}\btheta \right) = - \mathcal{X}\)</span>, and so</p>
<div class="math notranslate nohighlight">
\[
\nabla J(\btheta) =  \left( \by - \mathcal{X}\btheta\right)^T \mathcal{X}.
\]</div>
<p>Setting the gradient to zero and solving gives</p>
<div class="math notranslate nohighlight">
\[
\mathcal{X}^T \mathcal{X} \btheta = \mathcal{X}^T \by,
\]</div>
<p>from which the desired equation follows. The only thing that is left to prove is that we have actually obtained a <em>maximizer</em>. This follows from concavity of the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span>, which you will establish in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/suggested-problems/11-2-suggested-problems.md#problem-2-solution">suggested problems</a>. Q.E.D.</p>
</div>
<p>Note that the maximizer of the objective function</p>
<div class="math notranslate nohighlight">
\[
J(\beta_0,\bbeta) = - \frac{1}{2} \sum_{i=1}^m \left( y^{(i)} - \mu^{(i)} \right)^2
\]</div>
<p>is the same as the minimizer of the objective function</p>
<div class="math notranslate nohighlight">
\[
\text{RSS}(\beta_0, \bbeta) \def \sum_{i=1}^m \left( y^{(i)} - \mu^{(i)} \right)^2,
\]</div>
<p>called the <em>residual sum of squares</em>. The name comes about from the terminology introduced in <a class="reference internal" href="11-models.html#lin-reg-sec"><span class="std std-numref">Section 11.2</span></a>, where we learned that the differences</p>
<div class="math notranslate nohighlight">
\[
y^{(i)} - \mu^{(i)} = y^{(i)} - \beta_0 - \beta_1 x_1^{(i)} - \cdots - \beta_n x_n^{(i)}
\]</div>
<p>are called the <em>residuals</em>. Thus, the maximum likelihood parameter estimates are those that minimize the residual sum of squares, which explains why the MLEs are also often called the <em>ordinary least squares</em> (<em>OLS</em>) estimates.</p>
<p>It is worth writing out the MLEs in the case of simple linear regression:</p>
<div class="proof corollary admonition" id="mle-simple-lin-reg-cor">
<p class="admonition-title"><span class="caption-number">Corollary 12.1 </span> (Maximum likelihood estimates for simple linear regression with known variance)</p>
<section class="corollary-content" id="proof-content">
<p>Letting the notation be as above, the MLEs for the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> in a simple linear regression model are given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\beta_1 &amp;= \frac{\sum_{i=1}^m \left(x^{(i)} - \bar{x} \right)\left( y^{(i)} - \bar{y} \right)}{\sum_{i=1}^m \left(x^{(i)} - \bar{x} \right)^2}, \\
\beta_0 &amp;= \bar{y} - \beta_1 \bar{x},
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{m} \sum_{i=1}^m x^{(i)}\)</span> and <span class="math notranslate nohighlight">\(\bar{y} = \frac{1}{m} \sum_{i=1}^m y^{(i)}\)</span> are the empirical means.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. First note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{X}^T \mathcal{X} = \begin{bmatrix} m &amp; m \bar{x} \\ m \bar{x} &amp; \sum_{i=1}^m {x^{(i)}}^2 \end{bmatrix}.
\end{split}\]</div>
<p>Assuming this matrix has nonzero determinant, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(\mathcal{X}^T \mathcal{X} \right)^{-1} = \frac{1}{m \sum_{i=1}^m {x^{(i)}}^2 - m^2 \bar{x}^2} \begin{bmatrix} \sum_{i=1}^m {x^{(i)}}^2 &amp; -m \bar{x} \\ -m \bar{x} &amp; m \end{bmatrix}.
\end{split}\]</div>
<p>But</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{X}^T \by = \begin{bmatrix} m \bar{y} \\ \sum_{i=1}^m x^{(i)} y^{(i)} \end{bmatrix},
\end{split}\]</div>
<p>and so from</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = \btheta =  \left(\mathcal{X}^T \mathcal{X}\right)^{-1}\mathcal{X}^T \by
\end{split}\]</div>
<p>we conclude</p>
<div class="math notranslate nohighlight">
\[
\beta_1 = \frac{\sum_{i=1}^m x^{(i)} y^{(i)} -m \bar{x}\bar{y} }{ \sum_{i=1}^m {x^{(i)}}^2 - m \bar{x}^2}.
\]</div>
<p>But as you may easily check, we have</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m x^{(i)} y^{(i)} -m \bar{x}\bar{y}  = \sum_{i=1}^m \left(x^{(i)} - \bar{x} \right)\left( y^{(i)} - \bar{y} \right)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m {x^{(i)}}^2 - m \bar{x}^2 = \sum_{i=1}^m \left(x^{(i)} - \bar{x} \right)^2,
\]</div>
<p>from which the desired equation for <span class="math notranslate nohighlight">\(\beta_1\)</span> follows. To obtain the equation for <span class="math notranslate nohighlight">\(\beta_0\)</span>, note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{X}^T \mathcal{X} \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = \mathcal{X}^T \by 
\end{split}\]</div>
<p>implies <span class="math notranslate nohighlight">\(m \beta_0  + m \beta_1 \bar{x} = m \bar{y}\)</span>, and so <span class="math notranslate nohighlight">\(\beta_0 = \bar{y} - \beta_1 \bar{x}\)</span>. Q.E.D.</p>
</div>
<p>To illustrate the concepts, let’s take a simple toy dataset consisting of the three points</p>
<div class="math notranslate nohighlight">
\[
(0, 0), (1, 1), (2, 3) \in \bbr^2.
\]</div>
<p>We may use the formulas above to obtain the MLEs for the two parameters <span class="math notranslate nohighlight">\(\beta_0,\beta_1 \in \bbr\)</span>. Plotting the regression line <span class="math notranslate nohighlight">\(y=\beta_0 + \beta_1 x\)</span> along with the data yields the left-hand plot in what follows, while the contours of the objective function <span class="math notranslate nohighlight">\(J(\btheta)\)</span> along with the MLE yield the right-hand plot:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># toy data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># MLEs for parameters</span>
<span class="n">beta0</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">2</span>

<span class="c1"># define objective function</span>
<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">theta</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="c1"># define grid</span>
<span class="n">linspace_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">linspace_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">linspace_x</span><span class="p">,</span> <span class="n">linspace_y</span><span class="p">)</span>
<span class="n">contour_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">x_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">T</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">contour_grid</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_grid</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">beta0</span> <span class="o">+</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">grid</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_grid</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">blue</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">beta0</span><span class="p">],</span> <span class="p">[</span><span class="n">beta1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">beta_0$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">beta_1$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/50292f7887b4df9784604b56b9934fcfdb6b0b995144a1515e325ce452d640df.svg" src="../_images/50292f7887b4df9784604b56b9934fcfdb6b0b995144a1515e325ce452d640df.svg" /></figure>
</div>
</div>
<p>You will compute the maximum likelihood estimates for the parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> in the <a class="reference external" href="https://github.com/jmyers7/stats-book-materials/blob/main/suggested-problems/11-2-suggested-problems.md#problem-4">suggested problems</a>.</p>
</section>
<section id="mle-for-logistic-regression">
<h2><span class="section-number">12.3. </span>MLE for logistic regression<a class="headerlink" href="#mle-for-logistic-regression" title="Permalink to this heading">#</a></h2>
<p>Recall from <a class="reference internal" href="11-models.html#log-reg-sec"><span class="std std-numref">Section 11.3</span></a> that the underlying graph of a logistic regression model is given by</p>
<a class="reference internal image-reference" href="../_images/log-reg-00.svg"><img alt="../_images/log-reg-00.svg" class="align-center" src="../_images/log-reg-00.svg" width="50%" /></a>
<p> </p>
<p>where <span class="math notranslate nohighlight">\(\beta_0 \in \bbr\)</span> is the bias term and <span class="math notranslate nohighlight">\(\bbeta \in \mathbb{R}^{n\times 1}\)</span> is the weight vector. The link function at <span class="math notranslate nohighlight">\(Y\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
Y \mid \bX ; \ \beta_0,\bbeta \sim \Ber(\phi) \quad \text{where} \quad \phi = \sigma( \beta_0 + \bx\bbeta),
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\sigma(z) \def \frac{1}{1+e^{-z}}
\]</div>
<p>is the <em>sigmoid function</em>. Given a dataset</p>
<div class="math notranslate nohighlight">
\[
(\bx^{(1)},y^{(1)}),\ldots,(\bx^{(m)},y^{(m)}) \in \bbr^{1\times n} \times \{0,1\},
\]</div>
<p>the data log-likelihood function is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\beta_0, \bbeta) &amp;= \sum_{i=1}^m \log \left[ \big(\phi^{(i)}\big)^{y^{(i)}} \big(1-\phi^{(i)}\big)^{1-y^{(i)}} \right] \\
&amp;= \sum_{i=1}^m \left[ y^{(i)} \log{\phi^{(i)}} + \big( 1-y^{(i)}\big) \log{ \big( 1- \phi^{(i)} \big)}  \right]
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi^{(i)} = \sigma \big(\beta_0 + \bx^{(i)} \bbeta\big)\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>.</p>
<p>As always, maximizers of <span class="math notranslate nohighlight">\(\ell(\beta_0,\bbeta)\)</span> will occur at places where the gradient vanishes, <span class="math notranslate nohighlight">\(\nabla \ell(\beta_0,\bbeta)=0\)</span>. Unlike linear regression models, closed form solutions to this latter equation are not available in general, so numerical approximation algorithms like those studied in <a class="reference internal" href="10-optim.html#optim"><span class="std std-numref">Chapter 10</span></a> are needed. However, linear and logistic regression models <em>do</em> have the following important property in common:</p>
<div class="proof theorem admonition" id="mle-log-reg-thm">
<p class="admonition-title"><span class="caption-number">Theorem 12.6 </span> (Maximum likelihood estimates for logistic regression)</p>
<section class="theorem-content" id="proof-content">
<p>The data log-likelihood function <span class="math notranslate nohighlight">\(\ell(\beta_0,\bbeta)\)</span> for a logistic regression model is concave.</p>
</section>
</div><p>You will prove this in the suggested problems for this section.</p>
<p>In <a class="reference internal" href="11-models.html#log-reg-sec"><span class="std std-numref">Section 11.3</span></a>, we used a black-box learning algorithm to train a logistic regression model on the following dataset:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import scaler from scikit-learn</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># import the data</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/ch10-book-data-01.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># convert the data to numpy arrays</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="s1">&#39;x_2&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># scale the input data</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ss</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># replaced the columns of the dataframe with the transformed data</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;x_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;x_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># convert the data to tensors</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># plot the data</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="c1"># change the default seaborn legend</span>
<span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">new_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">k2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">new_labels</span><span class="p">):</span>
    <span class="n">t</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">k2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/40f48e0e44b7086d306756e4d50aa98390412e2fb364aded44a090581b640bee.svg" src="../_images/40f48e0e44b7086d306756e4d50aa98390412e2fb364aded44a090581b640bee.svg" /></figure>
</div>
</div>
<p>Now, however, we may train a logistic regression model from scratch using MLE. In the following series of plots, we compute the (approximate) MLE using batch gradient descent to find the (approximate) minimizer of the negative stochastic objective function</p>
<div class="math notranslate nohighlight">
\[
- J(\beta_0, \bbeta) =  - E\big( \ell(\beta_0,\bbeta; \widehat{\bX}, \widehat{Y}) \big) =  -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log{\phi^{(i)}} + \big( 1-y^{(i)}\big) \log{ \big( 1- \phi^{(i)} \big)}  \right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{p}(\bx, y)\)</span> is the empirical mass function of the dataset, <span class="math notranslate nohighlight">\(\widehat{\bX},\widehat{Y}\sim \hat{p}(\bx, y)\)</span>, and <span class="math notranslate nohighlight">\(\phi^{(i)} = \sigma \big(\beta_0 + \bx^{(i)} \bbeta\big)\)</span> for each <span class="math notranslate nohighlight">\(i=1,\ldots,m\)</span>. The plots in the first column show the values of the objective function <span class="math notranslate nohighlight">\(J(\beta_0,\bbeta)\)</span> versus the gradient steps. The plots in the second column show the decision boundary at the given point in the execution of the algorithm represented by the large magenta dots in the plots in the first column. As the algorithm executes, we can <em>see</em> the decision boundary moving to the optimal position.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the SGD function</span>
<span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tracking</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">J_args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># if no arguments to the objective are passed, set `J_args` to the empty dictionary</span>
    <span class="n">J_args</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">J_args</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">J_args</span>

    <span class="c1"># define data loader</span>
    <span class="k">if</span> <span class="n">random_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">X</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>
    
    <span class="c1"># initialize lists and a dictionary to track objectives and parameters</span>
    <span class="n">running_objectives</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">running_parameters</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># begin looping through epochs</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        
        <span class="c1"># initialize a list to track per-step objectives. this will only be used if</span>
        <span class="c1"># tracking is set to &#39;epoch&#39;</span>
        <span class="n">per_step_objectives</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># begin gradient descent loop</span>
        <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            
            <span class="c1"># `mini_batch` will be a pair of tensors if `y` is not None; otherwise it will be a single tensor. when</span>
            <span class="c1"># computing the objective, we need to distinguish the case:</span>
            <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">objective</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="o">*</span><span class="n">mini_batch</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">J_args</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">objective</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="o">**</span><span class="n">J_args</span><span class="p">)</span>

            <span class="c1"># if we are tracking per gradient step, then add objective value and parameters to the </span>
            <span class="c1"># running lists. otherwise, we are tracking per epoch, so add the objective value to</span>
            <span class="c1"># the list of per-step objectives</span>
            <span class="k">if</span> <span class="n">tracking</span> <span class="o">==</span> <span class="s1">&#39;gd_step&#39;</span><span class="p">:</span>
                <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">running_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">per_step_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objective</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        
            <span class="c1"># compute gradients    </span>
            <span class="n">objective</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># take a gradient step and update the parameters</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">g</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">parameter</span><span class="o">.</span><span class="n">grad</span>
                    <span class="k">if</span> <span class="n">maximize</span><span class="p">:</span>
                        <span class="n">parameter</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">parameter</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span>
            
            <span class="c1"># zero out the gradients to prepare for the next iteration</span>
            <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                <span class="n">parameter</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="c1"># if we hit the maximum number of gradient steps, break out of the inner `for`</span>
            <span class="c1"># loop</span>
            <span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">step_count</span> <span class="o">==</span> <span class="n">max_steps</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="c1"># if we are tracking per epoch, then add the average per-step objective to the</span>
        <span class="c1"># list of running objectives. also, add the current parameters to the list of running</span>
        <span class="c1"># parameters</span>
        <span class="k">if</span> <span class="n">tracking</span> <span class="o">==</span> <span class="s1">&#39;epoch&#39;</span><span class="p">:</span>
            <span class="n">per_step_objectives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">per_step_objectives</span><span class="p">)</span>
            <span class="n">running_objectives</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">per_step_objectives</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">running_parameters</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
        
        <span class="c1"># if we hit the maximum number of gradient steps, break out of the outer `for`</span>
        <span class="c1"># loop</span>
        <span class="k">if</span> <span class="n">step_count</span> <span class="o">==</span> <span class="n">max_steps</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">running_parameters</span><span class="p">),</span> <span class="n">running_objectives</span>

<span class="k">def</span> <span class="nf">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
    <span class="n">beta0</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;beta0&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">beta0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="c1"># define the logistic regression model</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">probs</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

<span class="c1"># define the objective function for logistic regression</span>
<span class="k">def</span> <span class="nf">J</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probs</span><span class="p">))</span>

<span class="c1"># initialize the parameters</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">beta0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="n">beta</span><span class="p">,</span> <span class="s1">&#39;beta0&#39;</span><span class="p">:</span> <span class="n">beta0</span><span class="p">}</span>

<span class="c1"># define parameters for SGD</span>
<span class="n">sgd_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;parameters&#39;</span><span class="p">:</span> <span class="n">parameters</span><span class="p">,</span>
                  <span class="s1">&#39;J&#39;</span><span class="p">:</span> <span class="n">J</span><span class="p">,</span>
                  <span class="s1">&#39;X&#39;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                  <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
                  <span class="s1">&#39;num_epochs&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
                  <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>
                  <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="s1">&#39;maximize&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                  <span class="s1">&#39;tracking&#39;</span><span class="p">:</span> <span class="s1">&#39;gd_step&#39;</span><span class="p">}</span>

<span class="c1"># run SGD</span>
<span class="n">running_parameters</span><span class="p">,</span> <span class="n">running_objectives</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="o">**</span><span class="n">sgd_parameters</span><span class="p">)</span>

<span class="c1"># define grid for contour plot</span>
<span class="n">resolution</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x1_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="n">x2_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">x1_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">x2_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))))</span>

<span class="c1"># define colormap for the contour plots</span>
<span class="n">desat_blue</span> <span class="o">=</span> <span class="s1">&#39;#7F93FF&#39;</span>
<span class="n">desat_magenta</span> <span class="o">=</span> <span class="s1">&#39;#FF7CFE&#39;</span>
<span class="n">binary_cmap</span> <span class="o">=</span> <span class="n">clr</span><span class="o">.</span><span class="n">LinearSegmentedColormap</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="n">desat_blue</span><span class="p">,</span> <span class="n">desat_magenta</span><span class="p">],</span> <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">epoch_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sgd_parameters</span><span class="p">[</span><span class="s1">&#39;num_epochs&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span><span class="p">[</span><span class="n">epoch</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">running_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="c1"># plot the objective function</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">running_objectives</span><span class="p">)),</span> <span class="n">running_objectives</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gradient steps&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;objective $J(</span><span class="se">\\</span><span class="s1">theta)$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">epoch_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">running_objectives</span><span class="p">[</span><span class="n">epoch</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">magenta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># apply the fitted model to the grid</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">grid</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">)</span>

    <span class="c1"># plot the decision boundary and colors</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">resolution</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_grid</span><span class="p">,</span> <span class="n">x2_grid</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">binary_cmap</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>

    <span class="c1"># plot the data</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># change the default seaborn legend</span>
    <span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">legend_</span><span class="o">.</span><span class="n">texts</span><span class="p">,</span> <span class="n">new_labels</span><span class="p">):</span>
        <span class="n">t</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<figure class="align-center">
<img alt="../_images/42756f0dffe0ffdbe7066cd9b3a94612717080a2e47b7b71c1dff2e137314cda.svg" src="../_images/42756f0dffe0ffdbe7066cd9b3a94612717080a2e47b7b71c1dff2e137314cda.svg" /></figure>
</div>
</div>
</section>
<section id="mle-for-neural-networks">
<h2><span class="section-number">12.4. </span>MLE for neural networks<a class="headerlink" href="#mle-for-neural-networks" title="Permalink to this heading">#</a></h2>
</section>
<section id="expectation-maximization-for-gaussian-mixture-models">
<span id="em-gmm-sec"></span><h2><span class="section-number">12.5. </span>Expectation maximization for Gaussian mixture models<a class="headerlink" href="#expectation-maximization-for-gaussian-mixture-models" title="Permalink to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="11-models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Probabilistic graphical models</p>
      </div>
    </a>
    <a class="right-next"
       href="13-stats-estimators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Statistics and general parameter estimation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-based-learning-objectives">12.1. Likelihood-based learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-linear-regression">12.2. MLE for linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-logistic-regression">12.3. MLE for logistic regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-for-neural-networks">12.4. MLE for neural networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-for-gaussian-mixture-models">12.5. Expectation maximization for Gaussian mixture models</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By John Myers
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>